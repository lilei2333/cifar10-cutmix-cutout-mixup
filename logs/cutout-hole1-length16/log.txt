Namespace(GPU_ids='0', alpha=1.0, batch_size=128, cutmix_prob=0.0, cutout=True, device='cuda', epochs=200, length=16, lr=0.1, mixup=False, momentum=0.9, n_holes=1, output_dir='./logs/cutout-hole1-length16', print_freq=50, resume=None, save_every=10, seed=111, start_epoch=0, validation=True, weight_decay=0.0001, workers=4)
resnet32: #params=464.2K
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (linear): Linear(in_features=64, out_features=10, bias=True)
  )
)
CrossEntropyLoss()
Building dataset...
Start training
Epoch: [0]
epoch 0/200, train:, losses=1.4795, top1=30.2556, 0=32.2444, 1=42.5333, 2=13.8444, 3=22.0667, 4=23.1333, 5=22.0000, 6=36.0222, 7=29.3111, 8=44.2444, 9=37.1556, val:, losses=1.5714, top1=42.8800, 0=52.6000, 1=55.2000, 2=25.4000, 3=25.0000, 4=28.4000, 5=47.0000, 6=53.8000, 7=27.4000, 8=47.0000, 9=67.0000, test:, losses=1.5579, top1=43.6600, 0=54.0000, 1=55.0000, 2=28.8000, 3=25.3000, 4=29.0000, 5=47.7000, 6=53.1000, 7=28.0000, 8=51.9000, 9=63.8000, 46.6s 46.6s/2.6h
Epoch: [1]
epoch 1/200, train:, losses=1.1796, top1=44.4556, 0=48.1778, 1=60.8444, 2=26.3778, 3=29.3111, 4=28.5333, 5=37.3556, 6=57.2889, 7=48.4222, 8=57.3333, 9=50.9111, val:, losses=1.3517, top1=51.8400, 0=77.4000, 1=69.4000, 2=34.6000, 3=18.8000, 4=21.4000, 5=64.2000, 6=81.0000, 7=38.4000, 8=54.2000, 9=59.0000, test:, losses=1.3774, top1=50.5900, 0=78.3000, 1=67.2000, 2=37.2000, 3=18.2000, 4=19.8000, 5=61.6000, 6=80.7000, 7=37.2000, 8=50.9000, 9=54.8000, 49.9s 1.6m/2.7h
Epoch: [2]
epoch 2/200, train:, losses=1.0270, top1=52.5356, 0=54.7333, 1=68.0889, 2=35.2000, 3=33.4222, 4=39.3333, 5=44.2667, 6=64.6444, 7=58.8222, 8=65.1778, 9=61.6667, val:, losses=1.3845, top1=54.1000, 0=69.8000, 1=70.2000, 2=21.4000, 3=47.4000, 4=30.0000, 5=71.6000, 6=60.8000, 7=48.2000, 8=56.0000, 9=65.6000, test:, losses=1.3737, top1=54.1300, 0=69.0000, 1=69.8000, 2=21.2000, 3=46.6000, 4=26.8000, 5=72.9000, 6=61.0000, 7=50.7000, 8=58.6000, 9=64.7000, 52.4s 2.5m/2.8h
Epoch: [3]
epoch 3/200, train:, losses=0.9087, top1=58.4156, 0=60.6000, 1=74.6444, 2=43.7778, 3=38.0444, 4=49.0889, 5=47.4889, 6=68.8000, 7=62.3778, 8=70.2667, 9=69.0667, val:, losses=1.0627, top1=64.3200, 0=54.4000, 1=77.2000, 2=56.2000, 3=57.4000, 4=43.0000, 5=65.2000, 6=59.8000, 7=66.6000, 8=83.0000, 9=80.4000, test:, losses=1.0727, top1=63.5800, 0=54.2000, 1=77.9000, 2=54.8000, 3=55.7000, 4=41.0000, 5=64.6000, 6=58.0000, 7=67.7000, 8=85.0000, 9=76.9000, 50.4s 3.3m/2.8h
Epoch: [4]
epoch 4/200, train:, losses=0.8199, top1=62.7267, 0=65.7111, 1=78.8444, 2=48.1111, 3=42.1778, 4=54.7778, 5=51.6667, 6=71.5778, 7=64.6889, 8=75.4222, 9=74.2889, val:, losses=1.4032, top1=58.1200, 0=44.0000, 1=35.2000, 2=51.4000, 3=45.2000, 4=55.8000, 5=70.0000, 6=47.2000, 7=64.8000, 8=69.2000, 9=98.4000, test:, losses=1.3517, top1=58.2300, 0=47.2000, 1=41.3000, 2=51.7000, 3=44.4000, 4=55.0000, 5=70.3000, 6=40.2000, 7=65.4000, 8=69.6000, 9=97.2000, 52.9s 4.2m/2.8h
Epoch: [5]
epoch 5/200, train:, losses=0.7576, top1=65.8111, 0=68.0444, 1=81.0444, 2=52.7111, 3=45.9778, 4=60.3556, 5=54.0000, 6=74.0000, 7=67.9778, 8=77.4667, 9=76.5333, val:, losses=0.8252, top1=71.8200, 0=80.0000, 1=91.6000, 2=41.0000, 3=50.4000, 4=61.0000, 5=67.0000, 6=81.0000, 7=77.4000, 8=90.4000, 9=78.4000, test:, losses=0.8565, top1=70.6700, 0=81.2000, 1=92.6000, 2=39.0000, 3=47.6000, 4=63.5000, 5=67.1000, 6=79.1000, 7=75.4000, 8=87.1000, 9=74.1000, 51.3s 5.1m/2.8h
Epoch: [6]
epoch 6/200, train:, losses=0.7016, top1=68.5267, 0=71.4667, 1=82.7333, 2=55.0444, 3=49.1111, 4=63.8667, 5=57.2222, 6=75.4667, 7=69.5556, 8=80.8222, 9=79.9778, val:, losses=1.1193, top1=66.6600, 0=73.4000, 1=73.0000, 2=43.6000, 3=31.2000, 4=54.2000, 5=87.2000, 6=70.6000, 7=89.4000, 8=70.6000, 9=73.4000, test:, losses=1.1201, top1=66.2900, 0=73.1000, 1=75.8000, 2=43.6000, 3=29.7000, 4=53.9000, 5=86.8000, 6=71.3000, 7=87.9000, 8=70.9000, 9=69.9000, 52.0s 5.9m/2.8h
Epoch: [7]
epoch 7/200, train:, losses=0.6597, top1=70.2200, 0=73.4222, 1=84.1333, 2=57.5111, 3=51.4222, 4=66.6000, 5=57.8444, 6=76.3111, 7=72.7333, 8=81.4889, 9=80.7333, val:, losses=0.9740, top1=69.0400, 0=92.2000, 1=90.6000, 2=51.6000, 3=22.4000, 4=72.4000, 5=48.6000, 6=65.8000, 7=76.0000, 8=83.4000, 9=87.4000, test:, losses=1.0080, top1=68.9200, 0=94.1000, 1=92.9000, 2=52.8000, 3=21.4000, 4=74.3000, 5=49.6000, 6=62.8000, 7=75.2000, 8=82.3000, 9=83.8000, 53.9s 6.8m/2.9h
Epoch: [8]
epoch 8/200, train:, losses=0.6164, top1=72.3089, 0=75.4667, 1=85.0000, 2=60.0000, 3=54.8667, 4=68.7556, 5=60.6444, 6=77.8889, 7=75.7778, 8=82.5556, 9=82.1333, val:, losses=0.7185, top1=75.3600, 0=79.8000, 1=95.0000, 2=58.4000, 3=47.4000, 4=68.6000, 5=69.4000, 6=87.0000, 7=83.8000, 8=95.2000, 9=69.0000, test:, losses=0.7546, top1=74.4600, 0=81.4000, 1=95.2000, 2=55.5000, 3=46.7000, 4=68.2000, 5=68.8000, 6=83.8000, 7=83.3000, 8=94.2000, 9=67.5000, 53.7s 7.7m/2.9h
Epoch: [9]
epoch 9/200, train:, losses=0.5887, top1=73.6067, 0=76.0000, 1=85.2222, 2=61.9333, 3=57.7778, 4=70.4222, 5=62.6444, 6=79.7556, 7=76.5556, 8=83.7333, 9=82.0222, val:, losses=0.7095, top1=76.1400, 0=82.6000, 1=98.0000, 2=65.0000, 3=61.8000, 4=78.0000, 5=46.2000, 6=87.6000, 7=79.8000, 8=82.8000, 9=79.6000, test:, losses=0.7414, top1=75.4200, 0=84.5000, 1=98.3000, 2=60.9000, 3=61.2000, 4=77.9000, 5=48.3000, 6=86.5000, 7=78.4000, 8=81.0000, 9=77.2000, 50.8s 8.6m/2.9h
Epoch: [10]
epoch 10/200, train:, losses=0.5591, top1=74.9311, 0=77.6889, 1=86.8444, 2=64.2000, 3=57.4000, 4=72.5111, 5=65.0222, 6=80.2889, 7=77.1556, 8=85.2222, 9=82.9778, val:, losses=0.9689, top1=69.6600, 0=47.6000, 1=74.0000, 2=87.4000, 3=36.4000, 4=60.8000, 5=59.0000, 6=90.2000, 7=82.4000, 8=81.8000, 9=77.0000, test:, losses=0.9537, top1=70.6900, 0=50.9000, 1=78.2000, 2=88.4000, 3=33.7000, 4=63.6000, 5=59.3000, 6=88.9000, 7=81.7000, 8=85.6000, 9=76.6000, 52.9s 9.4m/2.9h
Epoch: [11]
epoch 11/200, train:, losses=0.5354, top1=76.3067, 0=79.0889, 1=87.2222, 2=66.4000, 3=59.9778, 4=74.6222, 5=65.9333, 6=81.2444, 7=79.2667, 8=85.5333, 9=83.7778, val:, losses=0.8519, top1=73.3600, 0=76.6000, 1=82.6000, 2=76.6000, 3=37.0000, 4=65.2000, 5=83.6000, 6=49.6000, 7=91.4000, 8=75.2000, 9=95.8000, test:, losses=0.8564, top1=73.8600, 0=78.0000, 1=87.0000, 2=76.3000, 3=34.4000, 4=66.9000, 5=86.4000, 6=46.2000, 7=90.1000, 8=77.9000, 9=95.4000, 53.0s 10.3m/2.9h
Epoch: [12]
epoch 12/200, train:, losses=0.5220, top1=76.7800, 0=78.9778, 1=87.5778, 2=68.2222, 3=60.9111, 4=74.8222, 5=66.4667, 6=82.2222, 7=78.9111, 8=85.6667, 9=84.0222, val:, losses=0.6413, top1=78.8000, 0=79.0000, 1=94.0000, 2=88.4000, 3=53.6000, 4=80.2000, 5=58.4000, 6=87.4000, 7=73.2000, 8=88.4000, 9=85.4000, test:, losses=0.6533, top1=78.3400, 0=79.1000, 1=94.5000, 2=84.2000, 3=53.3000, 4=79.4000, 5=59.0000, 6=88.6000, 7=73.4000, 8=89.4000, 9=82.5000, 53.8s 11.2m/2.9h
Epoch: [13]
epoch 13/200, train:, losses=0.5021, top1=77.5956, 0=80.6889, 1=88.6000, 2=68.1333, 3=61.8889, 4=75.6444, 5=66.8000, 6=81.8667, 7=80.3778, 8=87.0000, 9=84.9556, val:, losses=0.6032, top1=79.3200, 0=67.8000, 1=93.2000, 2=80.2000, 3=65.2000, 4=77.0000, 5=46.0000, 6=88.6000, 7=92.8000, 8=95.4000, 9=87.0000, test:, losses=0.6104, top1=79.4900, 0=70.0000, 1=93.2000, 2=80.8000, 3=62.4000, 4=80.4000, 5=49.3000, 6=87.0000, 7=91.1000, 8=95.1000, 9=85.6000, 52.4s 12.1m/2.9h
Epoch: [14]
epoch 14/200, train:, losses=0.4902, top1=78.2289, 0=81.0667, 1=88.4667, 2=69.4444, 3=62.1111, 4=75.8444, 5=68.0667, 6=83.4889, 7=80.7111, 8=87.4889, 9=85.6000, val:, losses=0.6235, top1=79.5600, 0=79.2000, 1=85.8000, 2=76.4000, 3=68.0000, 4=83.6000, 5=44.8000, 6=93.6000, 7=77.8000, 8=93.0000, 9=93.4000, test:, losses=0.6051, top1=79.9000, 0=79.8000, 1=88.1000, 2=74.5000, 3=66.6000, 4=86.0000, 5=46.3000, 6=94.8000, 7=77.5000, 8=94.2000, 9=91.2000, 53.0s 13.0m/2.9h
Epoch: [15]
epoch 15/200, train:, losses=0.4758, top1=78.7044, 0=80.6889, 1=88.9778, 2=69.8222, 3=64.0000, 4=77.0000, 5=68.6667, 6=82.5556, 7=81.4444, 8=87.0667, 9=86.8222, val:, losses=0.5904, top1=80.9000, 0=79.8000, 1=97.4000, 2=76.2000, 3=73.4000, 4=82.0000, 5=58.6000, 6=87.0000, 7=81.2000, 8=94.4000, 9=79.0000, test:, losses=0.6008, top1=81.1100, 0=84.1000, 1=98.5000, 2=73.1000, 3=74.5000, 4=83.2000, 5=62.1000, 6=85.0000, 7=81.7000, 8=93.5000, 9=75.4000, 53.4s 13.9m/2.9h
Epoch: [16]
epoch 16/200, train:, losses=0.4651, top1=79.4178, 0=82.4222, 1=89.6444, 2=70.9556, 3=64.3333, 4=77.9556, 5=68.3556, 6=84.3556, 7=82.6444, 8=86.5111, 9=87.0000, val:, losses=0.6060, top1=80.1200, 0=75.4000, 1=86.2000, 2=80.8000, 3=46.8000, 4=78.4000, 5=87.0000, 6=92.2000, 7=78.2000, 8=84.2000, 9=92.0000, test:, losses=0.5899, top1=80.3600, 0=74.0000, 1=89.9000, 2=79.6000, 3=45.7000, 4=78.5000, 5=86.1000, 6=92.3000, 7=80.3000, 8=84.7000, 9=92.5000, 53.8s 14.8m/2.9h
Epoch: [17]
epoch 17/200, train:, losses=0.4579, top1=79.8244, 0=81.7556, 1=89.6222, 2=71.8889, 3=65.8889, 4=78.4889, 5=69.4667, 6=84.6000, 7=81.9556, 8=88.3333, 9=86.2444, val:, losses=0.5339, top1=81.7200, 0=76.6000, 1=94.0000, 2=80.0000, 3=69.2000, 4=86.0000, 5=71.0000, 6=87.8000, 7=70.2000, 8=93.0000, 9=89.4000, test:, losses=0.5297, top1=82.3500, 0=76.8000, 1=94.4000, 2=78.7000, 3=70.3000, 4=84.7000, 5=74.0000, 6=88.4000, 7=73.1000, 8=94.1000, 9=89.0000, 53.7s 15.7m/2.9h
Epoch: [18]
epoch 18/200, train:, losses=0.4521, top1=79.8756, 0=82.3111, 1=88.7111, 2=72.0444, 3=65.4667, 4=77.5333, 5=70.4222, 6=85.4222, 7=82.6667, 8=87.7111, 9=86.4667, val:, losses=0.4826, top1=83.3000, 0=92.4000, 1=93.0000, 2=65.2000, 3=71.4000, 4=86.0000, 5=73.6000, 6=86.8000, 7=83.2000, 8=92.4000, 9=89.0000, test:, losses=0.4935, top1=84.0000, 0=93.3000, 1=95.5000, 2=62.2000, 3=72.9000, 4=88.5000, 5=76.0000, 6=87.8000, 7=84.8000, 8=91.3000, 9=87.7000, 53.8s 16.6m/2.9h
Epoch: [19]
epoch 19/200, train:, losses=0.4343, top1=80.6289, 0=82.0889, 1=90.1556, 2=74.1778, 3=65.2889, 4=79.4667, 5=70.7778, 6=84.7111, 7=83.1111, 8=88.7556, 9=87.7556, val:, losses=0.5936, top1=81.3400, 0=93.2000, 1=92.6000, 2=86.0000, 3=62.0000, 4=84.2000, 5=73.4000, 6=67.4000, 7=70.4000, 8=95.0000, 9=89.2000, test:, losses=0.6248, top1=80.6600, 0=93.2000, 1=94.5000, 2=81.7000, 3=64.8000, 4=82.4000, 5=73.3000, 6=67.3000, 7=69.4000, 8=93.9000, 9=86.1000, 53.7s 17.5m/2.9h
Epoch: [20]
epoch 20/200, train:, losses=0.4291, top1=80.8200, 0=83.3778, 1=90.0889, 2=73.4667, 3=66.8889, 4=79.2444, 5=70.9778, 6=84.5333, 7=82.8000, 8=88.8000, 9=88.0222, val:, losses=0.5097, top1=81.8800, 0=86.6000, 1=92.2000, 2=68.4000, 3=78.6000, 4=78.0000, 5=56.6000, 6=91.6000, 7=93.2000, 8=90.6000, 9=83.0000, test:, losses=0.5159, top1=82.5600, 0=87.0000, 1=93.4000, 2=66.4000, 3=75.6000, 4=82.2000, 5=59.6000, 6=91.0000, 7=94.1000, 8=92.4000, 9=83.9000, 51.9s 18.3m/2.9h
Epoch: [21]
epoch 21/200, train:, losses=0.4213, top1=81.2889, 0=82.9333, 1=90.4889, 2=73.4222, 3=68.2444, 4=80.3778, 5=71.0222, 6=85.7111, 7=84.1333, 8=89.4889, 9=87.0667, val:, losses=0.4745, top1=83.9600, 0=93.4000, 1=96.8000, 2=79.8000, 3=69.8000, 4=80.6000, 5=68.4000, 6=89.4000, 7=81.8000, 8=91.8000, 9=87.8000, test:, losses=0.4955, top1=83.8400, 0=92.2000, 1=97.8000, 2=77.5000, 3=68.3000, 4=82.0000, 5=70.0000, 6=88.4000, 7=85.5000, 8=91.1000, 9=85.6000, 52.6s 19.2m/2.9h
Epoch: [22]
epoch 22/200, train:, losses=0.4116, top1=81.6267, 0=84.6444, 1=90.8444, 2=73.8222, 3=67.9778, 4=79.2889, 5=72.2889, 6=85.7778, 7=84.1778, 8=89.2667, 9=88.1778, val:, losses=0.4448, top1=84.8200, 0=92.2000, 1=91.0000, 2=71.8000, 3=64.6000, 4=82.4000, 5=86.6000, 6=86.0000, 7=89.2000, 8=94.0000, 9=90.4000, test:, losses=0.4641, top1=84.7000, 0=93.4000, 1=94.3000, 2=70.9000, 3=65.0000, 4=84.2000, 5=84.5000, 6=85.1000, 7=87.4000, 8=92.0000, 9=90.2000, 51.5s 20.1m/2.9h
Epoch: [23]
epoch 23/200, train:, losses=0.4085, top1=81.8200, 0=83.7333, 1=90.7333, 2=75.1556, 3=67.6889, 4=81.4889, 5=71.2889, 6=86.3556, 7=84.7111, 8=88.8444, 9=88.2000, val:, losses=0.4624, top1=84.4200, 0=86.8000, 1=92.6000, 2=69.4000, 3=76.0000, 4=90.2000, 5=67.8000, 6=82.2000, 7=91.2000, 8=94.2000, 9=93.8000, test:, losses=0.4661, top1=84.2300, 0=87.6000, 1=94.4000, 2=67.6000, 3=74.8000, 4=90.8000, 5=68.3000, 6=82.2000, 7=91.5000, 8=92.2000, 9=92.9000, 52.1s 20.9m/2.9h
Epoch: [24]
epoch 24/200, train:, losses=0.4074, top1=81.8711, 0=84.5333, 1=90.6444, 2=75.2444, 3=68.4000, 4=81.1333, 5=71.8444, 6=85.8000, 7=84.1333, 8=88.9556, 9=88.0222, val:, losses=0.6134, top1=79.7800, 0=91.2000, 1=88.8000, 2=80.0000, 3=82.2000, 4=59.8000, 5=55.4000, 6=71.4000, 7=86.2000, 8=95.6000, 9=87.2000, test:, losses=0.6076, top1=79.9100, 0=89.9000, 1=90.7000, 2=80.3000, 3=84.4000, 4=63.7000, 5=56.3000, 6=68.1000, 7=83.6000, 8=95.2000, 9=86.9000, 52.1s 21.8m/2.9h
Epoch: [25]
epoch 25/200, train:, losses=0.4011, top1=82.0911, 0=84.6667, 1=91.0222, 2=75.0222, 3=68.3778, 4=80.5778, 5=72.6889, 6=85.8889, 7=84.5556, 8=89.3778, 9=88.7333, val:, losses=0.5089, top1=84.0200, 0=87.4000, 1=91.4000, 2=88.6000, 3=75.6000, 4=92.8000, 5=69.8000, 6=81.0000, 7=74.2000, 8=87.4000, 9=92.0000, test:, losses=0.5031, top1=83.8200, 0=87.3000, 1=93.5000, 2=86.9000, 3=71.7000, 4=92.4000, 5=66.6000, 6=83.0000, 7=76.6000, 8=90.0000, 9=90.2000, 52.3s 22.7m/2.9h
Epoch: [26]
epoch 26/200, train:, losses=0.3901, top1=82.4667, 0=85.2000, 1=90.8000, 2=76.0889, 3=68.8222, 4=81.2667, 5=72.2000, 6=86.4000, 7=85.3333, 8=89.9778, 9=88.5778, val:, losses=0.6426, top1=80.4600, 0=80.0000, 1=90.6000, 2=73.2000, 3=42.6000, 4=95.0000, 5=66.4000, 6=93.0000, 7=84.4000, 8=88.6000, 9=90.8000, test:, losses=0.6613, top1=80.4000, 0=79.9000, 1=91.8000, 2=68.4000, 3=42.0000, 4=95.2000, 5=69.5000, 6=93.7000, 7=86.6000, 8=88.7000, 9=88.2000, 52.8s 23.6m/2.9h
Epoch: [27]
epoch 27/200, train:, losses=0.3865, top1=82.6600, 0=84.0444, 1=91.0222, 2=77.0000, 3=69.9778, 4=81.2667, 5=72.6222, 6=86.7778, 7=85.1333, 8=89.9111, 9=88.8444, val:, losses=0.4693, top1=84.9000, 0=83.4000, 1=90.8000, 2=78.8000, 3=66.2000, 4=80.2000, 5=78.2000, 6=83.8000, 7=93.8000, 8=97.2000, 9=96.6000, test:, losses=0.4866, top1=84.3400, 0=81.5000, 1=92.6000, 2=76.3000, 3=63.1000, 4=82.4000, 5=79.2000, 6=82.7000, 7=94.5000, 8=95.4000, 9=95.7000, 54.3s 24.5m/2.9h
Epoch: [28]
epoch 28/200, train:, losses=0.3795, top1=83.1311, 0=85.8222, 1=91.4444, 2=76.5778, 3=69.8000, 4=81.6222, 5=73.4222, 6=86.6444, 7=86.2444, 8=90.3111, 9=89.4222, val:, losses=0.4206, top1=85.9200, 0=90.6000, 1=96.8000, 2=86.6000, 3=65.4000, 4=85.2000, 5=77.6000, 6=92.0000, 7=80.6000, 8=92.0000, 9=92.4000, test:, losses=0.4378, top1=85.4600, 0=87.9000, 1=95.9000, 2=87.0000, 3=62.4000, 4=86.6000, 5=76.9000, 6=93.5000, 7=81.5000, 8=92.7000, 9=90.2000, 53.0s 25.3m/2.9h
Epoch: [29]
epoch 29/200, train:, losses=0.3777, top1=83.1911, 0=85.6444, 1=91.5556, 2=77.7111, 3=70.9333, 4=81.8667, 5=72.6444, 6=86.9111, 7=85.6667, 8=89.9333, 9=89.0444, val:, losses=0.4225, top1=85.7000, 0=94.0000, 1=85.4000, 2=80.0000, 3=75.8000, 4=89.4000, 5=73.0000, 6=84.2000, 7=88.8000, 8=93.4000, 9=93.0000, test:, losses=0.4352, top1=85.4800, 0=92.6000, 1=89.7000, 2=76.1000, 3=75.6000, 4=88.1000, 5=70.1000, 6=84.9000, 7=90.2000, 8=95.5000, 9=92.0000, 51.6s 26.2m/2.9h
Epoch: [30]
epoch 30/200, train:, losses=0.3674, top1=83.6444, 0=85.8444, 1=92.0000, 2=77.7333, 3=69.8889, 4=81.9333, 5=74.4000, 6=88.1333, 7=86.5556, 8=90.0222, 9=89.9333, val:, losses=0.4205, top1=85.6600, 0=89.0000, 1=90.2000, 2=77.2000, 3=69.6000, 4=93.8000, 5=70.2000, 6=84.8000, 7=90.2000, 8=95.4000, 9=96.2000, test:, losses=0.4387, top1=85.4700, 0=89.5000, 1=92.6000, 2=73.1000, 3=67.4000, 4=93.2000, 5=73.7000, 6=84.5000, 7=91.2000, 8=94.7000, 9=94.8000, 53.3s 27.1m/2.9h
Epoch: [31]
epoch 31/200, train:, losses=0.3701, top1=83.5133, 0=85.7333, 1=91.2889, 2=77.2222, 3=70.3333, 4=82.0889, 5=74.5111, 6=87.0222, 7=86.8889, 8=90.5111, 9=89.5333, val:, losses=0.5370, top1=83.2200, 0=86.4000, 1=96.2000, 2=92.6000, 3=78.2000, 4=81.8000, 5=65.6000, 6=75.8000, 7=71.4000, 8=96.4000, 9=87.8000, test:, losses=0.5455, top1=82.0500, 0=84.0000, 1=96.7000, 2=91.0000, 3=78.8000, 4=80.4000, 5=63.7000, 6=74.6000, 7=70.1000, 8=95.1000, 9=86.1000, 52.0s 28.0m/2.9h
Epoch: [32]
epoch 32/200, train:, losses=0.3690, top1=83.6067, 0=86.0222, 1=91.3556, 2=77.2000, 3=72.3111, 4=82.6667, 5=72.7778, 6=87.8667, 7=85.8667, 8=90.6000, 9=89.4000, val:, losses=0.4383, top1=85.5800, 0=87.2000, 1=88.4000, 2=88.4000, 3=62.2000, 4=83.6000, 5=82.4000, 6=88.0000, 7=92.4000, 8=86.2000, 9=97.0000, test:, losses=0.4266, top1=85.9500, 0=87.3000, 1=91.3000, 2=87.9000, 3=61.4000, 4=83.8000, 5=82.5000, 6=87.2000, 7=94.3000, 8=88.3000, 9=95.5000, 52.7s 28.8m/2.9h
Epoch: [33]
epoch 33/200, train:, losses=0.3655, top1=83.6378, 0=85.3333, 1=91.1556, 2=76.9333, 3=71.2444, 4=82.6444, 5=74.5778, 6=88.2222, 7=86.3111, 8=90.4444, 9=89.5111, val:, losses=0.4572, top1=85.1400, 0=88.0000, 1=91.6000, 2=85.6000, 3=72.2000, 4=87.6000, 5=80.2000, 6=90.8000, 7=85.0000, 8=84.2000, 9=86.2000, test:, losses=0.4735, top1=85.1400, 0=90.0000, 1=93.3000, 2=82.5000, 3=69.6000, 4=89.3000, 5=79.5000, 6=91.6000, 7=85.8000, 8=86.7000, 9=83.1000, 53.0s 29.7m/2.9h
Epoch: [34]
epoch 34/200, train:, losses=0.3568, top1=84.0533, 0=86.6667, 1=91.5333, 2=78.6444, 3=71.6667, 4=82.5778, 5=74.8667, 6=87.9111, 7=86.8889, 8=90.2222, 9=89.5556, val:, losses=0.5788, top1=82.1400, 0=86.2000, 1=96.0000, 2=74.8000, 3=90.0000, 4=85.0000, 5=63.6000, 6=56.8000, 7=85.8000, 8=93.8000, 9=89.4000, test:, losses=0.6028, top1=81.7100, 0=88.6000, 1=97.6000, 2=71.0000, 3=88.9000, 4=84.4000, 5=65.1000, 6=55.4000, 7=85.4000, 8=92.8000, 9=87.9000, 53.4s 30.6m/2.9h
Epoch: [35]
epoch 35/200, train:, losses=0.3550, top1=84.4711, 0=86.7333, 1=92.0667, 2=78.3111, 3=72.7556, 4=83.0444, 5=76.0444, 6=87.8444, 7=86.7556, 8=90.8222, 9=90.3333, val:, losses=0.4226, top1=86.8000, 0=88.6000, 1=91.0000, 2=84.6000, 3=60.4000, 4=86.8000, 5=84.6000, 6=86.8000, 7=97.2000, 8=91.4000, 9=96.6000, test:, losses=0.4432, top1=85.5900, 0=89.6000, 1=91.5000, 2=80.2000, 3=58.5000, 4=84.4000, 5=80.9000, 6=87.8000, 7=95.7000, 8=92.5000, 9=94.8000, 53.7s 31.5m/2.9h
Epoch: [36]
epoch 36/200, train:, losses=0.3538, top1=84.2244, 0=86.4000, 1=92.0000, 2=78.4000, 3=71.4889, 4=83.8444, 5=75.0000, 6=87.6444, 7=86.6444, 8=90.8889, 9=89.9333, val:, losses=0.4571, top1=85.2200, 0=94.4000, 1=97.8000, 2=75.2000, 3=76.4000, 4=87.0000, 5=80.4000, 6=93.8000, 7=77.6000, 8=85.6000, 9=84.0000, test:, losses=0.4789, top1=84.4200, 0=93.3000, 1=98.1000, 2=71.5000, 3=72.6000, 4=88.3000, 5=79.9000, 6=95.2000, 7=79.0000, 8=85.6000, 9=80.7000, 53.4s 32.4m/2.9h
Epoch: [37]
epoch 37/200, train:, losses=0.3530, top1=84.2600, 0=85.6667, 1=91.8222, 2=78.5333, 3=71.4889, 4=83.2444, 5=75.6889, 6=88.2667, 7=87.4667, 8=90.6889, 9=89.7333, val:, losses=0.4498, top1=85.0000, 0=95.4000, 1=96.0000, 2=87.2000, 3=78.0000, 4=81.2000, 5=62.2000, 6=87.6000, 7=87.4000, 8=88.6000, 9=86.4000, test:, losses=0.4634, top1=84.6000, 0=95.6000, 1=97.0000, 2=86.6000, 3=75.6000, 4=78.9000, 5=61.4000, 6=88.3000, 7=88.1000, 8=90.6000, 9=83.9000, 51.9s 33.3m/2.9h
Epoch: [38]
epoch 38/200, train:, losses=0.3486, top1=84.4444, 0=86.0444, 1=91.8444, 2=78.7556, 3=72.4667, 4=83.3111, 5=75.6667, 6=87.9778, 7=87.6889, 8=90.4889, 9=90.2000, val:, losses=0.4430, top1=86.8000, 0=90.4000, 1=92.4000, 2=74.6000, 3=71.6000, 4=88.0000, 5=83.6000, 6=97.2000, 7=89.8000, 8=92.6000, 9=87.8000, test:, losses=0.4659, top1=85.7200, 0=89.4000, 1=91.4000, 2=70.0000, 3=70.1000, 4=91.6000, 5=82.1000, 6=96.1000, 7=87.8000, 8=90.6000, 9=88.1000, 53.2s 34.1m/2.9h
Epoch: [39]
epoch 39/200, train:, losses=0.3482, top1=84.4067, 0=86.4667, 1=91.4444, 2=78.8222, 3=72.8222, 4=83.2667, 5=75.3556, 6=88.2667, 7=86.8889, 8=90.4667, 9=90.2667, val:, losses=0.5120, top1=83.6200, 0=96.0000, 1=91.8000, 2=88.6000, 3=56.2000, 4=74.4000, 5=86.6000, 6=88.2000, 7=95.2000, 8=72.8000, 9=86.4000, test:, losses=0.5138, top1=83.6500, 0=95.4000, 1=92.6000, 2=86.3000, 3=58.5000, 4=76.8000, 5=83.9000, 6=89.9000, 7=94.9000, 8=74.3000, 9=83.9000, 53.6s 35.0m/2.9h
Epoch: [40]
epoch 40/200, train:, losses=0.3406, top1=84.8244, 0=86.5556, 1=92.1556, 2=79.1333, 3=73.1556, 4=84.1111, 5=76.0667, 6=88.5556, 7=87.1111, 8=91.3111, 9=90.0889, val:, losses=0.4219, top1=86.3600, 0=78.4000, 1=82.0000, 2=81.8000, 3=78.6000, 4=91.2000, 5=77.6000, 6=88.6000, 7=92.8000, 8=94.8000, 9=97.8000, test:, losses=0.4009, top1=86.3900, 0=79.4000, 1=87.3000, 2=81.5000, 3=76.6000, 4=91.1000, 5=74.1000, 6=89.3000, 7=93.2000, 8=93.9000, 9=97.5000, 53.0s 35.9m/2.9h
Epoch: [41]
epoch 41/200, train:, losses=0.3446, top1=84.5467, 0=85.9111, 1=92.2000, 2=78.7333, 3=72.6444, 4=83.5556, 5=75.2222, 6=88.8444, 7=87.2222, 8=90.9778, 9=90.1556, val:, losses=0.6573, top1=80.5600, 0=56.8000, 1=92.4000, 2=69.4000, 3=87.0000, 4=91.0000, 5=65.6000, 6=86.2000, 7=73.6000, 8=91.2000, 9=92.4000, test:, losses=0.6457, top1=81.3400, 0=60.7000, 1=94.4000, 2=65.8000, 3=90.1000, 4=90.7000, 5=69.1000, 6=85.8000, 7=74.0000, 8=91.5000, 9=91.3000, 53.2s 36.8m/2.9h
Epoch: [42]
epoch 42/200, train:, losses=0.3423, top1=84.7533, 0=86.5333, 1=91.9111, 2=79.4889, 3=72.7111, 4=84.0444, 5=76.1778, 6=88.3111, 7=87.8222, 8=90.3111, 9=90.2222, val:, losses=0.4657, top1=85.1200, 0=93.8000, 1=90.2000, 2=87.8000, 3=67.2000, 4=86.2000, 5=69.6000, 6=93.4000, 7=75.2000, 8=93.4000, 9=94.4000, test:, losses=0.4624, top1=85.2200, 0=92.7000, 1=93.8000, 2=84.8000, 3=69.5000, 4=87.5000, 5=68.4000, 6=93.2000, 7=74.0000, 8=95.0000, 9=93.3000, 52.3s 37.7m/2.9h
Epoch: [43]
epoch 43/200, train:, losses=0.3358, top1=84.9244, 0=86.0444, 1=91.9333, 2=78.9778, 3=73.0889, 4=84.0000, 5=76.8889, 6=89.2444, 7=87.6667, 8=90.9333, 9=90.4667, val:, losses=0.5815, top1=82.0200, 0=81.8000, 1=83.2000, 2=71.8000, 3=51.6000, 4=86.4000, 5=94.8000, 6=81.2000, 7=81.8000, 8=89.6000, 9=98.0000, test:, losses=0.5796, top1=82.3700, 0=83.8000, 1=86.4000, 2=70.0000, 3=50.8000, 4=85.8000, 5=95.2000, 6=81.4000, 7=82.5000, 8=90.1000, 9=97.7000, 53.2s 38.6m/2.9h
Epoch: [44]
epoch 44/200, train:, losses=0.3395, top1=85.0356, 0=87.2000, 1=92.2222, 2=80.2667, 3=73.4667, 4=83.8000, 5=76.2444, 6=88.5333, 7=87.5333, 8=90.9333, 9=90.1556, val:, losses=0.4471, top1=85.8400, 0=92.8000, 1=93.6000, 2=86.8000, 3=65.8000, 4=83.6000, 5=84.8000, 6=77.6000, 7=90.6000, 8=94.2000, 9=88.6000, test:, losses=0.4513, top1=85.4200, 0=90.7000, 1=93.6000, 2=86.1000, 3=67.3000, 4=84.7000, 5=83.1000, 6=77.3000, 7=89.7000, 8=94.6000, 9=87.1000, 52.5s 39.4m/2.9h
Epoch: [45]
epoch 45/200, train:, losses=0.3328, top1=85.1756, 0=86.9778, 1=92.2444, 2=79.5111, 3=73.8000, 4=84.1333, 5=77.2000, 6=88.8222, 7=88.0444, 8=90.8667, 9=90.1556, val:, losses=0.3922, top1=87.1400, 0=87.2000, 1=95.8000, 2=83.2000, 3=73.0000, 4=91.8000, 5=84.8000, 6=78.2000, 7=89.6000, 8=96.0000, 9=91.8000, test:, losses=0.4074, top1=86.6000, 0=87.1000, 1=96.2000, 2=80.4000, 3=74.9000, 4=88.8000, 5=85.9000, 6=77.4000, 7=89.7000, 8=95.2000, 9=90.4000, 52.3s 40.3m/2.9h
Epoch: [46]
epoch 46/200, train:, losses=0.3282, top1=85.3200, 0=86.9333, 1=92.6444, 2=80.0889, 3=73.4222, 4=84.3556, 5=76.8000, 6=89.2222, 7=87.7778, 8=91.1111, 9=90.8444, val:, losses=0.4371, top1=86.1200, 0=88.2000, 1=95.4000, 2=82.8000, 3=77.8000, 4=80.8000, 5=78.6000, 6=75.6000, 7=91.2000, 8=95.2000, 9=95.6000, test:, losses=0.4490, top1=85.9900, 0=87.2000, 1=96.5000, 2=81.2000, 3=78.3000, 4=80.7000, 5=78.5000, 6=77.5000, 7=91.8000, 8=94.9000, 9=93.3000, 52.6s 41.2m/2.9h
Epoch: [47]
epoch 47/200, train:, losses=0.3310, top1=85.1156, 0=86.9111, 1=92.3333, 2=79.5111, 3=72.2222, 4=85.2444, 5=75.8444, 6=89.4889, 7=88.0667, 8=91.0667, 9=90.4667, val:, losses=0.6742, top1=80.5200, 0=87.8000, 1=90.0000, 2=76.8000, 3=53.4000, 4=77.4000, 5=86.4000, 6=62.8000, 7=89.6000, 8=83.2000, 9=97.8000, test:, losses=0.6947, top1=80.4300, 0=91.3000, 1=90.6000, 2=74.6000, 3=51.7000, 4=79.4000, 5=85.0000, 6=63.0000, 7=88.7000, 8=83.7000, 9=96.3000, 52.3s 42.1m/2.9h
Epoch: [48]
epoch 48/200, train:, losses=0.3292, top1=85.3422, 0=87.1778, 1=92.3111, 2=79.7556, 3=74.1333, 4=84.5778, 5=77.5111, 6=88.6444, 7=87.9556, 8=90.9778, 9=90.3778, val:, losses=0.6558, top1=80.3400, 0=96.6000, 1=93.4000, 2=53.4000, 3=85.2000, 4=85.6000, 5=61.2000, 6=82.0000, 7=80.8000, 8=70.8000, 9=94.4000, test:, losses=0.6880, top1=80.2000, 0=97.4000, 1=95.1000, 2=49.1000, 3=83.7000, 4=88.9000, 5=58.9000, 6=84.5000, 7=80.6000, 8=71.3000, 9=92.5000, 52.0s 42.9m/2.9h
Epoch: [49]
epoch 49/200, train:, losses=0.3253, top1=85.4511, 0=87.0667, 1=92.8889, 2=79.6000, 3=74.1111, 4=85.2000, 5=76.4667, 6=88.9111, 7=88.1778, 8=91.3333, 9=90.7556, val:, losses=0.4272, top1=86.3000, 0=94.6000, 1=91.4000, 2=80.6000, 3=78.4000, 4=79.8000, 5=69.8000, 6=85.6000, 7=94.4000, 8=96.2000, 9=92.2000, test:, losses=0.4365, top1=85.8200, 0=93.1000, 1=92.4000, 2=77.8000, 3=77.0000, 4=81.3000, 5=69.3000, 6=86.4000, 7=94.2000, 8=95.1000, 9=91.6000, 53.4s 43.8m/2.9h
Epoch: [50]
epoch 50/200, train:, losses=0.3254, top1=85.3822, 0=87.0444, 1=92.6667, 2=80.2000, 3=73.7111, 4=84.7333, 5=76.5556, 6=88.6222, 7=88.2222, 8=91.5556, 9=90.5111, val:, losses=0.4642, top1=85.2600, 0=81.8000, 1=94.2000, 2=92.0000, 3=84.6000, 4=86.8000, 5=70.2000, 6=80.8000, 7=82.8000, 8=92.8000, 9=86.6000, test:, losses=0.4636, top1=85.0500, 0=81.4000, 1=96.0000, 2=90.9000, 3=85.4000, 4=87.5000, 5=70.9000, 6=81.1000, 7=81.5000, 8=93.1000, 9=82.7000, 52.4s 44.7m/2.9h
Epoch: [51]
epoch 51/200, train:, losses=0.3214, top1=85.6111, 0=87.7778, 1=92.5111, 2=81.6667, 3=73.6667, 4=84.7333, 5=76.3333, 6=89.2000, 7=87.9333, 8=91.8444, 9=90.4444, val:, losses=0.3894, top1=87.4000, 0=80.4000, 1=90.0000, 2=79.6000, 3=70.6000, 4=85.2000, 5=89.4000, 6=94.4000, 7=93.0000, 8=94.4000, 9=97.0000, test:, losses=0.3914, top1=87.0100, 0=79.8000, 1=91.7000, 2=79.7000, 3=67.5000, 4=86.0000, 5=89.5000, 6=92.6000, 7=92.8000, 8=93.8000, 9=96.7000, 52.2s 45.6m/2.9h
Epoch: [52]
epoch 52/200, train:, losses=0.3185, top1=85.8311, 0=88.0667, 1=92.9111, 2=80.0000, 3=74.4444, 4=85.0000, 5=77.4667, 6=88.9333, 7=87.9556, 8=92.0000, 9=91.5333, val:, losses=0.3944, top1=87.4800, 0=86.6000, 1=95.6000, 2=89.0000, 3=63.8000, 4=91.6000, 5=77.4000, 6=89.4000, 7=95.0000, 8=93.4000, 9=93.0000, test:, losses=0.4063, top1=86.8800, 0=86.3000, 1=96.7000, 2=87.7000, 3=62.1000, 4=90.1000, 5=76.4000, 6=89.1000, 7=95.1000, 8=93.1000, 9=92.2000, 52.8s 46.5m/2.9h
Epoch: [53]
epoch 53/200, train:, losses=0.3159, top1=85.9022, 0=87.6222, 1=92.5111, 2=81.6222, 3=74.3333, 4=85.8667, 5=76.6889, 6=88.9111, 7=88.8222, 8=91.3333, 9=91.3111, val:, losses=0.5779, top1=82.5800, 0=88.2000, 1=85.8000, 2=93.6000, 3=62.0000, 4=81.2000, 5=75.4000, 6=80.2000, 7=89.4000, 8=97.4000, 9=72.6000, test:, losses=0.5891, top1=82.5200, 0=86.7000, 1=86.2000, 2=93.7000, 3=62.9000, 4=83.0000, 5=75.2000, 6=81.2000, 7=89.2000, 8=97.1000, 9=70.0000, 51.4s 47.3m/2.9h
Epoch: [54]
epoch 54/200, train:, losses=0.3157, top1=85.9622, 0=87.4667, 1=92.6667, 2=81.4222, 3=75.0444, 4=85.1778, 5=77.4667, 6=89.4444, 7=88.8667, 8=91.6222, 9=90.4444, val:, losses=0.4004, top1=87.0200, 0=89.8000, 1=88.6000, 2=80.6000, 3=63.4000, 4=91.0000, 5=91.6000, 6=89.8000, 7=85.4000, 8=96.2000, 9=93.8000, test:, losses=0.3971, top1=87.0800, 0=89.6000, 1=90.5000, 2=77.6000, 3=63.7000, 4=91.2000, 5=91.5000, 6=91.5000, 7=86.2000, 8=95.7000, 9=93.3000, 52.6s 48.2m/2.9h
Epoch: [55]
epoch 55/200, train:, losses=0.3160, top1=85.8778, 0=88.2444, 1=92.8444, 2=80.6444, 3=74.6444, 4=85.3778, 5=76.8444, 6=89.0000, 7=89.2667, 8=91.0222, 9=90.8889, val:, losses=0.4483, top1=85.9000, 0=85.4000, 1=83.6000, 2=75.8000, 3=78.8000, 4=90.6000, 5=74.6000, 6=86.4000, 7=96.2000, 8=89.4000, 9=98.2000, test:, losses=0.4473, top1=85.0800, 0=83.6000, 1=83.6000, 2=72.4000, 3=76.3000, 4=90.6000, 5=72.4000, 6=88.4000, 7=95.6000, 8=90.0000, 9=97.9000, 53.8s 49.1m/2.9h
Epoch: [56]
epoch 56/200, train:, losses=0.3176, top1=85.9089, 0=87.4667, 1=92.6222, 2=80.7556, 3=74.7111, 4=85.6889, 5=77.9111, 6=89.2444, 7=88.7111, 8=91.2222, 9=90.7556, val:, losses=0.5500, top1=83.5200, 0=87.4000, 1=82.2000, 2=87.0000, 3=63.8000, 4=86.0000, 5=67.0000, 6=97.4000, 7=78.2000, 8=91.2000, 9=95.0000, test:, losses=0.5302, top1=83.3800, 0=88.2000, 1=82.7000, 2=83.8000, 3=65.1000, 4=87.7000, 5=64.2000, 6=97.8000, 7=78.3000, 8=90.9000, 9=95.1000, 54.1s 50.0m/2.9h
Epoch: [57]
epoch 57/200, train:, losses=0.3110, top1=86.1444, 0=87.5111, 1=93.1333, 2=81.3778, 3=74.9111, 4=85.1556, 5=77.0667, 6=90.0444, 7=88.7556, 8=92.1111, 9=91.3778, val:, losses=0.4310, top1=86.3400, 0=86.4000, 1=94.2000, 2=78.6000, 3=77.4000, 4=95.0000, 5=65.2000, 6=96.2000, 7=83.0000, 8=93.8000, 9=93.6000, test:, losses=0.4289, top1=86.7400, 0=87.2000, 1=95.9000, 2=76.8000, 3=79.4000, 4=95.3000, 5=67.5000, 6=96.9000, 7=82.6000, 8=93.4000, 9=92.4000, 53.6s 50.9m/2.9h
Epoch: [58]
epoch 58/200, train:, losses=0.3163, top1=85.8200, 0=87.2667, 1=92.3333, 2=80.9333, 3=74.9111, 4=84.9778, 5=77.4667, 6=89.4222, 7=88.7333, 8=91.4889, 9=90.6667, val:, losses=0.3775, top1=87.5000, 0=88.2000, 1=93.2000, 2=85.0000, 3=69.0000, 4=81.2000, 5=86.2000, 6=88.4000, 7=96.2000, 8=95.8000, 9=91.8000, test:, losses=0.3742, top1=87.5300, 0=89.5000, 1=95.5000, 2=83.7000, 3=65.7000, 4=83.3000, 5=87.8000, 6=89.4000, 7=94.4000, 8=94.3000, 9=91.7000, 54.3s 51.8m/2.9h
Epoch: [59]
epoch 59/200, train:, losses=0.3098, top1=86.3222, 0=87.9111, 1=92.9556, 2=81.0444, 3=75.1778, 4=86.0444, 5=78.2667, 6=90.0889, 7=88.6444, 8=92.1778, 9=90.9111, val:, losses=0.3557, top1=88.9000, 0=95.0000, 1=95.2000, 2=85.2000, 3=78.0000, 4=86.0000, 5=84.0000, 6=87.4000, 7=90.6000, 8=95.2000, 9=92.4000, test:, losses=0.3625, top1=88.4100, 0=95.2000, 1=96.7000, 2=82.3000, 3=77.7000, 4=85.6000, 5=84.4000, 6=85.7000, 7=91.1000, 8=95.2000, 9=90.2000, 53.8s 52.7m/2.9h
Epoch: [60]
epoch 60/200, train:, losses=0.2287, top1=89.8333, 0=92.0000, 1=94.6000, 2=85.2889, 3=81.2222, 4=89.0222, 5=83.2222, 6=93.0000, 7=91.7111, 8=94.7333, 9=93.5333, val:, losses=0.2644, top1=91.2600, 0=91.2000, 1=95.2000, 2=90.8000, 3=79.4000, 4=95.0000, 5=85.4000, 6=92.0000, 7=93.0000, 8=95.4000, 9=95.2000, test:, losses=0.2689, top1=91.0500, 0=90.3000, 1=96.3000, 2=89.1000, 3=80.1000, 4=94.7000, 5=85.3000, 6=91.8000, 7=93.0000, 8=94.7000, 9=95.2000, 51.9s 53.5m/2.9h
Epoch: [61]
epoch 61/200, train:, losses=0.2098, top1=90.5800, 0=91.9778, 1=95.6222, 2=86.8889, 3=81.9111, 4=89.9556, 5=83.8667, 6=93.1111, 7=93.1111, 8=95.2667, 9=94.0889, val:, losses=0.2597, top1=91.4200, 0=91.0000, 1=95.2000, 2=89.4000, 3=83.8000, 4=93.2000, 5=83.6000, 6=91.6000, 7=94.2000, 8=97.4000, 9=94.8000, test:, losses=0.2656, top1=91.4400, 0=90.5000, 1=97.1000, 2=87.5000, 3=84.8000, 4=93.8000, 5=84.3000, 6=91.8000, 7=93.5000, 8=96.8000, 9=94.3000, 52.2s 54.4m/2.9h
Epoch: [62]
epoch 62/200, train:, losses=0.2045, top1=90.7867, 0=92.8000, 1=95.5111, 2=87.0667, 3=81.6222, 4=90.8000, 5=83.9333, 6=93.4222, 7=92.8667, 8=95.0222, 9=94.8222, val:, losses=0.2524, top1=91.5200, 0=90.4000, 1=95.4000, 2=90.0000, 3=81.4000, 4=92.8000, 5=88.2000, 6=92.4000, 7=91.8000, 8=96.8000, 9=96.0000, test:, losses=0.2621, top1=91.0900, 0=88.9000, 1=96.2000, 2=86.9000, 3=80.2000, 4=94.5000, 5=87.3000, 6=92.6000, 7=91.9000, 8=96.5000, 9=95.9000, 52.4s 55.3m/2.9h
Epoch: [63]
epoch 63/200, train:, losses=0.1934, top1=91.2111, 0=93.1778, 1=95.2667, 2=88.5111, 3=82.6667, 4=91.3778, 5=84.5111, 6=93.9556, 7=93.2444, 8=95.2889, 9=94.1111, val:, losses=0.2517, top1=91.9000, 0=91.4000, 1=95.0000, 2=88.8000, 3=83.4000, 4=94.8000, 5=87.2000, 6=91.8000, 7=93.0000, 8=97.8000, 9=95.8000, test:, losses=0.2675, top1=91.6100, 0=90.9000, 1=96.7000, 2=85.8000, 3=82.6000, 4=95.3000, 5=86.9000, 6=92.2000, 7=93.7000, 8=96.1000, 9=95.9000, 53.6s 56.2m/2.9h
Epoch: [64]
epoch 64/200, train:, losses=0.1913, top1=91.3911, 0=93.3111, 1=96.0667, 2=87.6000, 3=83.4444, 4=91.5556, 5=84.5333, 6=93.4667, 7=93.4222, 8=95.6889, 9=94.8222, val:, losses=0.2508, top1=91.5600, 0=95.8000, 1=96.8000, 2=91.0000, 3=82.4000, 4=92.2000, 5=84.8000, 6=92.4000, 7=91.0000, 8=95.2000, 9=94.0000, test:, losses=0.2611, top1=91.5400, 0=94.3000, 1=97.5000, 2=88.4000, 3=82.6000, 4=93.1000, 5=85.7000, 6=93.5000, 7=92.0000, 8=94.4000, 9=93.9000, 50.7s 57.0m/2.9h
Epoch: [65]
epoch 65/200, train:, losses=0.1865, top1=91.5978, 0=93.8667, 1=95.6889, 2=88.3556, 3=83.2444, 4=91.6222, 5=84.8222, 6=94.4000, 7=93.4667, 8=95.6000, 9=94.9111, val:, losses=0.2718, top1=91.2600, 0=93.0000, 1=94.6000, 2=89.0000, 3=80.0000, 4=89.4000, 5=88.0000, 6=92.2000, 7=95.0000, 8=95.6000, 9=95.8000, test:, losses=0.2644, top1=91.4600, 0=92.2000, 1=95.9000, 2=86.4000, 3=80.9000, 4=90.9000, 5=89.1000, 6=93.6000, 7=94.5000, 8=96.1000, 9=95.0000, 53.1s 57.9m/2.9h
Epoch: [66]
epoch 66/200, train:, losses=0.1855, top1=91.5778, 0=92.7556, 1=95.9111, 2=88.8889, 3=82.9111, 4=91.2444, 5=85.8000, 6=93.8000, 7=93.6222, 8=95.9333, 9=94.9111, val:, losses=0.2713, top1=91.3800, 0=93.2000, 1=96.0000, 2=90.2000, 3=86.0000, 4=94.4000, 5=79.0000, 6=93.6000, 7=91.4000, 8=97.8000, 9=92.2000, test:, losses=0.2735, top1=91.2600, 0=91.0000, 1=97.1000, 2=88.4000, 3=86.5000, 4=95.4000, 5=79.5000, 6=94.0000, 7=92.0000, 8=97.1000, 9=91.6000, 53.6s 58.8m/2.9h
Epoch: [67]
epoch 67/200, train:, losses=0.1836, top1=91.7111, 0=93.3111, 1=96.0889, 2=88.2444, 3=83.8889, 4=91.4444, 5=85.3556, 6=94.1556, 7=93.6667, 8=95.6444, 9=95.3111, val:, losses=0.2643, top1=91.3800, 0=93.8000, 1=98.0000, 2=92.6000, 3=76.2000, 4=91.4000, 5=86.6000, 6=92.4000, 7=95.2000, 8=96.0000, 9=91.6000, test:, losses=0.2642, top1=91.6700, 0=93.9000, 1=98.1000, 2=91.2000, 3=78.1000, 4=91.1000, 5=88.4000, 6=93.7000, 7=94.5000, 8=96.0000, 9=91.7000, 52.8s 59.7m/2.9h
Epoch: [68]
epoch 68/200, train:, losses=0.1839, top1=91.7133, 0=93.4444, 1=96.1111, 2=89.0000, 3=83.2444, 4=91.5778, 5=85.5778, 6=94.2444, 7=93.1556, 8=95.5778, 9=95.2000, val:, losses=0.2708, top1=91.4800, 0=93.4000, 1=96.2000, 2=92.2000, 3=71.6000, 4=92.4000, 5=89.0000, 6=92.8000, 7=94.4000, 8=96.2000, 9=96.6000, test:, losses=0.2772, top1=91.1700, 0=92.3000, 1=96.9000, 2=89.3000, 3=72.0000, 4=93.2000, 5=89.2000, 6=93.3000, 7=94.5000, 8=95.6000, 9=95.4000, 51.6s 1.0h/2.9h
Epoch: [69]
epoch 69/200, train:, losses=0.1809, top1=91.9133, 0=93.6889, 1=95.9333, 2=89.3333, 3=83.6444, 4=91.8667, 5=86.0889, 6=94.4000, 7=93.3556, 8=95.7556, 9=95.0667, val:, losses=0.2716, top1=91.4000, 0=92.6000, 1=95.2000, 2=91.4000, 3=71.0000, 4=94.2000, 5=87.8000, 6=93.0000, 7=96.0000, 8=96.6000, 9=96.2000, test:, losses=0.2760, top1=91.4300, 0=91.9000, 1=96.5000, 2=90.4000, 3=71.5000, 4=93.3000, 5=89.1000, 6=94.1000, 7=96.2000, 8=96.0000, 9=95.3000, 52.5s 1.0h/2.9h
Epoch: [70]
epoch 70/200, train:, losses=0.1766, top1=92.0622, 0=93.4667, 1=96.2889, 2=89.2000, 3=84.8444, 4=91.9333, 5=86.2222, 6=94.4889, 7=93.4444, 8=95.8000, 9=94.9333, val:, losses=0.2542, top1=91.7400, 0=95.2000, 1=96.6000, 2=90.8000, 3=77.4000, 4=93.4000, 5=83.8000, 6=93.6000, 7=96.0000, 8=95.4000, 9=95.2000, test:, losses=0.2598, top1=91.7600, 0=94.5000, 1=97.2000, 2=89.4000, 3=80.4000, 4=91.7000, 5=86.4000, 6=94.6000, 7=94.7000, 8=94.9000, 9=93.8000, 53.1s 1.0h/2.9h
Epoch: [71]
epoch 71/200, train:, losses=0.1748, top1=92.1511, 0=93.7556, 1=96.0444, 2=88.6000, 3=84.4889, 4=92.2222, 5=86.4889, 6=94.5333, 7=94.2667, 8=96.0222, 9=95.0889, val:, losses=0.2655, top1=91.7200, 0=93.4000, 1=93.4000, 2=89.4000, 3=79.0000, 4=93.8000, 5=87.2000, 6=92.6000, 7=94.4000, 8=97.2000, 9=96.8000, test:, losses=0.2652, top1=91.6600, 0=93.7000, 1=95.3000, 2=88.4000, 3=78.9000, 4=95.2000, 5=87.0000, 6=92.2000, 7=94.1000, 8=95.9000, 9=95.9000, 54.0s 1.1h/2.9h
Epoch: [72]
epoch 72/200, train:, losses=0.1734, top1=92.1844, 0=93.8444, 1=95.9333, 2=89.2222, 3=84.8222, 4=92.0444, 5=85.7333, 6=94.8444, 7=93.7111, 8=96.0222, 9=95.6667, val:, losses=0.2647, top1=91.9400, 0=93.8000, 1=98.6000, 2=88.8000, 3=78.8000, 4=94.8000, 5=88.8000, 6=91.2000, 7=94.4000, 8=96.4000, 9=93.8000, test:, losses=0.2711, top1=91.3400, 0=92.2000, 1=98.3000, 2=86.7000, 3=79.5000, 4=95.4000, 5=89.3000, 6=90.6000, 7=93.8000, 8=94.4000, 9=93.2000, 51.8s 1.1h/2.9h
Epoch: [73]
epoch 73/200, train:, losses=0.1755, top1=92.0244, 0=93.6444, 1=95.7333, 2=89.4222, 3=83.8444, 4=92.1111, 5=85.4889, 6=94.9333, 7=93.7556, 8=96.1111, 9=95.2000, val:, losses=0.2552, top1=92.0600, 0=93.0000, 1=97.2000, 2=91.8000, 3=79.2000, 4=96.2000, 5=86.0000, 6=93.4000, 7=92.6000, 8=96.6000, 9=94.6000, test:, losses=0.2697, top1=91.1700, 0=93.2000, 1=98.2000, 2=88.7000, 3=78.0000, 4=96.1000, 5=84.1000, 6=92.6000, 7=92.5000, 8=95.4000, 9=92.9000, 53.5s 1.1h/2.9h
Epoch: [74]
epoch 74/200, train:, losses=0.1739, top1=92.2489, 0=93.5778, 1=96.0667, 2=90.3111, 3=85.1556, 4=91.8667, 5=86.4667, 6=94.6222, 7=93.5778, 8=95.7556, 9=95.0889, val:, losses=0.2589, top1=91.5400, 0=92.4000, 1=97.2000, 2=90.0000, 3=82.6000, 4=93.0000, 5=84.8000, 6=91.0000, 7=94.2000, 8=96.2000, 9=94.0000, test:, losses=0.2579, top1=91.6800, 0=92.4000, 1=97.8000, 2=89.8000, 3=84.1000, 4=93.3000, 5=85.6000, 6=90.9000, 7=94.5000, 8=95.1000, 9=93.3000, 53.2s 1.1h/2.9h
Epoch: [75]
epoch 75/200, train:, losses=0.1702, top1=92.2889, 0=93.4889, 1=96.1556, 2=89.2444, 3=84.8889, 4=92.3111, 5=86.4667, 6=94.7333, 7=94.2444, 8=96.0444, 9=95.3111, val:, losses=0.2542, top1=92.0800, 0=90.2000, 1=94.6000, 2=90.8000, 3=80.2000, 4=95.2000, 5=87.4000, 6=92.6000, 7=95.8000, 8=98.2000, 9=95.8000, test:, losses=0.2705, top1=91.5200, 0=90.2000, 1=96.6000, 2=88.7000, 3=79.1000, 4=93.9000, 5=87.6000, 6=91.8000, 7=95.2000, 8=97.0000, 9=95.1000, 51.9s 1.1h/2.9h
Epoch: [76]
epoch 76/200, train:, losses=0.1698, top1=92.2933, 0=93.7556, 1=96.1333, 2=89.3333, 3=84.7778, 4=91.9111, 5=86.6000, 6=94.9111, 7=94.3333, 8=95.9333, 9=95.2444, val:, losses=0.2745, top1=91.2800, 0=93.8000, 1=95.6000, 2=89.0000, 3=71.8000, 4=92.8000, 5=92.8000, 6=93.8000, 7=91.4000, 8=96.4000, 9=95.4000, test:, losses=0.2842, top1=91.0900, 0=94.2000, 1=96.7000, 2=87.0000, 3=73.4000, 4=93.3000, 5=91.9000, 6=92.8000, 7=91.2000, 8=95.5000, 9=94.9000, 51.9s 1.1h/2.9h
Epoch: [77]
epoch 77/200, train:, losses=0.1716, top1=92.2622, 0=93.4667, 1=95.9333, 2=88.5556, 3=84.9778, 4=92.4889, 5=86.8222, 6=94.7556, 7=94.1778, 8=96.0000, 9=95.4444, val:, losses=0.2622, top1=91.9800, 0=92.0000, 1=96.8000, 2=90.6000, 3=82.2000, 4=95.4000, 5=86.6000, 6=93.2000, 7=90.8000, 8=96.4000, 9=95.8000, test:, losses=0.2687, top1=91.7200, 0=92.8000, 1=97.2000, 2=89.4000, 3=80.7000, 4=95.6000, 5=86.0000, 6=93.7000, 7=91.1000, 8=95.8000, 9=94.9000, 53.1s 1.1h/2.9h
Epoch: [78]
epoch 78/200, train:, losses=0.1709, top1=92.3689, 0=93.7333, 1=96.3111, 2=89.4444, 3=85.4000, 4=92.0444, 5=86.2444, 6=94.9556, 7=94.1778, 8=95.7333, 9=95.6444, val:, losses=0.2705, top1=91.7000, 0=93.0000, 1=94.0000, 2=94.4000, 3=80.6000, 4=93.6000, 5=82.2000, 6=92.4000, 7=94.0000, 8=98.6000, 9=94.2000, test:, losses=0.2734, top1=91.6500, 0=92.6000, 1=96.6000, 2=92.0000, 3=82.5000, 4=93.3000, 5=82.1000, 6=92.3000, 7=94.0000, 8=97.1000, 9=94.0000, 53.3s 1.2h/2.9h
Epoch: [79]
epoch 79/200, train:, losses=0.1652, top1=92.4689, 0=93.6000, 1=95.9556, 2=89.5556, 3=84.9333, 4=92.5778, 5=87.1111, 6=94.8000, 7=94.1333, 8=96.6889, 9=95.3333, val:, losses=0.2557, top1=92.1400, 0=94.6000, 1=96.4000, 2=90.8000, 3=85.2000, 4=94.4000, 5=80.2000, 6=92.4000, 7=96.0000, 8=95.8000, 9=95.6000, test:, losses=0.2712, top1=91.5200, 0=94.1000, 1=97.6000, 2=87.7000, 3=85.6000, 4=93.8000, 5=80.3000, 6=91.6000, 7=95.1000, 8=95.7000, 9=93.7000, 53.7s 1.2h/2.9h
Epoch: [80]
epoch 80/200, train:, losses=0.1681, top1=92.3111, 0=93.5778, 1=96.4444, 2=89.1111, 3=85.3778, 4=92.2000, 5=86.3333, 6=94.5333, 7=94.4667, 8=96.1111, 9=94.9556, val:, losses=0.2688, top1=91.6400, 0=92.6000, 1=94.4000, 2=90.6000, 3=82.4000, 4=94.2000, 5=81.0000, 6=94.2000, 7=94.0000, 8=98.0000, 9=95.0000, test:, losses=0.2799, top1=91.4600, 0=91.9000, 1=95.9000, 2=88.0000, 3=83.7000, 4=94.6000, 5=81.9000, 6=94.1000, 7=92.6000, 8=96.7000, 9=95.2000, 53.1s 1.2h/2.9h
Epoch: [81]
epoch 81/200, train:, losses=0.1656, top1=92.3289, 0=93.9333, 1=96.1556, 2=89.7333, 3=84.8222, 4=92.4000, 5=86.2222, 6=94.6667, 7=93.9333, 8=95.9556, 9=95.4667, val:, losses=0.2678, top1=91.9600, 0=94.2000, 1=97.0000, 2=91.2000, 3=77.2000, 4=93.2000, 5=85.6000, 6=93.6000, 7=95.6000, 8=96.4000, 9=95.6000, test:, losses=0.2770, top1=91.3500, 0=92.4000, 1=97.0000, 2=88.6000, 3=76.1000, 4=93.1000, 5=87.1000, 6=93.2000, 7=94.9000, 8=95.7000, 9=95.4000, 54.2s 1.2h/2.9h
Epoch: [82]
epoch 82/200, train:, losses=0.1669, top1=92.4956, 0=93.9333, 1=96.0000, 2=90.7111, 3=84.8667, 4=92.6000, 5=86.5111, 6=94.6444, 7=94.0444, 8=95.9556, 9=95.6889, val:, losses=0.2779, top1=91.4800, 0=94.2000, 1=96.8000, 2=88.2000, 3=86.4000, 4=92.8000, 5=82.0000, 6=86.6000, 7=95.4000, 8=97.0000, 9=95.4000, test:, losses=0.2819, top1=91.4400, 0=93.4000, 1=97.8000, 2=87.1000, 3=87.9000, 4=93.4000, 5=82.1000, 6=89.0000, 7=94.2000, 8=95.5000, 9=94.0000, 53.2s 1.2h/2.9h
Epoch: [83]
epoch 83/200, train:, losses=0.1670, top1=92.5356, 0=94.6667, 1=96.1111, 2=90.2000, 3=85.2667, 4=91.4667, 5=87.1778, 6=94.7333, 7=94.1111, 8=96.1778, 9=95.4444, val:, losses=0.2651, top1=91.4800, 0=92.6000, 1=98.2000, 2=90.2000, 3=76.0000, 4=91.8000, 5=87.6000, 6=92.4000, 7=97.2000, 8=95.8000, 9=93.0000, test:, losses=0.2620, top1=91.8200, 0=92.9000, 1=98.8000, 2=88.3000, 3=79.3000, 4=93.4000, 5=89.8000, 6=93.7000, 7=95.6000, 8=95.1000, 9=91.3000, 53.3s 1.2h/2.9h
Epoch: [84]
epoch 84/200, train:, losses=0.1641, top1=92.5600, 0=93.8000, 1=96.5556, 2=89.8444, 3=85.3333, 4=91.8667, 5=87.0222, 6=95.3778, 7=94.6222, 8=95.8222, 9=95.3556, val:, losses=0.2914, top1=91.0200, 0=94.0000, 1=96.0000, 2=92.0000, 3=76.6000, 4=92.0000, 5=82.2000, 6=94.0000, 7=93.4000, 8=95.6000, 9=94.4000, test:, losses=0.2897, top1=91.3100, 0=93.3000, 1=97.6000, 2=92.2000, 3=78.5000, 4=93.0000, 5=81.5000, 6=94.4000, 7=93.1000, 8=95.8000, 9=93.7000, 52.8s 1.2h/2.9h
Epoch: [85]
epoch 85/200, train:, losses=0.1660, top1=92.5178, 0=93.6222, 1=96.1333, 2=90.1778, 3=85.4444, 4=92.8000, 5=87.0889, 6=94.4889, 7=94.6000, 8=95.6889, 9=95.1333, val:, losses=0.2762, top1=91.4000, 0=90.6000, 1=97.8000, 2=90.0000, 3=79.6000, 4=91.4000, 5=86.2000, 6=94.0000, 7=95.8000, 8=95.8000, 9=92.8000, test:, losses=0.2824, top1=90.9700, 0=90.4000, 1=98.2000, 2=89.0000, 3=78.1000, 4=93.0000, 5=85.2000, 6=94.2000, 7=93.6000, 8=95.9000, 9=92.1000, 53.3s 1.3h/2.9h
Epoch: [86]
epoch 86/200, train:, losses=0.1642, top1=92.5667, 0=93.7778, 1=96.6000, 2=89.7333, 3=85.7778, 4=92.5333, 5=86.7111, 6=94.4667, 7=94.5556, 8=96.2444, 9=95.2667, val:, losses=0.2628, top1=91.7000, 0=93.8000, 1=95.6000, 2=87.4000, 3=87.2000, 4=94.2000, 5=81.2000, 6=92.4000, 7=94.6000, 8=95.8000, 9=94.8000, test:, losses=0.2773, top1=91.4400, 0=92.1000, 1=96.2000, 2=84.4000, 3=89.0000, 4=95.3000, 5=81.0000, 6=92.3000, 7=93.7000, 8=95.1000, 9=95.3000, 53.3s 1.3h/2.9h
Epoch: [87]
epoch 87/200, train:, losses=0.1668, top1=92.4733, 0=94.1556, 1=96.1778, 2=89.6222, 3=85.8444, 4=92.8444, 5=86.7778, 6=94.4222, 7=93.6889, 8=96.0444, 9=95.1556, val:, losses=0.2680, top1=91.8600, 0=94.0000, 1=92.8000, 2=89.0000, 3=82.4000, 4=94.0000, 5=85.0000, 6=94.4000, 7=94.2000, 8=96.8000, 9=96.0000, test:, losses=0.2669, top1=91.5700, 0=93.1000, 1=94.0000, 2=86.6000, 3=80.4000, 4=94.6000, 5=87.1000, 6=93.3000, 7=93.8000, 8=96.3000, 9=96.5000, 53.1s 1.3h/2.9h
Epoch: [88]
epoch 88/200, train:, losses=0.1598, top1=92.7711, 0=94.1778, 1=96.6000, 2=90.3333, 3=85.3556, 4=92.8222, 5=87.6667, 6=94.8444, 7=94.3111, 8=95.8889, 9=95.7111, val:, losses=0.2882, top1=91.4000, 0=96.6000, 1=92.8000, 2=90.8000, 3=83.6000, 4=93.8000, 5=80.4000, 6=90.4000, 7=92.8000, 8=97.2000, 9=95.6000, test:, losses=0.2963, top1=90.7900, 0=95.1000, 1=94.3000, 2=89.1000, 3=81.3000, 4=94.5000, 5=81.1000, 6=89.7000, 7=92.9000, 8=95.4000, 9=94.5000, 52.4s 1.3h/2.9h
Epoch: [89]
epoch 89/200, train:, losses=0.1619, top1=92.7067, 0=94.4444, 1=96.5111, 2=89.8222, 3=85.6222, 4=92.1556, 5=87.6444, 6=95.0222, 7=93.8444, 8=96.0444, 9=95.9556, val:, losses=0.2693, top1=91.5400, 0=93.8000, 1=93.2000, 2=91.8000, 3=77.4000, 4=90.0000, 5=86.8000, 6=93.6000, 7=95.4000, 8=96.0000, 9=97.4000, test:, losses=0.2767, top1=91.3800, 0=93.9000, 1=95.4000, 2=88.7000, 3=77.6000, 4=90.6000, 5=88.5000, 6=93.6000, 7=94.3000, 8=95.1000, 9=96.1000, 51.0s 1.3h/2.9h
Epoch: [90]
epoch 90/200, train:, losses=0.1601, top1=92.8667, 0=94.0889, 1=96.3333, 2=90.2444, 3=86.3111, 4=92.9556, 5=87.4667, 6=95.4000, 7=94.5111, 8=96.0889, 9=95.2667, val:, losses=0.2548, top1=92.2200, 0=94.2000, 1=95.6000, 2=90.6000, 3=81.6000, 4=93.8000, 5=87.8000, 6=92.0000, 7=94.2000, 8=96.2000, 9=96.2000, test:, losses=0.2677, top1=91.8200, 0=92.3000, 1=96.4000, 2=87.3000, 3=82.5000, 4=94.4000, 5=88.9000, 6=92.4000, 7=94.1000, 8=95.3000, 9=94.6000, 51.7s 1.3h/2.9h
Epoch: [91]
epoch 91/200, train:, losses=0.1603, top1=92.7400, 0=93.9333, 1=96.4222, 2=89.9778, 3=86.1556, 4=92.6000, 5=87.4667, 6=94.7556, 7=94.5556, 8=96.1778, 9=95.3556, val:, losses=0.2748, top1=91.6200, 0=92.4000, 1=95.8000, 2=88.2000, 3=81.4000, 4=96.4000, 5=82.4000, 6=93.0000, 7=94.0000, 8=96.0000, 9=96.6000, test:, losses=0.2825, top1=91.4100, 0=92.1000, 1=95.1000, 2=86.0000, 3=80.7000, 4=96.9000, 5=84.3000, 6=93.8000, 7=93.9000, 8=96.0000, 9=95.3000, 51.9s 1.3h/2.9h
Epoch: [92]
epoch 92/200, train:, losses=0.1603, top1=92.8444, 0=94.2000, 1=96.4000, 2=90.8444, 3=86.3111, 4=92.5111, 5=87.2000, 6=94.9778, 7=94.3111, 8=96.1556, 9=95.5333, val:, losses=0.2963, top1=91.1800, 0=94.2000, 1=97.0000, 2=91.0000, 3=86.8000, 4=95.2000, 5=77.2000, 6=90.8000, 7=90.8000, 8=95.0000, 9=93.8000, test:, losses=0.2928, top1=90.8800, 0=92.9000, 1=97.0000, 2=89.4000, 3=85.6000, 4=95.5000, 5=77.2000, 6=91.8000, 7=91.6000, 8=95.0000, 9=92.8000, 52.2s 1.4h/2.9h
Epoch: [93]
epoch 93/200, train:, losses=0.1647, top1=92.6200, 0=94.1111, 1=96.4444, 2=90.3111, 3=85.5111, 4=92.0889, 5=86.9333, 6=94.9333, 7=94.2667, 8=96.0000, 9=95.6000, val:, losses=0.2731, top1=92.0200, 0=90.6000, 1=98.2000, 2=90.8000, 3=84.6000, 4=93.0000, 5=84.6000, 6=90.8000, 7=96.0000, 8=96.8000, 9=94.8000, test:, losses=0.2828, top1=91.2600, 0=89.9000, 1=98.6000, 2=88.2000, 3=84.8000, 4=93.5000, 5=84.4000, 6=89.7000, 7=94.1000, 8=96.1000, 9=93.3000, 54.3s 1.4h/2.9h
Epoch: [94]
epoch 94/200, train:, losses=0.1598, top1=92.6533, 0=93.9111, 1=96.3778, 2=90.0000, 3=86.2444, 4=92.6889, 5=87.0889, 6=95.0667, 7=94.4000, 8=95.4889, 9=95.2667, val:, losses=0.3012, top1=91.1600, 0=97.0000, 1=94.8000, 2=90.2000, 3=80.0000, 4=89.0000, 5=85.0000, 6=91.2000, 7=92.8000, 8=96.4000, 9=95.2000, test:, losses=0.3009, top1=90.9500, 0=96.6000, 1=95.5000, 2=87.9000, 3=82.1000, 4=89.8000, 5=84.5000, 6=92.3000, 7=91.9000, 8=95.2000, 9=93.7000, 53.6s 1.4h/2.9h
Epoch: [95]
epoch 95/200, train:, losses=0.1648, top1=92.5356, 0=93.9778, 1=96.2222, 2=90.4667, 3=85.6667, 4=92.3556, 5=87.0444, 6=94.8667, 7=93.4667, 8=95.7778, 9=95.5111, val:, losses=0.2593, top1=91.8600, 0=93.2000, 1=96.8000, 2=89.6000, 3=78.8000, 4=94.4000, 5=86.2000, 6=94.6000, 7=94.8000, 8=95.8000, 9=94.4000, test:, losses=0.2664, top1=91.7700, 0=93.1000, 1=97.6000, 2=88.6000, 3=79.8000, 4=94.5000, 5=86.7000, 6=93.7000, 7=95.0000, 8=95.1000, 9=93.6000, 51.7s 1.4h/2.9h
Epoch: [96]
epoch 96/200, train:, losses=0.1610, top1=92.7622, 0=93.9778, 1=96.4667, 2=90.2444, 3=86.3333, 4=92.1778, 5=87.6889, 6=94.8667, 7=94.4444, 8=95.9333, 9=95.4889, val:, losses=0.2877, top1=91.3200, 0=95.8000, 1=96.6000, 2=90.2000, 3=84.8000, 4=88.2000, 5=83.6000, 6=91.4000, 7=93.8000, 8=94.8000, 9=94.0000, test:, losses=0.3052, top1=90.8200, 0=94.5000, 1=97.0000, 2=90.5000, 3=86.9000, 4=86.0000, 5=84.2000, 6=91.2000, 7=91.4000, 8=94.0000, 9=92.5000, 52.6s 1.4h/2.9h
Epoch: [97]
epoch 97/200, train:, losses=0.1656, top1=92.5311, 0=93.8889, 1=96.2889, 2=90.0444, 3=85.2222, 4=92.0667, 5=87.1333, 6=94.7111, 7=94.3333, 8=95.7778, 9=95.8444, val:, losses=0.2744, top1=91.6000, 0=94.8000, 1=97.8000, 2=91.6000, 3=82.8000, 4=91.2000, 5=83.2000, 6=92.4000, 7=94.4000, 8=93.6000, 9=94.2000, test:, losses=0.2898, top1=91.2900, 0=95.5000, 1=97.9000, 2=89.3000, 3=84.7000, 4=89.7000, 5=84.0000, 6=91.3000, 7=93.1000, 8=94.1000, 9=93.3000, 52.1s 1.4h/2.9h
Epoch: [98]
epoch 98/200, train:, losses=0.1563, top1=92.9467, 0=94.2000, 1=96.2222, 2=90.8000, 3=86.4000, 4=92.9778, 5=86.9111, 6=95.0444, 7=94.4444, 8=96.3111, 9=96.1556, val:, losses=0.2888, top1=90.9800, 0=93.8000, 1=95.4000, 2=94.2000, 3=76.0000, 4=92.8000, 5=84.2000, 6=91.0000, 7=93.0000, 8=95.0000, 9=94.4000, test:, losses=0.3007, top1=90.8000, 0=93.2000, 1=95.7000, 2=92.0000, 3=78.1000, 4=93.1000, 5=83.9000, 6=91.0000, 7=92.8000, 8=93.8000, 9=94.4000, 53.2s 1.4h/2.9h
Epoch: [99]
epoch 99/200, train:, losses=0.1619, top1=92.6822, 0=93.7333, 1=96.4667, 2=89.5333, 3=85.8889, 4=92.6667, 5=87.5778, 6=95.1333, 7=94.2000, 8=96.1333, 9=95.4889, val:, losses=0.2804, top1=91.7400, 0=90.6000, 1=95.0000, 2=93.6000, 3=83.0000, 4=95.6000, 5=83.8000, 6=87.2000, 7=96.0000, 8=94.6000, 9=98.0000, test:, losses=0.2839, top1=91.1300, 0=91.2000, 1=95.3000, 2=91.1000, 3=82.7000, 4=93.3000, 5=83.2000, 6=88.8000, 7=94.6000, 8=94.7000, 9=96.4000, 52.0s 1.5h/2.9h
Epoch: [100]
epoch 100/200, train:, losses=0.1595, top1=92.8844, 0=93.9556, 1=96.4889, 2=90.2667, 3=86.7333, 4=92.5778, 5=87.1333, 6=95.1778, 7=94.2889, 8=96.6000, 9=95.6222, val:, losses=0.3073, top1=90.8600, 0=88.2000, 1=90.8000, 2=90.8000, 3=77.0000, 4=92.8000, 5=87.0000, 6=94.8000, 7=94.0000, 8=95.6000, 9=97.6000, test:, losses=0.2982, top1=90.7700, 0=88.2000, 1=91.7000, 2=89.6000, 3=78.3000, 4=92.4000, 5=88.0000, 6=94.4000, 7=92.8000, 8=94.6000, 9=97.7000, 53.7s 1.5h/2.9h
Epoch: [101]
epoch 101/200, train:, losses=0.1645, top1=92.6244, 0=93.9333, 1=96.4667, 2=90.3556, 3=86.0667, 4=92.4889, 5=87.0444, 6=94.8889, 7=93.9333, 8=95.9111, 9=95.1556, val:, losses=0.3071, top1=91.3800, 0=90.4000, 1=93.0000, 2=89.6000, 3=76.0000, 4=94.8000, 5=87.2000, 6=91.0000, 7=97.8000, 8=97.0000, 9=97.0000, test:, losses=0.2954, top1=90.8600, 0=89.0000, 1=94.5000, 2=88.3000, 3=76.2000, 4=94.2000, 5=87.7000, 6=90.3000, 7=96.7000, 8=95.3000, 9=96.4000, 53.6s 1.5h/2.9h
Epoch: [102]
epoch 102/200, train:, losses=0.1609, top1=92.6600, 0=93.6667, 1=96.4667, 2=89.6444, 3=86.2889, 4=92.9333, 5=87.2889, 6=94.4889, 7=94.1111, 8=96.2667, 9=95.4444, val:, losses=0.3023, top1=91.0600, 0=87.0000, 1=94.0000, 2=90.6000, 3=74.4000, 4=95.8000, 5=86.0000, 6=95.6000, 7=94.2000, 8=97.4000, 9=95.6000, test:, losses=0.2953, top1=91.2600, 0=88.2000, 1=95.4000, 2=89.9000, 3=75.6000, 4=95.6000, 5=86.4000, 6=96.0000, 7=93.5000, 8=96.2000, 9=95.8000, 53.3s 1.5h/2.9h
Epoch: [103]
epoch 103/200, train:, losses=0.1593, top1=92.7689, 0=94.0889, 1=96.2667, 2=90.3556, 3=86.2667, 4=92.0889, 5=88.1556, 6=94.7333, 7=93.9778, 8=96.0667, 9=95.6889, val:, losses=0.2958, top1=91.0600, 0=91.8000, 1=90.6000, 2=91.8000, 3=76.6000, 4=91.8000, 5=88.2000, 6=93.8000, 7=91.8000, 8=98.2000, 9=96.0000, test:, losses=0.2987, top1=91.0500, 0=91.2000, 1=94.2000, 2=90.4000, 3=76.6000, 4=91.7000, 5=89.4000, 6=92.4000, 7=92.3000, 8=96.9000, 9=95.4000, 53.5s 1.5h/2.9h
Epoch: [104]
epoch 104/200, train:, losses=0.1584, top1=92.9178, 0=93.8889, 1=96.5333, 2=89.6667, 3=86.3333, 4=92.7111, 5=87.7111, 6=95.1111, 7=94.7333, 8=96.4000, 9=96.0889, val:, losses=0.2972, top1=91.1000, 0=88.6000, 1=94.6000, 2=91.4000, 3=85.2000, 4=91.8000, 5=81.0000, 6=95.0000, 7=93.2000, 8=95.0000, 9=95.2000, test:, losses=0.2926, top1=91.3300, 0=87.2000, 1=96.1000, 2=91.0000, 3=87.6000, 4=92.2000, 5=82.1000, 6=94.1000, 7=92.7000, 8=96.2000, 9=94.1000, 53.8s 1.5h/2.9h
Epoch: [105]
epoch 105/200, train:, losses=0.1584, top1=92.8356, 0=94.4444, 1=96.2000, 2=89.9556, 3=85.7111, 4=92.6222, 5=87.2222, 6=95.5333, 7=94.5778, 8=96.2667, 9=95.8222, val:, losses=0.2997, top1=91.2600, 0=90.2000, 1=93.6000, 2=91.0000, 3=87.2000, 4=92.6000, 5=80.2000, 6=93.0000, 7=92.6000, 8=96.2000, 9=96.0000, test:, losses=0.2959, top1=91.4200, 0=90.7000, 1=95.2000, 2=89.1000, 3=87.8000, 4=93.2000, 5=81.2000, 6=91.2000, 7=93.8000, 8=95.1000, 9=96.9000, 53.8s 1.6h/2.9h
Epoch: [106]
epoch 106/200, train:, losses=0.1575, top1=92.8800, 0=94.2000, 1=96.4222, 2=90.0667, 3=86.4889, 4=92.7111, 5=87.4444, 6=95.2222, 7=94.5333, 8=95.9111, 9=95.8000, val:, losses=0.2858, top1=91.5400, 0=89.6000, 1=96.2000, 2=91.2000, 3=86.2000, 4=95.2000, 5=84.0000, 6=88.8000, 7=91.8000, 8=96.8000, 9=95.6000, test:, losses=0.3033, top1=91.4900, 0=88.6000, 1=98.5000, 2=89.9000, 3=88.1000, 4=94.7000, 5=85.3000, 6=89.5000, 7=90.7000, 8=94.9000, 9=94.7000, 52.4s 1.6h/2.9h
Epoch: [107]
epoch 107/200, train:, losses=0.1611, top1=92.7311, 0=94.1111, 1=96.9556, 2=89.7778, 3=86.4222, 4=92.5778, 5=87.2000, 6=94.5556, 7=94.1333, 8=95.7778, 9=95.8000, val:, losses=0.2685, top1=91.8800, 0=89.0000, 1=96.0000, 2=91.2000, 3=80.2000, 4=95.0000, 5=82.8000, 6=95.4000, 7=96.2000, 8=96.8000, 9=96.2000, test:, losses=0.2910, top1=91.0600, 0=87.6000, 1=96.8000, 2=89.9000, 3=78.9000, 4=93.8000, 5=83.7000, 6=94.4000, 7=95.8000, 8=94.2000, 9=95.5000, 52.5s 1.6h/2.9h
Epoch: [108]
epoch 108/200, train:, losses=0.1597, top1=92.7911, 0=94.1333, 1=96.4222, 2=90.6000, 3=86.3333, 4=92.0667, 5=87.7556, 6=94.5333, 7=94.7111, 8=95.8889, 9=95.4667, val:, losses=0.2728, top1=91.7600, 0=93.6000, 1=95.0000, 2=89.8000, 3=87.2000, 4=94.4000, 5=81.0000, 6=94.6000, 7=90.8000, 8=94.0000, 9=97.2000, test:, losses=0.2876, top1=91.5000, 0=92.2000, 1=95.6000, 2=87.6000, 3=87.4000, 4=95.2000, 5=81.0000, 6=93.7000, 7=91.6000, 8=93.8000, 9=96.9000, 53.5s 1.6h/2.9h
Epoch: [109]
epoch 109/200, train:, losses=0.1602, top1=92.7556, 0=93.7778, 1=96.5333, 2=90.0000, 3=86.3333, 4=92.9333, 5=88.0667, 6=94.6889, 7=94.2444, 8=95.5556, 9=95.4222, val:, losses=0.2696, top1=91.5800, 0=93.8000, 1=92.2000, 2=89.0000, 3=84.8000, 4=92.8000, 5=83.2000, 6=95.8000, 7=92.2000, 8=95.2000, 9=96.8000, test:, losses=0.2851, top1=91.3300, 0=93.4000, 1=93.7000, 2=86.5000, 3=84.1000, 4=93.4000, 5=85.0000, 6=93.8000, 7=92.1000, 8=94.4000, 9=96.9000, 53.5s 1.6h/2.9h
Epoch: [110]
epoch 110/200, train:, losses=0.1570, top1=92.8178, 0=94.0889, 1=96.4000, 2=90.4222, 3=85.6667, 4=92.1111, 5=87.6667, 6=95.2889, 7=94.7556, 8=96.0667, 9=95.7111, val:, losses=0.2888, top1=91.4800, 0=89.2000, 1=97.4000, 2=91.4000, 3=88.2000, 4=92.6000, 5=82.4000, 6=93.0000, 7=91.0000, 8=96.2000, 9=93.4000, test:, losses=0.3066, top1=90.9700, 0=89.6000, 1=98.0000, 2=89.3000, 3=88.0000, 4=92.5000, 5=83.5000, 6=91.7000, 7=91.0000, 8=95.2000, 9=90.9000, 52.8s 1.6h/2.9h
Epoch: [111]
epoch 111/200, train:, losses=0.1573, top1=92.8689, 0=94.0444, 1=96.5333, 2=90.4444, 3=86.2889, 4=92.8667, 5=87.9778, 6=94.8889, 7=94.5111, 8=96.0000, 9=95.1333, val:, losses=0.2744, top1=91.9000, 0=93.2000, 1=94.2000, 2=87.6000, 3=81.8000, 4=94.0000, 5=87.4000, 6=93.0000, 7=96.0000, 8=97.4000, 9=94.4000, test:, losses=0.2903, top1=91.4300, 0=90.9000, 1=96.2000, 2=85.5000, 3=81.6000, 4=94.1000, 5=87.7000, 6=92.2000, 7=94.7000, 8=97.1000, 9=94.3000, 52.0s 1.6h/2.9h
Epoch: [112]
epoch 112/200, train:, losses=0.1505, top1=93.1311, 0=94.2667, 1=96.3778, 2=90.3556, 3=86.9333, 4=93.3778, 5=88.1556, 6=94.7778, 7=94.6889, 8=96.4222, 9=95.9556, val:, losses=0.3055, top1=91.0600, 0=92.4000, 1=96.6000, 2=86.8000, 3=87.0000, 4=93.0000, 5=79.0000, 6=95.2000, 7=93.6000, 8=93.8000, 9=93.2000, test:, losses=0.3189, top1=91.3100, 0=92.4000, 1=97.2000, 2=86.5000, 3=87.6000, 4=93.2000, 5=81.6000, 6=94.3000, 7=93.1000, 8=94.1000, 9=93.1000, 52.9s 1.7h/2.9h
Epoch: [113]
epoch 113/200, train:, losses=0.1597, top1=92.7444, 0=94.2667, 1=96.4444, 2=90.0444, 3=86.1556, 4=92.4222, 5=87.2667, 6=95.1111, 7=94.2222, 8=95.7778, 9=95.7333, val:, losses=0.2711, top1=91.8400, 0=89.2000, 1=97.0000, 2=90.6000, 3=81.2000, 4=92.6000, 5=85.6000, 6=95.2000, 7=95.4000, 8=96.6000, 9=95.0000, test:, losses=0.2844, top1=91.3700, 0=88.4000, 1=97.0000, 2=89.8000, 3=81.0000, 4=92.0000, 5=87.0000, 6=93.2000, 7=95.0000, 8=96.5000, 9=93.8000, 51.9s 1.7h/2.9h
Epoch: [114]
epoch 114/200, train:, losses=0.1581, top1=92.8044, 0=93.7556, 1=96.6000, 2=90.0667, 3=86.5111, 4=92.2444, 5=87.7111, 6=95.1333, 7=94.0667, 8=96.2222, 9=95.7333, val:, losses=0.2855, top1=91.4800, 0=95.4000, 1=95.8000, 2=82.4000, 3=81.0000, 4=89.2000, 5=90.4000, 6=95.8000, 7=95.6000, 8=93.6000, 9=95.6000, test:, losses=0.3113, top1=91.1200, 0=95.3000, 1=96.7000, 2=80.6000, 3=80.5000, 4=89.3000, 5=88.8000, 6=95.6000, 7=95.8000, 8=93.0000, 9=95.6000, 52.5s 1.7h/2.9h
Epoch: [115]
epoch 115/200, train:, losses=0.1591, top1=92.7511, 0=94.2000, 1=96.2000, 2=90.4667, 3=86.3333, 4=92.1333, 5=87.1556, 6=94.9333, 7=94.6000, 8=95.9333, 9=95.5556, val:, losses=0.2703, top1=91.6800, 0=89.0000, 1=96.8000, 2=92.4000, 3=84.4000, 4=94.0000, 5=82.4000, 6=91.6000, 7=94.4000, 8=98.0000, 9=93.8000, test:, losses=0.2850, top1=91.4000, 0=87.7000, 1=97.6000, 2=91.1000, 3=85.2000, 4=93.3000, 5=84.1000, 6=92.0000, 7=94.0000, 8=96.1000, 9=92.9000, 54.3s 1.7h/2.9h
Epoch: [116]
epoch 116/200, train:, losses=0.1556, top1=92.9667, 0=93.7778, 1=96.3333, 2=90.2000, 3=87.3111, 4=92.3111, 5=88.1556, 6=95.3111, 7=94.2000, 8=96.0444, 9=96.0222, val:, losses=0.2999, top1=91.1200, 0=89.4000, 1=95.0000, 2=90.4000, 3=83.4000, 4=94.2000, 5=86.4000, 6=87.4000, 7=93.8000, 8=96.0000, 9=95.2000, test:, losses=0.2932, top1=91.4800, 0=89.7000, 1=96.3000, 2=89.5000, 3=83.8000, 4=94.7000, 5=87.4000, 6=89.0000, 7=93.5000, 8=96.2000, 9=94.7000, 52.7s 1.7h/2.9h
Epoch: [117]
epoch 117/200, train:, losses=0.1556, top1=92.8978, 0=93.8667, 1=96.5556, 2=90.4000, 3=86.4889, 4=92.7333, 5=87.7333, 6=94.9111, 7=94.8000, 8=95.8444, 9=95.6444, val:, losses=0.2964, top1=91.5800, 0=89.4000, 1=94.4000, 2=87.0000, 3=89.0000, 4=94.8000, 5=81.8000, 6=91.2000, 7=95.8000, 8=98.2000, 9=94.2000, test:, losses=0.3128, top1=91.0500, 0=89.2000, 1=96.0000, 2=84.3000, 3=88.1000, 4=95.5000, 5=81.6000, 6=90.6000, 7=94.7000, 8=96.7000, 9=93.8000, 53.7s 1.7h/2.9h
Epoch: [118]
epoch 118/200, train:, losses=0.1623, top1=92.6289, 0=94.0667, 1=95.6667, 2=90.1778, 3=85.9111, 4=92.5556, 5=88.0667, 6=94.3556, 7=94.1111, 8=96.1333, 9=95.2444, val:, losses=0.2865, top1=91.9200, 0=93.2000, 1=98.0000, 2=86.6000, 3=76.0000, 4=95.0000, 5=87.8000, 6=94.4000, 7=97.4000, 8=97.8000, 9=93.0000, test:, losses=0.3117, top1=91.1300, 0=92.7000, 1=98.4000, 2=84.8000, 3=74.6000, 4=95.0000, 5=88.0000, 6=94.3000, 7=95.3000, 8=96.4000, 9=91.8000, 50.2s 1.7h/2.9h
Epoch: [119]
epoch 119/200, train:, losses=0.1582, top1=92.6689, 0=94.3778, 1=96.0889, 2=90.2667, 3=85.8222, 4=92.7556, 5=87.2667, 6=94.2000, 7=94.4444, 8=96.4667, 9=95.0000, val:, losses=0.2920, top1=91.2400, 0=91.0000, 1=94.0000, 2=85.6000, 3=83.0000, 4=93.2000, 5=86.6000, 6=94.4000, 7=92.4000, 8=96.2000, 9=96.0000, test:, losses=0.2978, top1=91.0400, 0=91.7000, 1=95.6000, 2=84.6000, 3=82.6000, 4=94.1000, 5=86.3000, 6=94.0000, 7=91.8000, 8=94.1000, 9=95.6000, 50.5s 1.8h/2.9h
Epoch: [120]
epoch 120/200, train:, losses=0.1276, top1=94.2511, 0=95.1556, 1=97.0222, 2=92.3111, 3=88.6444, 4=94.0889, 5=89.8667, 6=96.2000, 7=95.5778, 8=96.9111, 9=96.7333, val:, losses=0.2313, top1=92.8600, 0=94.8000, 1=96.6000, 2=92.2000, 3=80.8000, 4=93.8000, 5=89.6000, 6=93.2000, 7=95.6000, 8=97.2000, 9=94.8000, test:, losses=0.2516, top1=92.2300, 0=94.2000, 1=98.0000, 2=90.7000, 3=81.2000, 4=92.9000, 5=88.5000, 6=92.8000, 7=93.9000, 8=95.8000, 9=94.3000, 53.1s 1.8h/2.9h
Epoch: [121]
epoch 121/200, train:, losses=0.1174, top1=94.7267, 0=96.1556, 1=97.5556, 2=92.9111, 3=88.8000, 4=94.4667, 5=90.6000, 6=96.3556, 7=96.1333, 8=97.2889, 9=97.0000, val:, losses=0.2318, top1=92.9200, 0=93.8000, 1=96.8000, 2=91.6000, 3=83.4000, 4=94.2000, 5=86.2000, 6=94.2000, 7=96.4000, 8=97.0000, 9=95.6000, test:, losses=0.2456, top1=92.3900, 0=92.9000, 1=97.4000, 2=89.4000, 3=84.6000, 4=93.4000, 5=87.0000, 6=94.1000, 7=94.6000, 8=95.9000, 9=94.6000, 53.3s 1.8h/2.9h
Epoch: [122]
epoch 122/200, train:, losses=0.1163, top1=94.8022, 0=96.0222, 1=97.2667, 2=93.1778, 3=89.8889, 4=94.4889, 5=90.4667, 6=96.7333, 7=95.8222, 8=97.7556, 9=96.4000, val:, losses=0.2317, top1=92.9600, 0=94.2000, 1=96.0000, 2=91.8000, 3=85.2000, 4=94.8000, 5=86.4000, 6=93.6000, 7=94.8000, 8=97.6000, 9=95.2000, test:, losses=0.2466, top1=92.5000, 0=93.8000, 1=97.6000, 2=90.6000, 3=83.7000, 4=94.1000, 5=86.6000, 6=93.3000, 7=94.2000, 8=96.1000, 9=95.0000, 53.1s 1.8h/2.9h
Epoch: [123]
epoch 123/200, train:, losses=0.1099, top1=95.1511, 0=96.2000, 1=97.8000, 2=93.7111, 3=90.6222, 4=95.2000, 5=90.9111, 6=96.8444, 7=96.2000, 8=97.3111, 9=96.7111, val:, losses=0.2336, top1=93.0200, 0=95.0000, 1=96.8000, 2=92.6000, 3=83.0000, 4=95.8000, 5=85.0000, 6=94.4000, 7=94.6000, 8=96.6000, 9=96.4000, test:, losses=0.2470, top1=92.6400, 0=94.0000, 1=97.3000, 2=89.5000, 3=83.7000, 4=95.7000, 5=86.1000, 6=94.8000, 7=94.0000, 8=95.5000, 9=95.8000, 53.0s 1.8h/2.9h
Epoch: [124]
epoch 124/200, train:, losses=0.1075, top1=95.2244, 0=96.3556, 1=97.6667, 2=94.1778, 3=90.3111, 4=94.9111, 5=90.8889, 6=96.6222, 7=96.6889, 8=97.7111, 9=96.9111, val:, losses=0.2285, top1=92.9400, 0=94.0000, 1=97.0000, 2=91.4000, 3=87.8000, 4=94.2000, 5=85.2000, 6=93.6000, 7=93.8000, 8=96.4000, 9=96.0000, test:, losses=0.2481, top1=92.5200, 0=92.6000, 1=97.2000, 2=90.1000, 3=87.5000, 4=94.0000, 5=84.9000, 6=94.2000, 7=93.5000, 8=95.5000, 9=95.7000, 53.9s 1.8h/2.9h
Epoch: [125]
epoch 125/200, train:, losses=0.1055, top1=95.3000, 0=96.4667, 1=97.7556, 2=93.8000, 3=91.0000, 4=94.9111, 5=91.2889, 6=96.6000, 7=96.4222, 8=97.3778, 9=97.3778, val:, losses=0.2325, top1=93.1400, 0=95.4000, 1=96.0000, 2=91.0000, 3=84.2000, 4=94.8000, 5=88.4000, 6=93.6000, 7=96.2000, 8=96.8000, 9=95.0000, test:, losses=0.2510, top1=92.8600, 0=95.3000, 1=97.0000, 2=89.4000, 3=82.8000, 4=94.1000, 5=89.4000, 6=94.5000, 7=95.2000, 8=95.5000, 9=95.4000, 54.6s 1.8h/2.9h
Epoch: [126]
epoch 126/200, train:, losses=0.1062, top1=95.2778, 0=96.5333, 1=97.3111, 2=93.8000, 3=90.7333, 4=95.0222, 5=91.7778, 6=96.7333, 7=96.4000, 8=97.3778, 9=97.0889, val:, losses=0.2315, top1=93.3400, 0=94.6000, 1=97.6000, 2=92.2000, 3=85.4000, 4=94.8000, 5=86.4000, 6=94.2000, 7=95.6000, 8=97.2000, 9=95.4000, test:, losses=0.2461, top1=92.6900, 0=93.6000, 1=98.1000, 2=90.5000, 3=84.7000, 4=95.1000, 5=85.1000, 6=94.9000, 7=94.3000, 8=96.2000, 9=94.4000, 52.3s 1.9h/2.9h
Epoch: [127]
epoch 127/200, train:, losses=0.1065, top1=95.2467, 0=96.1556, 1=97.9778, 2=93.6667, 3=90.4889, 4=95.2000, 5=91.1333, 6=96.4000, 7=96.6222, 8=97.5778, 9=97.2444, val:, losses=0.2317, top1=93.3400, 0=95.0000, 1=98.0000, 2=91.8000, 3=85.6000, 4=95.8000, 5=85.8000, 6=93.4000, 7=96.0000, 8=96.4000, 9=95.6000, test:, losses=0.2449, top1=92.8800, 0=94.5000, 1=97.9000, 2=89.2000, 3=85.9000, 4=95.2000, 5=86.7000, 6=94.6000, 7=94.8000, 8=95.5000, 9=94.5000, 53.4s 1.9h/2.9h
Epoch: [128]
epoch 128/200, train:, losses=0.1032, top1=95.3689, 0=96.4444, 1=97.9556, 2=93.8444, 3=90.7111, 4=95.2000, 5=91.5778, 6=96.5333, 7=96.6444, 8=97.8000, 9=96.9778, val:, losses=0.2368, top1=93.2200, 0=94.4000, 1=96.6000, 2=92.4000, 3=83.6000, 4=95.2000, 5=88.4000, 6=93.0000, 7=96.0000, 8=97.2000, 9=95.4000, test:, losses=0.2513, top1=92.8300, 0=94.2000, 1=97.0000, 2=89.8000, 3=84.5000, 4=94.3000, 5=88.3000, 6=93.5000, 7=94.7000, 8=96.1000, 9=95.9000, 52.6s 1.9h/2.9h
Epoch: [129]
epoch 129/200, train:, losses=0.1018, top1=95.4000, 0=96.1778, 1=98.0444, 2=93.5556, 3=90.8000, 4=95.0889, 5=91.7556, 6=96.7556, 7=96.6222, 8=97.6444, 9=97.5556, val:, losses=0.2299, top1=93.2800, 0=94.0000, 1=97.2000, 2=92.4000, 3=83.4000, 4=95.2000, 5=87.0000, 6=95.0000, 7=95.8000, 8=97.2000, 9=95.6000, test:, losses=0.2478, top1=92.8000, 0=94.6000, 1=97.6000, 2=88.7000, 3=83.4000, 4=95.1000, 5=87.7000, 6=95.2000, 7=94.3000, 8=95.7000, 9=95.7000, 52.8s 1.9h/2.9h
Epoch: [130]
epoch 130/200, train:, losses=0.1021, top1=95.3333, 0=96.6667, 1=97.7111, 2=93.8000, 3=90.7333, 4=95.4000, 5=91.5111, 6=96.6000, 7=96.3333, 8=97.3778, 9=97.2000, val:, losses=0.2319, top1=93.4400, 0=94.8000, 1=98.2000, 2=92.6000, 3=84.4000, 4=95.8000, 5=86.4000, 6=93.8000, 7=96.8000, 8=96.8000, 9=94.8000, test:, losses=0.2518, top1=92.7100, 0=95.0000, 1=97.8000, 2=89.9000, 3=85.1000, 4=94.1000, 5=85.7000, 6=94.2000, 7=95.3000, 8=95.6000, 9=94.4000, 52.6s 1.9h/2.9h
Epoch: [131]
epoch 131/200, train:, losses=0.1046, top1=95.3400, 0=96.3778, 1=97.6444, 2=93.5111, 3=91.2222, 4=95.2444, 5=91.5556, 6=96.9333, 7=96.4889, 8=97.3333, 9=97.0889, val:, losses=0.2382, top1=93.0400, 0=93.0000, 1=98.2000, 2=92.0000, 3=83.2000, 4=94.4000, 5=87.4000, 6=92.8000, 7=97.2000, 8=97.0000, 9=95.2000, test:, losses=0.2483, top1=92.9400, 0=93.8000, 1=98.1000, 2=89.9000, 3=84.8000, 4=94.1000, 5=88.3000, 6=93.8000, 7=95.3000, 8=96.2000, 9=95.1000, 51.6s 1.9h/2.9h
Epoch: [132]
epoch 132/200, train:, losses=0.1002, top1=95.4889, 0=96.8889, 1=97.6222, 2=94.0889, 3=91.4444, 4=95.6000, 5=91.2889, 6=96.7778, 7=96.7111, 8=97.4000, 9=97.0667, val:, losses=0.2331, top1=93.4600, 0=94.4000, 1=98.0000, 2=92.2000, 3=82.8000, 4=95.2000, 5=88.6000, 6=94.4000, 7=97.4000, 8=96.6000, 9=95.0000, test:, losses=0.2475, top1=92.9000, 0=95.1000, 1=97.3000, 2=89.5000, 3=83.0000, 4=93.6000, 5=89.0000, 6=94.8000, 7=95.5000, 8=96.2000, 9=95.0000, 50.8s 1.9h/2.9h
Epoch: [133]
epoch 133/200, train:, losses=0.1003, top1=95.5311, 0=96.2889, 1=97.8889, 2=94.0222, 3=91.2444, 4=95.7333, 5=91.8000, 6=96.8222, 7=96.5556, 8=97.5778, 9=97.3778, val:, losses=0.2336, top1=93.0600, 0=93.4000, 1=96.0000, 2=92.8000, 3=84.4000, 4=95.2000, 5=87.2000, 6=94.0000, 7=95.0000, 8=97.0000, 9=95.6000, test:, losses=0.2538, top1=93.0000, 0=94.8000, 1=96.2000, 2=90.7000, 3=84.7000, 4=95.5000, 5=87.3000, 6=94.3000, 7=93.7000, 8=96.5000, 9=96.3000, 52.2s 2.0h/2.9h
Epoch: [134]
epoch 134/200, train:, losses=0.1009, top1=95.4356, 0=96.6222, 1=97.7333, 2=93.7111, 3=90.5556, 4=95.7333, 5=91.5778, 6=97.2222, 7=96.3556, 8=97.8222, 9=97.0222, val:, losses=0.2356, top1=93.1000, 0=93.4000, 1=96.8000, 2=92.6000, 3=82.8000, 4=94.4000, 5=87.8000, 6=94.8000, 7=95.8000, 8=97.6000, 9=95.0000, test:, losses=0.2504, top1=92.9500, 0=93.4000, 1=96.8000, 2=90.8000, 3=85.3000, 4=94.1000, 5=89.2000, 6=93.3000, 7=94.2000, 8=96.9000, 9=95.5000, 52.8s 2.0h/2.9h
Epoch: [135]
epoch 135/200, train:, losses=0.0997, top1=95.4311, 0=96.2667, 1=97.7111, 2=94.3778, 3=90.9556, 4=95.1778, 5=92.1778, 6=96.4889, 7=96.1111, 8=97.6000, 9=97.4444, val:, losses=0.2386, top1=93.0600, 0=94.4000, 1=95.8000, 2=91.4000, 3=83.2000, 4=94.8000, 5=89.0000, 6=94.2000, 7=95.4000, 8=97.0000, 9=95.4000, test:, losses=0.2552, top1=92.8700, 0=93.8000, 1=96.8000, 2=89.9000, 3=84.3000, 4=94.6000, 5=88.2000, 6=94.1000, 7=94.6000, 8=96.5000, 9=95.9000, 52.1s 2.0h/2.9h
Epoch: [136]
epoch 136/200, train:, losses=0.1008, top1=95.4822, 0=96.7778, 1=97.4444, 2=94.3111, 3=91.0000, 4=95.3556, 5=91.4444, 6=97.2889, 7=96.3556, 8=97.4889, 9=97.3556, val:, losses=0.2404, top1=93.0400, 0=94.2000, 1=96.8000, 2=92.0000, 3=81.6000, 4=95.0000, 5=87.8000, 6=94.8000, 7=95.4000, 8=97.6000, 9=95.2000, test:, losses=0.2533, top1=93.0500, 0=94.0000, 1=97.9000, 2=90.5000, 3=82.5000, 4=95.7000, 5=88.9000, 6=94.9000, 7=94.0000, 8=96.5000, 9=95.6000, 53.3s 2.0h/2.9h
Epoch: [137]
epoch 137/200, train:, losses=0.1007, top1=95.5200, 0=96.4222, 1=97.7556, 2=94.2222, 3=90.7333, 4=95.7333, 5=91.8222, 6=96.7556, 7=96.8444, 8=97.6222, 9=97.2889, val:, losses=0.2339, top1=93.1200, 0=94.2000, 1=96.2000, 2=91.2000, 3=84.2000, 4=94.2000, 5=86.4000, 6=94.8000, 7=97.6000, 8=96.8000, 9=95.6000, test:, losses=0.2515, top1=92.9400, 0=93.9000, 1=96.9000, 2=90.5000, 3=85.9000, 4=94.5000, 5=85.4000, 6=95.0000, 7=95.1000, 8=96.6000, 9=95.6000, 52.6s 2.0h/2.9h
Epoch: [138]
epoch 138/200, train:, losses=0.1008, top1=95.5289, 0=96.4222, 1=97.5778, 2=93.9111, 3=91.1556, 4=95.6444, 5=91.7778, 6=96.9333, 7=96.7778, 8=97.6667, 9=97.4222, val:, losses=0.2445, top1=92.7400, 0=93.8000, 1=97.8000, 2=90.8000, 3=84.2000, 4=94.4000, 5=87.0000, 6=94.6000, 7=94.4000, 8=95.4000, 9=95.0000, test:, losses=0.2578, top1=92.8300, 0=94.7000, 1=98.1000, 2=89.9000, 3=85.3000, 4=95.1000, 5=87.5000, 6=94.3000, 7=93.4000, 8=95.3000, 9=94.7000, 52.2s 2.0h/2.9h
Epoch: [139]
epoch 139/200, train:, losses=0.0996, top1=95.5467, 0=96.7111, 1=98.0667, 2=93.9111, 3=91.2889, 4=95.2222, 5=92.1556, 6=96.6000, 7=96.1778, 8=97.8444, 9=97.4889, val:, losses=0.2384, top1=93.2800, 0=95.6000, 1=97.2000, 2=92.4000, 3=85.6000, 4=95.0000, 5=85.6000, 6=94.2000, 7=95.0000, 8=97.0000, 9=95.2000, test:, losses=0.2544, top1=93.0100, 0=95.2000, 1=98.0000, 2=91.1000, 3=85.3000, 4=94.5000, 5=86.8000, 6=94.2000, 7=94.0000, 8=96.1000, 9=94.9000, 53.1s 2.1h/2.9h
Epoch: [140]
epoch 140/200, train:, losses=0.0942, top1=95.7222, 0=96.9111, 1=97.9556, 2=94.3333, 3=91.6222, 4=95.5778, 5=92.3333, 6=97.2889, 7=96.6222, 8=97.5111, 9=97.0667, val:, losses=0.2384, top1=93.1800, 0=93.6000, 1=96.4000, 2=92.8000, 3=82.8000, 4=94.6000, 5=88.0000, 6=94.0000, 7=96.6000, 8=97.2000, 9=95.8000, test:, losses=0.2575, top1=92.8300, 0=93.4000, 1=97.0000, 2=90.0000, 3=83.8000, 4=94.8000, 5=88.0000, 6=94.9000, 7=94.0000, 8=96.5000, 9=95.9000, 52.4s 2.1h/2.9h
Epoch: [141]
epoch 141/200, train:, losses=0.0977, top1=95.5111, 0=96.6667, 1=97.7778, 2=94.0222, 3=90.6889, 4=95.0222, 5=92.3111, 6=97.2444, 7=96.6667, 8=97.4222, 9=97.2889, val:, losses=0.2381, top1=93.2400, 0=93.0000, 1=97.0000, 2=92.8000, 3=84.4000, 4=94.6000, 5=86.6000, 6=95.2000, 7=96.6000, 8=97.0000, 9=95.2000, test:, losses=0.2545, top1=92.8300, 0=93.1000, 1=97.5000, 2=90.9000, 3=84.6000, 4=94.6000, 5=86.6000, 6=93.9000, 7=95.1000, 8=96.6000, 9=95.4000, 52.2s 2.1h/2.9h
Epoch: [142]
epoch 142/200, train:, losses=0.0978, top1=95.6822, 0=96.5333, 1=97.9556, 2=94.2222, 3=91.8222, 4=95.4222, 5=92.5333, 6=96.5778, 7=96.5778, 8=97.8222, 9=97.3556, val:, losses=0.2441, top1=92.9000, 0=94.0000, 1=96.8000, 2=92.2000, 3=82.6000, 4=94.0000, 5=87.2000, 6=94.6000, 7=95.6000, 8=97.0000, 9=95.0000, test:, losses=0.2535, top1=93.2200, 0=93.8000, 1=97.6000, 2=91.2000, 3=85.7000, 4=95.2000, 5=87.5000, 6=95.2000, 7=94.8000, 8=95.9000, 9=95.3000, 52.0s 2.1h/2.9h
Epoch: [143]
epoch 143/200, train:, losses=0.0939, top1=95.7956, 0=96.6000, 1=98.0222, 2=94.5111, 3=91.8889, 4=95.5556, 5=92.2889, 6=96.9556, 7=96.8667, 8=97.9333, 9=97.3333, val:, losses=0.2474, top1=92.9600, 0=93.8000, 1=96.4000, 2=92.4000, 3=81.4000, 4=94.2000, 5=88.8000, 6=95.2000, 7=96.0000, 8=96.6000, 9=94.8000, test:, losses=0.2593, top1=92.9000, 0=94.6000, 1=97.3000, 2=90.9000, 3=81.3000, 4=94.6000, 5=89.6000, 6=95.0000, 7=94.6000, 8=95.8000, 9=95.3000, 52.6s 2.1h/2.9h
Epoch: [144]
epoch 144/200, train:, losses=0.0967, top1=95.5956, 0=96.4444, 1=97.4889, 2=93.7333, 3=91.7778, 4=95.3556, 5=92.3778, 6=97.0667, 7=96.4000, 8=97.9778, 9=97.3333, val:, losses=0.2368, top1=93.3200, 0=93.4000, 1=97.4000, 2=92.6000, 3=84.8000, 4=94.8000, 5=87.4000, 6=95.2000, 7=94.8000, 8=97.0000, 9=95.8000, test:, losses=0.2550, top1=92.9200, 0=93.7000, 1=97.8000, 2=89.9000, 3=85.2000, 4=94.9000, 5=88.1000, 6=94.6000, 7=93.7000, 8=95.8000, 9=95.5000, 52.5s 2.1h/2.9h
Epoch: [145]
epoch 145/200, train:, losses=0.0925, top1=95.7511, 0=96.6889, 1=98.1333, 2=94.1333, 3=91.8444, 4=95.5111, 5=92.3778, 6=97.4444, 7=96.9333, 8=97.5778, 9=96.8667, val:, losses=0.2409, top1=92.9400, 0=94.6000, 1=97.0000, 2=90.2000, 3=82.0000, 4=95.4000, 5=87.8000, 6=94.0000, 7=96.4000, 8=97.2000, 9=94.8000, test:, losses=0.2565, top1=92.9500, 0=95.3000, 1=97.4000, 2=90.1000, 3=83.3000, 4=94.9000, 5=88.2000, 6=94.3000, 7=94.7000, 8=95.9000, 9=95.4000, 52.7s 2.1h/2.9h
Epoch: [146]
epoch 146/200, train:, losses=0.0955, top1=95.7311, 0=96.7556, 1=97.8889, 2=94.2667, 3=91.1111, 4=95.6444, 5=92.2444, 6=96.9778, 7=96.9111, 8=97.9778, 9=97.5333, val:, losses=0.2401, top1=93.2400, 0=93.8000, 1=97.6000, 2=91.2000, 3=84.0000, 4=95.0000, 5=88.0000, 6=95.0000, 7=96.0000, 8=97.0000, 9=94.8000, test:, losses=0.2542, top1=92.9600, 0=94.2000, 1=97.5000, 2=88.9000, 3=85.5000, 4=94.0000, 5=88.4000, 6=94.4000, 7=95.1000, 8=96.2000, 9=95.4000, 51.6s 2.2h/2.9h
Epoch: [147]
epoch 147/200, train:, losses=0.0937, top1=95.8133, 0=96.3778, 1=98.3778, 2=94.0222, 3=91.9556, 4=95.4667, 5=92.3778, 6=97.5333, 7=96.8444, 8=97.7556, 9=97.4222, val:, losses=0.2375, top1=93.1400, 0=93.4000, 1=96.8000, 2=90.8000, 3=85.6000, 4=94.8000, 5=87.2000, 6=94.8000, 7=95.6000, 8=97.2000, 9=95.2000, test:, losses=0.2568, top1=92.9800, 0=94.2000, 1=97.0000, 2=89.5000, 3=86.8000, 4=94.8000, 5=87.2000, 6=94.3000, 7=94.8000, 8=95.9000, 9=95.3000, 51.6s 2.2h/2.9h
Epoch: [148]
epoch 148/200, train:, losses=0.0927, top1=95.8133, 0=96.5333, 1=98.0222, 2=94.5333, 3=91.6222, 4=95.5111, 5=92.7111, 6=96.9111, 7=96.8444, 8=98.1111, 9=97.3333, val:, losses=0.2431, top1=93.0600, 0=94.8000, 1=95.6000, 2=91.8000, 3=79.6000, 4=95.0000, 5=89.4000, 6=96.2000, 7=96.0000, 8=96.8000, 9=95.4000, test:, losses=0.2609, top1=93.1300, 0=95.6000, 1=96.9000, 2=90.6000, 3=82.1000, 4=95.3000, 5=89.8000, 6=95.3000, 7=94.5000, 8=96.1000, 9=95.1000, 53.6s 2.2h/2.9h
Epoch: [149]
epoch 149/200, train:, losses=0.0973, top1=95.6333, 0=96.4444, 1=98.1333, 2=94.1333, 3=90.9778, 4=95.4667, 5=92.8667, 6=97.0222, 7=96.5333, 8=97.4000, 9=97.3556, val:, losses=0.2399, top1=93.3400, 0=93.2000, 1=96.0000, 2=91.6000, 3=84.2000, 4=95.2000, 5=88.2000, 6=95.6000, 7=96.4000, 8=97.6000, 9=95.4000, test:, losses=0.2558, top1=92.9800, 0=93.8000, 1=96.8000, 2=89.8000, 3=85.9000, 4=94.3000, 5=88.0000, 6=95.0000, 7=94.6000, 8=95.8000, 9=95.8000, 53.7s 2.2h/2.9h
Epoch: [150]
epoch 150/200, train:, losses=0.0918, top1=95.8822, 0=97.3778, 1=98.3556, 2=94.4889, 3=92.1778, 4=95.4667, 5=92.3333, 6=96.9111, 7=96.4444, 8=97.7778, 9=97.4889, val:, losses=0.2358, top1=93.3000, 0=93.2000, 1=97.2000, 2=91.6000, 3=83.6000, 4=94.4000, 5=88.0000, 6=95.6000, 7=96.6000, 8=97.4000, 9=95.4000, test:, losses=0.2586, top1=92.9100, 0=93.7000, 1=97.2000, 2=90.2000, 3=84.9000, 4=94.3000, 5=87.5000, 6=94.9000, 7=94.6000, 8=96.4000, 9=95.4000, 53.6s 2.2h/2.9h
Epoch: [151]
epoch 151/200, train:, losses=0.0973, top1=95.6422, 0=96.2889, 1=97.7556, 2=93.4222, 3=92.1333, 4=95.3333, 5=92.0889, 6=97.3111, 7=97.0222, 8=97.5556, 9=97.5111, val:, losses=0.2398, top1=93.4000, 0=94.8000, 1=97.6000, 2=92.6000, 3=86.8000, 4=94.6000, 5=86.2000, 6=94.8000, 7=94.6000, 8=96.8000, 9=95.2000, test:, losses=0.2597, top1=92.8900, 0=94.2000, 1=97.8000, 2=90.4000, 3=86.0000, 4=94.5000, 5=86.8000, 6=95.2000, 7=93.2000, 8=95.6000, 9=95.2000, 53.3s 2.2h/2.9h
Epoch: [152]
epoch 152/200, train:, losses=0.0932, top1=95.8800, 0=96.6444, 1=97.8889, 2=94.4667, 3=92.5111, 4=95.3778, 5=92.7778, 6=97.1556, 7=97.0222, 8=97.8000, 9=97.1556, val:, losses=0.2404, top1=93.1400, 0=93.2000, 1=97.0000, 2=90.8000, 3=85.6000, 4=95.0000, 5=87.0000, 6=95.2000, 7=95.0000, 8=97.2000, 9=95.4000, test:, losses=0.2639, top1=92.7700, 0=93.3000, 1=97.4000, 2=88.7000, 3=85.6000, 4=93.8000, 5=87.1000, 6=95.6000, 7=94.5000, 8=96.3000, 9=95.4000, 53.9s 2.2h/2.9h
Epoch: [153]
epoch 153/200, train:, losses=0.0937, top1=95.8489, 0=96.6889, 1=97.8000, 2=94.3778, 3=91.8889, 4=96.0444, 5=92.4444, 6=97.1778, 7=96.6222, 8=97.7556, 9=97.6889, val:, losses=0.2436, top1=93.1200, 0=93.6000, 1=96.4000, 2=90.6000, 3=85.6000, 4=94.6000, 5=87.4000, 6=94.6000, 7=95.6000, 8=96.8000, 9=96.0000, test:, losses=0.2631, top1=92.7200, 0=93.9000, 1=96.7000, 2=88.8000, 3=86.9000, 4=93.9000, 5=87.3000, 6=93.5000, 7=94.2000, 8=95.8000, 9=96.2000, 53.2s 2.3h/2.9h
Epoch: [154]
epoch 154/200, train:, losses=0.0929, top1=95.9733, 0=96.8444, 1=98.2000, 2=94.2667, 3=92.4667, 4=95.8000, 5=92.4444, 6=97.4889, 7=96.6000, 8=98.1333, 9=97.4889, val:, losses=0.2431, top1=93.3800, 0=93.4000, 1=97.0000, 2=93.4000, 3=82.6000, 4=95.0000, 5=87.6000, 6=95.0000, 7=96.0000, 8=97.6000, 9=96.2000, test:, losses=0.2573, top1=92.7900, 0=92.8000, 1=97.2000, 2=91.8000, 3=83.1000, 4=93.7000, 5=87.8000, 6=93.8000, 7=95.4000, 8=96.5000, 9=95.8000, 52.2s 2.3h/2.9h
Epoch: [155]
epoch 155/200, train:, losses=0.0916, top1=95.8444, 0=96.7556, 1=97.9111, 2=94.2889, 3=92.0222, 4=96.0444, 5=91.9556, 6=97.1778, 7=96.9556, 8=98.0222, 9=97.3111, val:, losses=0.2422, top1=93.0400, 0=92.6000, 1=96.8000, 2=93.0000, 3=84.6000, 4=93.8000, 5=87.4000, 6=94.4000, 7=95.4000, 8=96.8000, 9=95.6000, test:, losses=0.2627, top1=92.9500, 0=92.8000, 1=97.7000, 2=91.3000, 3=85.6000, 4=94.1000, 5=88.4000, 6=92.9000, 7=94.5000, 8=96.2000, 9=96.0000, 52.7s 2.3h/2.9h
Epoch: [156]
epoch 156/200, train:, losses=0.0915, top1=95.9533, 0=96.6889, 1=97.9556, 2=94.6000, 3=92.2667, 4=95.7111, 5=92.8222, 6=97.1333, 7=96.9111, 8=98.0000, 9=97.4444, val:, losses=0.2445, top1=93.1400, 0=94.4000, 1=97.4000, 2=92.2000, 3=83.0000, 4=95.4000, 5=86.8000, 6=94.8000, 7=95.4000, 8=97.0000, 9=95.0000, test:, losses=0.2586, top1=92.8600, 0=94.8000, 1=97.7000, 2=90.3000, 3=84.2000, 4=94.5000, 5=87.5000, 6=94.8000, 7=94.0000, 8=96.1000, 9=94.7000, 52.4s 2.3h/2.9h
Epoch: [157]
epoch 157/200, train:, losses=0.0918, top1=95.9000, 0=96.7333, 1=97.8889, 2=94.6667, 3=91.7111, 4=95.5778, 5=92.6667, 6=97.0222, 7=97.0667, 8=98.1556, 9=97.5111, val:, losses=0.2386, top1=93.2400, 0=95.4000, 1=97.2000, 2=92.8000, 3=84.0000, 4=95.0000, 5=86.8000, 6=95.0000, 7=94.4000, 8=96.8000, 9=95.0000, test:, losses=0.2583, top1=93.2500, 0=94.9000, 1=97.5000, 2=90.7000, 3=85.6000, 4=94.9000, 5=87.9000, 6=95.1000, 7=93.6000, 8=96.0000, 9=96.3000, 53.1s 2.3h/2.9h
Epoch: [158]
epoch 158/200, train:, losses=0.0927, top1=95.8889, 0=96.7778, 1=98.2889, 2=94.1333, 3=92.1111, 4=95.6444, 5=92.9333, 6=97.1778, 7=96.7778, 8=97.6222, 9=97.4222, val:, losses=0.2396, top1=93.2600, 0=95.8000, 1=96.6000, 2=92.6000, 3=83.2000, 4=95.4000, 5=86.8000, 6=94.6000, 7=95.2000, 8=97.2000, 9=95.2000, test:, losses=0.2568, top1=92.9700, 0=94.2000, 1=96.9000, 2=91.3000, 3=84.1000, 4=95.1000, 5=87.0000, 6=95.0000, 7=94.1000, 8=96.2000, 9=95.8000, 52.1s 2.3h/2.9h
Epoch: [159]
epoch 159/200, train:, losses=0.0919, top1=95.8489, 0=97.0889, 1=98.0444, 2=94.2000, 3=92.1111, 4=95.3333, 5=92.8222, 6=96.7778, 7=96.7778, 8=97.6667, 9=97.6667, val:, losses=0.2431, top1=92.9600, 0=93.6000, 1=96.8000, 2=92.8000, 3=83.2000, 4=93.8000, 5=86.8000, 6=95.6000, 7=95.2000, 8=97.2000, 9=94.6000, test:, losses=0.2550, top1=93.2700, 0=94.0000, 1=97.5000, 2=91.9000, 3=84.8000, 4=94.4000, 5=89.3000, 6=95.6000, 7=94.2000, 8=96.0000, 9=95.0000, 53.2s 2.3h/2.9h
Epoch: [160]
epoch 160/200, train:, losses=0.0859, top1=96.1044, 0=96.7778, 1=98.2444, 2=94.5778, 3=92.5333, 4=95.5333, 5=93.3111, 6=97.5333, 7=97.1111, 8=97.9111, 9=97.5111, val:, losses=0.2350, top1=93.2800, 0=93.8000, 1=96.4000, 2=92.6000, 3=85.0000, 4=95.4000, 5=86.4000, 6=94.4000, 7=95.6000, 8=97.6000, 9=95.6000, test:, losses=0.2512, top1=93.2200, 0=94.2000, 1=96.9000, 2=91.6000, 3=85.4000, 4=94.8000, 5=88.7000, 6=94.6000, 7=94.6000, 8=95.5000, 9=95.9000, 47.7s 2.4h/2.9h
Epoch: [161]
epoch 161/200, train:, losses=0.0870, top1=96.0778, 0=96.8000, 1=97.9333, 2=95.0444, 3=92.6222, 4=95.4222, 5=93.5778, 6=97.1333, 7=96.6889, 8=98.1556, 9=97.4000, val:, losses=0.2359, top1=93.5400, 0=94.8000, 1=96.8000, 2=92.0000, 3=86.4000, 4=94.4000, 5=87.2000, 6=94.2000, 7=96.8000, 8=97.6000, 9=95.2000, test:, losses=0.2524, top1=93.0800, 0=95.0000, 1=96.6000, 2=89.6000, 3=85.6000, 4=93.9000, 5=88.8000, 6=94.4000, 7=94.8000, 8=96.0000, 9=96.1000, 36.3s 2.4h/2.9h
Epoch: [162]
epoch 162/200, train:, losses=0.0858, top1=96.2422, 0=97.0889, 1=97.8667, 2=94.6667, 3=93.0222, 4=96.0667, 5=93.4889, 6=97.2000, 7=97.2667, 8=98.0000, 9=97.7556, val:, losses=0.2356, top1=93.3600, 0=95.2000, 1=97.0000, 2=92.6000, 3=84.4000, 4=95.2000, 5=86.4000, 6=94.6000, 7=96.6000, 8=96.6000, 9=95.0000, test:, losses=0.2514, top1=93.2500, 0=95.3000, 1=97.1000, 2=90.8000, 3=84.6000, 4=94.8000, 5=88.8000, 6=95.0000, 7=94.7000, 8=95.5000, 9=95.9000, 36.2s 2.4h/2.9h
Epoch: [163]
epoch 163/200, train:, losses=0.0831, top1=96.2822, 0=97.1556, 1=98.0667, 2=94.5556, 3=92.9556, 4=96.1778, 5=93.3111, 6=97.7333, 7=97.3778, 8=97.9111, 9=97.5778, val:, losses=0.2358, top1=93.3800, 0=95.0000, 1=97.0000, 2=93.4000, 3=83.8000, 4=95.2000, 5=87.4000, 6=93.6000, 7=96.0000, 8=97.6000, 9=94.8000, test:, losses=0.2531, top1=93.2400, 0=94.8000, 1=97.4000, 2=91.5000, 3=84.3000, 4=94.9000, 5=89.1000, 6=94.2000, 7=94.7000, 8=96.5000, 9=95.0000, 38.0s 2.4h/2.9h
Epoch: [164]
epoch 164/200, train:, losses=0.0829, top1=96.3356, 0=97.2000, 1=98.1778, 2=95.3111, 3=92.6000, 4=96.4222, 5=93.2889, 6=97.7111, 7=97.0444, 8=98.1111, 9=97.4889, val:, losses=0.2327, top1=93.5000, 0=95.0000, 1=97.6000, 2=92.2000, 3=84.6000, 4=95.0000, 5=87.4000, 6=95.0000, 7=95.8000, 8=97.2000, 9=95.2000, test:, losses=0.2526, top1=93.1400, 0=94.6000, 1=97.7000, 2=90.4000, 3=84.7000, 4=94.8000, 5=87.6000, 6=95.0000, 7=94.9000, 8=95.8000, 9=95.9000, 38.1s 2.4h/2.9h
Epoch: [165]
epoch 165/200, train:, losses=0.0830, top1=96.2578, 0=97.2667, 1=98.5778, 2=95.1111, 3=92.5111, 4=95.8222, 5=93.4222, 6=97.1333, 7=96.9778, 8=98.2444, 9=97.5111, val:, losses=0.2325, top1=93.4200, 0=94.8000, 1=97.4000, 2=92.2000, 3=85.0000, 4=94.4000, 5=88.0000, 6=94.2000, 7=96.6000, 8=96.8000, 9=94.8000, test:, losses=0.2506, top1=93.2700, 0=94.6000, 1=97.5000, 2=90.5000, 3=84.7000, 4=95.0000, 5=89.1000, 6=94.4000, 7=95.0000, 8=96.3000, 9=95.6000, 38.8s 2.4h/2.9h
Epoch: [166]
epoch 166/200, train:, losses=0.0840, top1=96.2467, 0=97.2889, 1=97.9333, 2=94.5778, 3=93.6444, 4=95.8222, 5=93.3556, 6=97.4444, 7=97.0889, 8=97.7778, 9=97.5333, val:, losses=0.2353, top1=93.5200, 0=94.2000, 1=97.4000, 2=93.0000, 3=85.4000, 4=94.6000, 5=87.0000, 6=94.2000, 7=96.6000, 8=97.2000, 9=95.6000, test:, losses=0.2517, top1=93.1000, 0=94.0000, 1=97.7000, 2=91.0000, 3=85.3000, 4=94.5000, 5=87.5000, 6=94.6000, 7=94.9000, 8=95.8000, 9=95.7000, 38.3s 2.4h/2.9h
Epoch: [167]
epoch 167/200, train:, losses=0.0845, top1=96.2800, 0=97.0667, 1=98.1778, 2=94.7778, 3=92.9778, 4=95.9778, 5=93.7778, 6=97.0889, 7=97.2000, 8=98.2667, 9=97.4889, val:, losses=0.2320, top1=93.6600, 0=95.0000, 1=97.2000, 2=92.2000, 3=85.2000, 4=95.0000, 5=86.6000, 6=95.6000, 7=97.0000, 8=97.2000, 9=95.6000, test:, losses=0.2494, top1=93.2100, 0=94.2000, 1=97.5000, 2=90.1000, 3=85.5000, 4=94.6000, 5=87.5000, 6=95.3000, 7=95.3000, 8=96.3000, 9=95.8000, 37.8s 2.4h/2.9h
Epoch: [168]
epoch 168/200, train:, losses=0.0805, top1=96.3333, 0=96.9333, 1=97.9111, 2=95.1556, 3=92.7333, 4=96.4667, 5=93.6000, 6=97.4889, 7=97.0667, 8=98.3111, 9=97.6667, val:, losses=0.2343, top1=93.5800, 0=95.0000, 1=97.0000, 2=92.6000, 3=85.2000, 4=95.0000, 5=86.4000, 6=95.2000, 7=96.8000, 8=97.4000, 9=95.2000, test:, losses=0.2527, top1=93.1600, 0=94.3000, 1=97.5000, 2=90.6000, 3=85.4000, 4=95.2000, 5=87.0000, 6=94.1000, 7=95.2000, 8=96.6000, 9=95.7000, 38.1s 2.4h/2.9h
Epoch: [169]
epoch 169/200, train:, losses=0.0813, top1=96.3711, 0=97.3111, 1=98.3111, 2=94.7333, 3=92.9778, 4=96.4667, 5=92.9333, 6=97.4222, 7=97.2889, 8=98.1556, 9=98.1111, val:, losses=0.2342, top1=93.4400, 0=94.6000, 1=97.6000, 2=92.2000, 3=84.8000, 4=94.6000, 5=87.6000, 6=94.6000, 7=96.6000, 8=97.0000, 9=94.8000, test:, losses=0.2515, top1=93.1200, 0=94.8000, 1=97.7000, 2=90.2000, 3=84.8000, 4=94.2000, 5=89.0000, 6=94.2000, 7=95.1000, 8=96.1000, 9=95.1000, 37.8s 2.5h/2.9h
Epoch: [170]
epoch 170/200, train:, losses=0.0819, top1=96.4222, 0=97.3333, 1=98.0000, 2=95.4667, 3=92.9556, 4=96.3333, 5=93.6000, 6=97.4889, 7=97.1778, 8=98.1111, 9=97.7556, val:, losses=0.2345, top1=93.5200, 0=94.2000, 1=97.6000, 2=92.2000, 3=85.4000, 4=95.4000, 5=86.0000, 6=94.2000, 7=97.2000, 8=97.2000, 9=95.8000, test:, losses=0.2554, top1=92.9900, 0=93.8000, 1=97.0000, 2=90.1000, 3=85.4000, 4=94.9000, 5=87.2000, 6=94.3000, 7=95.6000, 8=96.1000, 9=95.5000, 29.9s 2.5h/2.9h
Epoch: [171]
epoch 171/200, train:, losses=0.0798, top1=96.4489, 0=97.6222, 1=98.3778, 2=94.9778, 3=93.1111, 4=96.1333, 5=93.5556, 6=97.5111, 7=97.3333, 8=98.1778, 9=97.6889, val:, losses=0.2349, top1=93.4200, 0=94.4000, 1=97.4000, 2=92.0000, 3=84.0000, 4=95.4000, 5=87.2000, 6=95.0000, 7=96.8000, 8=97.2000, 9=94.8000, test:, losses=0.2520, top1=93.2900, 0=94.5000, 1=97.5000, 2=90.7000, 3=84.6000, 4=94.7000, 5=89.0000, 6=95.0000, 7=95.3000, 8=96.1000, 9=95.5000, 28.9s 2.5h/2.9h
Epoch: [172]
epoch 172/200, train:, losses=0.0828, top1=96.3156, 0=97.4222, 1=98.2889, 2=94.6889, 3=93.1333, 4=96.1111, 5=93.2667, 6=97.4222, 7=97.1556, 8=97.9111, 9=97.7556, val:, losses=0.2331, top1=93.4600, 0=94.2000, 1=97.4000, 2=91.8000, 3=84.6000, 4=95.4000, 5=86.8000, 6=94.6000, 7=96.6000, 8=97.8000, 9=95.4000, test:, losses=0.2524, top1=93.0200, 0=93.7000, 1=97.4000, 2=90.7000, 3=84.9000, 4=94.6000, 5=88.2000, 6=94.0000, 7=95.2000, 8=96.0000, 9=95.5000, 28.7s 2.5h/2.9h
Epoch: [173]
epoch 173/200, train:, losses=0.0812, top1=96.4311, 0=97.5111, 1=97.8667, 2=95.6889, 3=92.8000, 4=96.2667, 5=93.8222, 6=97.6889, 7=97.1111, 8=97.9556, 9=97.6000, val:, losses=0.2338, top1=93.4400, 0=94.4000, 1=97.6000, 2=92.0000, 3=84.6000, 4=95.2000, 5=86.4000, 6=94.4000, 7=96.6000, 8=98.2000, 9=95.0000, test:, losses=0.2526, top1=93.1900, 0=94.0000, 1=97.6000, 2=90.8000, 3=85.3000, 4=94.9000, 5=87.6000, 6=94.7000, 7=95.1000, 8=96.7000, 9=95.2000, 29.6s 2.5h/2.9h
Epoch: [174]
epoch 174/200, train:, losses=0.0801, top1=96.4467, 0=97.2222, 1=97.8889, 2=95.4889, 3=93.5333, 4=96.5111, 5=93.1556, 6=97.3111, 7=97.4000, 8=98.4889, 9=97.4667, val:, losses=0.2349, top1=93.4600, 0=94.4000, 1=97.2000, 2=92.0000, 3=84.6000, 4=95.2000, 5=87.8000, 6=94.8000, 7=96.2000, 8=97.2000, 9=95.2000, test:, losses=0.2534, top1=93.1300, 0=93.9000, 1=97.5000, 2=90.3000, 3=85.6000, 4=94.7000, 5=88.2000, 6=94.6000, 7=94.6000, 8=96.5000, 9=95.4000, 29.1s 2.5h/2.9h
Epoch: [175]
epoch 175/200, train:, losses=0.0814, top1=96.4178, 0=97.2000, 1=98.1778, 2=95.0667, 3=93.1778, 4=96.0222, 5=93.8444, 6=97.6000, 7=97.4222, 8=98.0667, 9=97.6000, val:, losses=0.2365, top1=93.5200, 0=94.8000, 1=97.2000, 2=92.6000, 3=85.0000, 4=94.8000, 5=86.6000, 6=95.0000, 7=96.6000, 8=97.2000, 9=95.4000, test:, losses=0.2513, top1=93.1900, 0=94.5000, 1=97.3000, 2=91.1000, 3=85.4000, 4=94.3000, 5=88.6000, 6=94.9000, 7=94.7000, 8=95.8000, 9=95.3000, 29.1s 2.5h/2.9h
Epoch: [176]
epoch 176/200, train:, losses=0.0804, top1=96.3311, 0=97.5111, 1=98.1111, 2=95.0889, 3=92.7556, 4=95.9778, 5=93.1111, 6=97.5778, 7=97.4889, 8=98.2000, 9=97.4889, val:, losses=0.2375, top1=93.3800, 0=95.2000, 1=97.2000, 2=92.6000, 3=85.0000, 4=94.4000, 5=86.4000, 6=94.8000, 7=96.8000, 8=96.6000, 9=94.8000, test:, losses=0.2544, top1=93.0800, 0=94.6000, 1=97.4000, 2=91.0000, 3=85.1000, 4=94.7000, 5=87.7000, 6=94.2000, 7=95.0000, 8=95.9000, 9=95.2000, 29.0s 2.5h/2.8h
Epoch: [177]
epoch 177/200, train:, losses=0.0795, top1=96.5289, 0=97.3556, 1=98.5111, 2=95.3333, 3=93.3111, 4=95.9556, 5=93.9556, 6=97.7333, 7=96.9556, 8=98.4222, 9=97.7556, val:, losses=0.2368, top1=93.4000, 0=94.4000, 1=97.2000, 2=92.4000, 3=85.0000, 4=94.0000, 5=86.6000, 6=95.2000, 7=97.2000, 8=97.0000, 9=95.0000, test:, losses=0.2541, top1=93.0500, 0=93.9000, 1=97.4000, 2=90.8000, 3=85.6000, 4=94.5000, 5=87.6000, 6=94.3000, 7=95.3000, 8=95.9000, 9=95.2000, 28.8s 2.5h/2.8h
Epoch: [178]
epoch 178/200, train:, losses=0.0804, top1=96.4800, 0=97.3333, 1=98.4000, 2=95.2667, 3=93.0000, 4=96.4444, 5=93.9556, 6=97.4000, 7=97.1111, 8=98.1111, 9=97.7778, val:, losses=0.2374, top1=93.4600, 0=93.8000, 1=97.0000, 2=92.6000, 3=85.2000, 4=95.4000, 5=86.8000, 6=94.8000, 7=96.8000, 8=97.2000, 9=95.0000, test:, losses=0.2556, top1=93.2600, 0=93.8000, 1=97.7000, 2=90.0000, 3=85.2000, 4=95.0000, 5=88.8000, 6=94.8000, 7=95.6000, 8=96.6000, 9=95.1000, 28.8s 2.5h/2.8h
Epoch: [179]
epoch 179/200, train:, losses=0.0806, top1=96.4667, 0=97.0667, 1=98.4222, 2=95.1778, 3=93.1333, 4=96.5333, 5=93.6222, 6=97.5778, 7=97.4222, 8=98.0000, 9=97.7111, val:, losses=0.2356, top1=93.4400, 0=94.8000, 1=97.0000, 2=92.6000, 3=84.6000, 4=96.2000, 5=87.0000, 6=93.8000, 7=96.0000, 8=97.6000, 9=94.8000, test:, losses=0.2582, top1=93.0500, 0=94.5000, 1=97.3000, 2=90.2000, 3=84.1000, 4=95.5000, 5=88.1000, 6=94.3000, 7=94.7000, 8=96.5000, 9=95.3000, 28.7s 2.5h/2.8h
Epoch: [180]
epoch 180/200, train:, losses=0.0818, top1=96.2800, 0=97.4667, 1=98.0667, 2=94.8889, 3=92.6000, 4=96.0222, 5=93.2889, 6=97.4889, 7=97.0889, 8=98.2667, 9=97.6222, val:, losses=0.2337, top1=93.4600, 0=94.4000, 1=97.6000, 2=92.6000, 3=85.6000, 4=95.6000, 5=86.4000, 6=94.8000, 7=96.0000, 8=97.0000, 9=94.6000, test:, losses=0.2554, top1=93.0700, 0=94.3000, 1=97.5000, 2=90.5000, 3=86.1000, 4=94.8000, 5=86.9000, 6=94.9000, 7=94.8000, 8=96.2000, 9=94.7000, 28.7s 2.5h/2.8h
Epoch: [181]
epoch 181/200, train:, losses=0.0807, top1=96.5089, 0=97.4000, 1=98.5333, 2=95.4667, 3=92.6222, 4=95.8222, 5=94.2222, 6=97.3333, 7=97.2667, 8=98.6222, 9=97.8000, val:, losses=0.2386, top1=93.4200, 0=94.2000, 1=97.2000, 2=93.2000, 3=84.2000, 4=95.2000, 5=87.4000, 6=94.0000, 7=96.4000, 8=96.6000, 9=95.8000, test:, losses=0.2550, top1=93.1400, 0=94.7000, 1=96.8000, 2=91.1000, 3=85.2000, 4=94.8000, 5=87.8000, 6=94.4000, 7=95.0000, 8=95.8000, 9=95.8000, 28.1s 2.5h/2.8h
Epoch: [182]
epoch 182/200, train:, losses=0.0769, top1=96.5000, 0=97.6222, 1=98.3778, 2=94.5556, 3=93.4667, 4=96.2222, 5=93.9556, 6=97.5778, 7=97.2889, 8=98.2889, 9=97.6444, val:, losses=0.2389, top1=93.3400, 0=94.8000, 1=97.2000, 2=92.8000, 3=83.6000, 4=95.2000, 5=87.4000, 6=93.8000, 7=97.0000, 8=96.6000, 9=95.0000, test:, losses=0.2557, top1=93.1900, 0=95.0000, 1=97.3000, 2=90.9000, 3=85.3000, 4=94.9000, 5=88.2000, 6=93.9000, 7=95.1000, 8=95.6000, 9=95.7000, 28.7s 2.6h/2.8h
Epoch: [183]
epoch 183/200, train:, losses=0.0801, top1=96.3867, 0=97.3333, 1=97.8889, 2=94.9111, 3=92.9778, 4=96.1556, 5=93.6889, 6=97.7333, 7=97.3556, 8=98.0667, 9=97.7556, val:, losses=0.2363, top1=93.4400, 0=94.4000, 1=97.8000, 2=92.8000, 3=84.2000, 4=94.8000, 5=87.0000, 6=94.6000, 7=96.4000, 8=97.2000, 9=95.2000, test:, losses=0.2536, top1=93.1400, 0=94.1000, 1=97.4000, 2=91.0000, 3=84.7000, 4=94.9000, 5=88.4000, 6=94.7000, 7=95.1000, 8=96.1000, 9=95.0000, 27.7s 2.6h/2.8h
Epoch: [184]
epoch 184/200, train:, losses=0.0784, top1=96.5622, 0=97.2000, 1=98.4889, 2=95.4444, 3=92.6000, 4=96.8667, 5=93.6222, 6=97.6667, 7=97.5111, 8=98.3111, 9=97.9111, val:, losses=0.2347, top1=93.4200, 0=94.2000, 1=97.6000, 2=92.4000, 3=84.0000, 4=94.8000, 5=87.6000, 6=94.8000, 7=96.0000, 8=97.0000, 9=95.8000, test:, losses=0.2543, top1=93.1000, 0=93.9000, 1=97.0000, 2=90.6000, 3=84.4000, 4=95.2000, 5=88.9000, 6=94.6000, 7=94.8000, 8=95.8000, 9=95.8000, 27.8s 2.6h/2.8h
Epoch: [185]
epoch 185/200, train:, losses=0.0772, top1=96.5356, 0=97.5333, 1=98.5333, 2=95.1333, 3=92.9778, 4=96.3556, 5=93.7556, 6=97.4667, 7=97.4444, 8=98.0222, 9=98.1333, val:, losses=0.2375, top1=93.3400, 0=93.8000, 1=97.4000, 2=92.8000, 3=84.0000, 4=94.6000, 5=87.4000, 6=94.6000, 7=96.8000, 8=97.0000, 9=95.0000, test:, losses=0.2530, top1=93.1700, 0=94.1000, 1=97.3000, 2=91.2000, 3=84.7000, 4=95.1000, 5=87.9000, 6=94.5000, 7=94.9000, 8=96.3000, 9=95.7000, 28.3s 2.6h/2.8h
Epoch: [186]
epoch 186/200, train:, losses=0.0770, top1=96.6533, 0=97.4889, 1=98.2000, 2=95.4222, 3=93.7778, 4=96.0444, 5=94.0667, 6=97.8889, 7=97.4222, 8=98.2667, 9=97.9556, val:, losses=0.2385, top1=93.3800, 0=94.8000, 1=97.2000, 2=92.0000, 3=83.8000, 4=95.4000, 5=88.0000, 6=94.4000, 7=96.4000, 8=97.0000, 9=94.8000, test:, losses=0.2565, top1=93.1800, 0=94.3000, 1=97.6000, 2=90.8000, 3=83.9000, 4=94.8000, 5=89.5000, 6=94.1000, 7=94.8000, 8=96.8000, 9=95.2000, 29.0s 2.6h/2.8h
Epoch: [187]
epoch 187/200, train:, losses=0.0794, top1=96.5067, 0=97.2444, 1=98.3111, 2=94.9778, 3=93.2222, 4=96.5556, 5=93.8000, 6=97.6000, 7=97.5111, 8=98.0222, 9=97.8222, val:, losses=0.2346, top1=93.3600, 0=94.4000, 1=97.4000, 2=92.0000, 3=83.6000, 4=94.8000, 5=87.4000, 6=95.4000, 7=96.6000, 8=97.0000, 9=95.0000, test:, losses=0.2535, top1=93.1200, 0=93.8000, 1=97.7000, 2=90.6000, 3=85.3000, 4=94.6000, 5=88.9000, 6=94.8000, 7=94.3000, 8=95.8000, 9=95.4000, 28.8s 2.6h/2.8h
Epoch: [188]
epoch 188/200, train:, losses=0.0780, top1=96.5600, 0=97.1111, 1=98.4222, 2=95.2444, 3=93.8667, 4=96.2000, 5=93.9333, 6=97.6889, 7=97.0889, 8=98.3333, 9=97.7111, val:, losses=0.2375, top1=93.4600, 0=94.2000, 1=97.2000, 2=92.8000, 3=84.4000, 4=95.8000, 5=86.2000, 6=94.8000, 7=96.8000, 8=97.2000, 9=95.2000, test:, losses=0.2571, top1=93.1200, 0=94.3000, 1=97.5000, 2=90.8000, 3=84.3000, 4=95.6000, 5=87.3000, 6=94.6000, 7=94.8000, 8=96.4000, 9=95.6000, 28.0s 2.6h/2.8h
Epoch: [189]
epoch 189/200, train:, losses=0.0809, top1=96.4489, 0=97.2000, 1=98.4000, 2=95.5778, 3=92.6444, 4=96.1333, 5=93.8667, 6=97.6000, 7=97.0222, 8=98.1778, 9=97.8667, val:, losses=0.2349, top1=93.3600, 0=94.0000, 1=97.6000, 2=91.4000, 3=84.0000, 4=95.2000, 5=87.4000, 6=95.0000, 7=96.6000, 8=97.4000, 9=95.0000, test:, losses=0.2558, top1=93.1500, 0=93.6000, 1=97.4000, 2=90.6000, 3=85.0000, 4=95.0000, 5=89.1000, 6=94.5000, 7=94.6000, 8=96.5000, 9=95.2000, 28.2s 2.6h/2.8h
Epoch: [190]
epoch 190/200, train:, losses=0.0781, top1=96.5333, 0=97.2667, 1=98.4667, 2=95.4000, 3=93.1333, 4=96.4667, 5=93.5778, 6=97.6889, 7=97.4000, 8=98.2222, 9=97.7111, val:, losses=0.2378, top1=93.4400, 0=94.2000, 1=97.4000, 2=92.6000, 3=84.0000, 4=95.4000, 5=87.4000, 6=94.8000, 7=96.4000, 8=97.2000, 9=95.0000, test:, losses=0.2554, top1=93.1800, 0=94.5000, 1=97.3000, 2=90.8000, 3=84.9000, 4=94.9000, 5=88.3000, 6=94.3000, 7=95.1000, 8=96.4000, 9=95.3000, 28.0s 2.6h/2.8h
Epoch: [191]
epoch 191/200, train:, losses=0.0815, top1=96.3689, 0=97.2667, 1=97.8222, 2=95.2889, 3=92.9111, 4=95.8667, 5=93.5333, 6=97.5556, 7=97.7333, 8=98.1111, 9=97.6000, val:, losses=0.2361, top1=93.4800, 0=94.0000, 1=97.6000, 2=92.4000, 3=85.2000, 4=95.2000, 5=86.8000, 6=95.2000, 7=96.0000, 8=97.2000, 9=95.2000, test:, losses=0.2558, top1=93.2000, 0=93.9000, 1=97.8000, 2=91.0000, 3=85.7000, 4=95.3000, 5=87.5000, 6=94.5000, 7=94.8000, 8=96.7000, 9=94.8000, 25.6s 2.6h/2.7h
Epoch: [192]
epoch 192/200, train:, losses=0.0787, top1=96.4778, 0=97.1778, 1=98.4000, 2=95.2000, 3=93.8222, 4=96.2000, 5=94.0889, 6=97.1333, 7=96.7333, 8=98.3556, 9=97.6667, val:, losses=0.2358, top1=93.3600, 0=94.2000, 1=97.4000, 2=92.4000, 3=84.2000, 4=95.4000, 5=86.8000, 6=95.0000, 7=95.8000, 8=97.2000, 9=95.2000, test:, losses=0.2551, top1=93.0700, 0=93.7000, 1=97.9000, 2=90.7000, 3=85.5000, 4=95.3000, 5=87.6000, 6=94.2000, 7=94.4000, 8=96.3000, 9=95.1000, 23.8s 2.6h/2.7h
Epoch: [193]
epoch 193/200, train:, losses=0.0796, top1=96.5333, 0=97.4889, 1=98.3111, 2=95.8667, 3=93.1556, 4=95.9556, 5=93.4667, 6=97.6667, 7=97.2667, 8=98.1111, 9=98.0444, val:, losses=0.2395, top1=93.6000, 0=94.2000, 1=97.0000, 2=92.8000, 3=84.8000, 4=95.6000, 5=86.6000, 6=95.2000, 7=96.2000, 8=97.4000, 9=96.2000, test:, losses=0.2569, top1=93.1500, 0=94.3000, 1=97.5000, 2=90.6000, 3=85.5000, 4=95.3000, 5=88.0000, 6=94.1000, 7=94.6000, 8=95.9000, 9=95.7000, 24.6s 2.6h/2.7h
Epoch: [194]
epoch 194/200, train:, losses=0.0791, top1=96.4911, 0=97.2444, 1=98.4667, 2=95.3556, 3=93.3111, 4=96.4667, 5=93.4667, 6=97.4889, 7=97.0000, 8=98.1333, 9=97.9778, val:, losses=0.2357, top1=93.4600, 0=94.6000, 1=97.8000, 2=92.0000, 3=84.8000, 4=95.0000, 5=85.6000, 6=95.4000, 7=96.6000, 8=97.4000, 9=95.4000, test:, losses=0.2530, top1=93.1300, 0=94.3000, 1=97.9000, 2=90.8000, 3=85.4000, 4=94.9000, 5=86.8000, 6=94.3000, 7=95.3000, 8=96.2000, 9=95.4000, 24.6s 2.6h/2.7h
Epoch: [195]
epoch 195/200, train:, losses=0.0788, top1=96.5111, 0=97.3556, 1=98.3111, 2=95.1333, 3=93.5333, 4=96.0444, 5=93.8000, 6=97.5556, 7=97.3333, 8=98.2444, 9=97.8000, val:, losses=0.2385, top1=93.5600, 0=94.4000, 1=97.6000, 2=92.8000, 3=83.2000, 4=95.6000, 5=87.6000, 6=95.6000, 7=96.4000, 8=96.8000, 9=95.6000, test:, losses=0.2544, top1=93.1900, 0=93.8000, 1=97.7000, 2=91.0000, 3=84.4000, 4=95.2000, 5=88.6000, 6=94.7000, 7=95.2000, 8=95.8000, 9=95.5000, 24.8s 2.7h/2.7h
Epoch: [196]
epoch 196/200, train:, losses=0.0789, top1=96.5733, 0=97.4222, 1=98.4444, 2=95.4667, 3=93.0222, 4=96.5556, 5=93.6000, 6=97.6889, 7=97.2667, 8=98.2889, 9=97.9778, val:, losses=0.2384, top1=93.4200, 0=94.6000, 1=97.4000, 2=92.6000, 3=83.6000, 4=95.8000, 5=87.0000, 6=95.2000, 7=96.2000, 8=97.0000, 9=94.8000, test:, losses=0.2542, top1=93.2200, 0=94.0000, 1=97.7000, 2=90.7000, 3=84.3000, 4=95.7000, 5=88.4000, 6=95.0000, 7=94.9000, 8=96.3000, 9=95.2000, 24.8s 2.7h/2.7h
Epoch: [197]
epoch 197/200, train:, losses=0.0804, top1=96.4467, 0=97.4444, 1=98.1556, 2=95.4000, 3=93.1333, 4=96.0667, 5=93.5556, 6=97.6444, 7=97.2444, 8=98.0444, 9=97.7778, val:, losses=0.2373, top1=93.4600, 0=94.4000, 1=97.2000, 2=92.0000, 3=82.8000, 4=95.6000, 5=87.8000, 6=95.8000, 7=96.4000, 8=97.2000, 9=95.4000, test:, losses=0.2559, top1=93.1300, 0=94.1000, 1=97.8000, 2=90.2000, 3=83.8000, 4=95.3000, 5=88.9000, 6=94.9000, 7=95.0000, 8=95.8000, 9=95.5000, 26.4s 2.7h/2.7h
Epoch: [198]
epoch 198/200, train:, losses=0.0768, top1=96.6533, 0=97.4444, 1=98.0444, 2=95.4000, 3=93.4000, 4=96.5556, 5=94.3111, 6=97.8667, 7=97.2222, 8=98.3111, 9=97.9778, val:, losses=0.2366, top1=93.5800, 0=94.4000, 1=97.6000, 2=92.4000, 3=84.2000, 4=95.8000, 5=87.0000, 6=95.0000, 7=97.2000, 8=97.2000, 9=95.0000, test:, losses=0.2591, top1=93.2600, 0=94.4000, 1=97.6000, 2=90.3000, 3=85.9000, 4=95.3000, 5=88.5000, 6=94.2000, 7=95.4000, 8=96.1000, 9=94.9000, 24.1s 2.7h/2.7h
Epoch: [199]
epoch 199/200, train:, losses=0.0765, top1=96.5800, 0=97.3778, 1=98.0667, 2=95.2000, 3=93.2222, 4=96.4000, 5=93.7556, 6=97.5333, 7=97.4667, 8=98.6000, 9=98.1778, val:, losses=0.2352, top1=93.5400, 0=94.6000, 1=97.6000, 2=92.8000, 3=85.0000, 4=95.2000, 5=86.2000, 6=95.2000, 7=96.8000, 8=97.0000, 9=95.0000, test:, losses=0.2557, top1=93.1600, 0=94.4000, 1=97.6000, 2=91.1000, 3=85.2000, 4=95.2000, 5=87.9000, 6=94.6000, 7=94.8000, 8=96.1000, 9=94.7000, 24.4s 2.7h/2.7h
Epoch: [200]
epoch 200/200, train:, losses=0.0772, top1=96.5044, 0=97.6222, 1=98.4000, 2=94.9556, 3=92.8000, 4=95.8889, 5=93.5556, 6=97.8667, 7=97.5111, 8=98.3556, 9=98.0889, val:, losses=0.2344, top1=93.5000, 0=94.8000, 1=97.0000, 2=92.2000, 3=83.4000, 4=95.2000, 5=88.0000, 6=95.0000, 7=96.6000, 8=97.4000, 9=95.4000, test:, losses=0.2543, top1=93.2800, 0=94.4000, 1=97.7000, 2=91.0000, 3=84.8000, 4=95.2000, 5=88.9000, 6=94.2000, 7=94.9000, 8=96.0000, 9=95.7000, 25.3s 2.7h/2.7h
