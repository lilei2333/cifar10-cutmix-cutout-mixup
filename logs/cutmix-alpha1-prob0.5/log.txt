Namespace(GPU_ids='0', alpha=1.0, batch_size=128, cutmix_prob=0.5, cutout=False, device='cuda', epochs=200, length=16, lr=0.1, mixup=False, momentum=0.9, n_holes=1, output_dir='./logs/cutmix-alpha1-prob0.5', print_freq=50, resume=None, save_every=10, seed=111, start_epoch=0, validation=True, weight_decay=0.0001, workers=4)
resnet32: #params=464.2K
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (linear): Linear(in_features=64, out_features=10, bias=True)
  )
)
CrossEntropyLoss()
Building dataset...
Start training
Epoch: [0]
epoch 0/200, train:, losses=1.5631, top1=27.6185, 0=29.6263, 1=39.8452, 2=14.6204, 3=16.2662, 4=23.8606, 5=29.9548, 6=39.9453, 7=27.9800, 8=47.8495, 9=38.3916, val:, losses=1.5002, top1=45.4400, 0=57.4000, 1=69.8000, 2=40.4000, 3=35.0000, 4=28.2000, 5=34.8000, 6=66.2000, 7=23.0000, 8=47.8000, 9=51.8000, test:, losses=1.4968, top1=44.8700, 0=62.2000, 1=68.1000, 2=39.7000, 3=30.9000, 4=30.3000, 5=34.6000, 6=61.6000, 7=22.3000, 8=50.4000, 9=48.6000, 41.6s 41.6s/2.3h
Epoch: [1]
epoch 1/200, train:, losses=1.2767, top1=42.2829, 0=50.9426, 1=65.8459, 2=32.7107, 3=24.3804, 4=33.4986, 5=48.1616, 6=64.3470, 7=57.7301, 8=62.2472, 9=62.0026, val:, losses=1.4317, top1=50.8800, 0=57.6000, 1=68.0000, 2=37.0000, 3=12.4000, 4=33.4000, 5=27.2000, 6=90.8000, 7=34.2000, 8=62.0000, 9=86.2000, test:, losses=1.4046, top1=51.6600, 0=60.4000, 1=70.0000, 2=35.5000, 3=12.4000, 4=34.4000, 5=29.0000, 6=92.0000, 7=35.0000, 8=63.6000, 9=84.3000, 41.4s 1.4m/2.3h
Epoch: [2]
epoch 2/200, train:, losses=1.1586, top1=49.1433, 0=62.1182, 1=77.3792, 2=43.7083, 3=34.3327, 4=50.6404, 5=55.2283, 6=70.8436, 7=65.2088, 8=72.2927, 9=75.6416, val:, losses=1.0715, top1=62.8800, 0=72.8000, 1=87.0000, 2=49.4000, 3=53.6000, 4=50.0000, 5=35.2000, 6=78.8000, 7=59.0000, 8=81.6000, 9=61.4000, test:, losses=1.0761, top1=62.7200, 0=73.9000, 1=86.5000, 2=47.6000, 3=53.5000, 4=49.6000, 5=38.4000, 6=79.7000, 7=58.9000, 8=83.9000, 9=55.2000, 42.2s 2.1m/2.3h
Epoch: [3]
epoch 3/200, train:, losses=1.0199, top1=55.8214, 0=69.2378, 1=84.2687, 2=50.5268, 3=43.0368, 4=57.9736, 5=56.6502, 6=74.3613, 7=68.5465, 8=79.5965, 9=81.6628, val:, losses=1.0573, top1=63.9200, 0=88.0000, 1=90.8000, 2=66.4000, 3=61.0000, 4=33.0000, 5=47.0000, 6=64.6000, 7=59.0000, 8=62.2000, 9=67.2000, test:, losses=1.0703, top1=62.6300, 0=86.5000, 1=92.2000, 2=66.4000, 3=60.8000, 4=31.5000, 5=44.0000, 6=62.7000, 7=57.2000, 8=63.0000, 9=62.0000, 40.9s 2.8m/2.3h
Epoch: [4]
epoch 4/200, train:, losses=0.9507, top1=59.5183, 0=74.4344, 1=88.5563, 2=54.8574, 3=50.1494, 4=66.1621, 5=62.0982, 6=77.6892, 7=75.9342, 8=83.2309, 9=84.2478, val:, losses=0.9163, top1=69.3800, 0=68.2000, 1=84.6000, 2=48.6000, 3=60.2000, 4=52.6000, 5=67.4000, 6=47.2000, 7=84.8000, 8=86.2000, 9=94.0000, test:, losses=0.9220, top1=68.3800, 0=70.5000, 1=88.4000, 2=45.5000, 3=58.7000, 4=52.5000, 5=66.3000, 6=42.9000, 7=83.2000, 8=83.6000, 9=92.2000, 41.7s 3.5m/2.3h
Epoch: [5]
epoch 5/200, train:, losses=0.8772, top1=63.0436, 0=74.4811, 1=89.5631, 2=59.6256, 3=53.9021, 4=71.6855, 5=63.6659, 6=80.2297, 7=78.3566, 8=85.0346, 9=85.9741, val:, losses=0.9852, top1=66.3000, 0=65.8000, 1=96.4000, 2=68.0000, 3=65.2000, 4=52.2000, 5=48.0000, 6=86.8000, 7=42.4000, 8=90.6000, 9=47.6000, test:, losses=1.0123, top1=65.6500, 0=69.9000, 1=97.1000, 2=63.6000, 3=65.5000, 4=51.0000, 5=46.1000, 6=84.8000, 7=41.9000, 8=92.9000, 9=43.7000, 42.2s 4.2m/2.3h
Epoch: [6]
epoch 6/200, train:, losses=0.8552, top1=64.4595, 0=80.5133, 1=91.9949, 2=65.6647, 3=59.2593, 4=74.6616, 5=65.2713, 6=82.4989, 7=80.4252, 8=86.8065, 9=86.7741, val:, losses=0.7578, top1=75.4400, 0=78.6000, 1=87.6000, 2=69.0000, 3=65.8000, 4=52.8000, 5=49.6000, 6=87.0000, 7=86.8000, 8=86.8000, 9=90.4000, test:, losses=0.7512, top1=75.1000, 0=78.4000, 1=89.2000, 2=67.1000, 3=65.7000, 4=55.2000, 5=49.8000, 6=85.7000, 7=84.3000, 8=86.9000, 9=88.7000, 42.5s 4.9m/2.3h
Epoch: [7]
epoch 7/200, train:, losses=0.8064, top1=67.0268, 0=80.0083, 1=92.1306, 2=68.6638, 3=62.2203, 4=77.3200, 5=68.4233, 6=82.6087, 7=82.5956, 8=89.3232, 9=88.4357, val:, losses=0.7090, top1=76.1200, 0=76.8000, 1=95.2000, 2=74.2000, 3=39.6000, 4=68.6000, 5=61.0000, 6=87.0000, 7=85.0000, 8=87.4000, 9=86.4000, test:, losses=0.7155, top1=76.3700, 0=77.8000, 1=95.2000, 2=73.9000, 3=41.9000, 4=70.4000, 5=63.1000, 6=88.7000, 7=80.6000, 8=88.6000, 9=83.5000, 43.2s 5.6m/2.3h
Epoch: [8]
epoch 8/200, train:, losses=0.7710, top1=68.0120, 0=83.3534, 1=91.9200, 2=70.9018, 3=63.5563, 4=77.8675, 5=69.0226, 6=84.2485, 7=83.4222, 8=89.5987, 9=89.5184, val:, losses=0.6152, top1=78.3800, 0=71.4000, 1=91.0000, 2=60.4000, 3=57.6000, 4=73.8000, 5=84.2000, 6=80.4000, 7=87.8000, 8=94.2000, 9=83.0000, test:, losses=0.6296, top1=78.9000, 0=73.6000, 1=93.0000, 2=62.0000, 3=58.8000, 4=77.6000, 5=84.1000, 6=79.2000, 7=86.7000, 8=95.1000, 9=78.9000, 41.9s 6.3m/2.3h
Epoch: [9]
epoch 9/200, train:, losses=0.7857, top1=67.5678, 0=84.1431, 1=92.6073, 2=72.8309, 3=65.6167, 4=79.6618, 5=71.5354, 6=86.4732, 7=84.1105, 8=91.1297, 9=89.9393, val:, losses=0.6243, top1=79.1400, 0=83.2000, 1=93.2000, 2=76.4000, 3=41.6000, 4=89.0000, 5=74.4000, 6=89.6000, 7=75.8000, 8=81.8000, 9=86.4000, test:, losses=0.6312, top1=79.2500, 0=83.4000, 1=91.8000, 2=74.8000, 3=44.4000, 4=87.0000, 5=76.5000, 6=89.8000, 7=76.9000, 8=83.3000, 9=84.6000, 41.0s 7.0m/2.3h
Epoch: [10]
epoch 10/200, train:, losses=0.7763, top1=68.4186, 0=84.2273, 1=92.4115, 2=73.6396, 3=65.3052, 4=80.3125, 5=72.8043, 6=86.3889, 7=85.8744, 8=91.3254, 9=89.8717, val:, losses=0.6121, top1=79.4200, 0=91.2000, 1=88.8000, 2=80.8000, 3=58.0000, 4=75.8000, 5=81.6000, 6=74.8000, 7=67.2000, 8=90.6000, 9=85.4000, test:, losses=0.6236, top1=79.0900, 0=90.8000, 1=91.2000, 2=81.6000, 3=57.9000, 4=75.4000, 5=80.2000, 6=72.8000, 7=67.1000, 8=91.8000, 9=82.1000, 42.2s 7.7m/2.3h
Epoch: [11]
epoch 11/200, train:, losses=0.7323, top1=70.0939, 0=85.5054, 1=93.0651, 2=75.2600, 3=68.2885, 4=81.0352, 5=72.3423, 6=88.1046, 7=86.3259, 8=91.0806, 9=91.1149, val:, losses=0.5829, top1=80.8000, 0=91.2000, 1=95.8000, 2=73.6000, 3=74.0000, 4=85.8000, 5=59.6000, 6=79.2000, 7=84.0000, 8=84.4000, 9=80.4000, test:, losses=0.6084, top1=79.6300, 0=90.5000, 1=93.7000, 2=70.7000, 3=73.2000, 4=87.8000, 5=59.7000, 6=78.6000, 7=81.4000, 8=86.2000, 9=74.5000, 42.2s 8.4m/2.3h
Epoch: [12]
epoch 12/200, train:, losses=0.7294, top1=70.0743, 0=85.6964, 1=92.9297, 2=75.7498, 3=67.5325, 4=81.7689, 5=73.0179, 6=88.0182, 7=86.0009, 8=92.1788, 9=90.4242, val:, losses=0.5523, top1=82.0200, 0=89.0000, 1=95.6000, 2=77.8000, 3=65.6000, 4=62.8000, 5=78.4000, 6=85.6000, 7=90.4000, 8=92.4000, 9=82.6000, test:, losses=0.5601, top1=82.0000, 0=89.6000, 1=95.1000, 2=76.0000, 3=65.9000, 4=64.1000, 5=78.8000, 6=87.5000, 7=89.5000, 8=92.0000, 9=81.5000, 42.8s 9.1m/2.3h
Epoch: [13]
epoch 13/200, train:, losses=0.7251, top1=70.1897, 0=86.1456, 1=93.5133, 2=76.7615, 3=69.8878, 4=82.2686, 5=75.4898, 6=88.3111, 7=86.1903, 8=91.3270, 9=91.1855, val:, losses=0.5729, top1=80.5000, 0=76.0000, 1=86.6000, 2=79.4000, 3=56.0000, 4=88.4000, 5=71.0000, 6=90.0000, 7=70.2000, 8=90.4000, 9=97.0000, test:, losses=0.5709, top1=81.0400, 0=77.7000, 1=90.2000, 2=80.1000, 3=56.5000, 4=87.4000, 5=69.8000, 6=92.2000, 7=68.4000, 8=92.5000, 9=95.6000, 42.1s 9.8m/2.3h
Epoch: [14]
epoch 14/200, train:, losses=0.7389, top1=69.8449, 0=86.6455, 1=93.5558, 2=77.9298, 3=71.0466, 4=84.2733, 5=75.3150, 6=88.4328, 7=87.7072, 8=91.3801, 9=92.1149, val:, losses=0.5256, top1=82.0800, 0=84.4000, 1=93.2000, 2=86.4000, 3=66.6000, 4=80.8000, 5=69.8000, 6=67.8000, 7=90.4000, 8=93.0000, 9=88.4000, test:, losses=0.5415, top1=81.9700, 0=85.2000, 1=94.5000, 2=85.4000, 3=66.4000, 4=82.4000, 5=72.8000, 6=65.1000, 7=88.9000, 8=93.3000, 9=85.7000, 41.0s 10.5m/2.3h
Epoch: [15]
epoch 15/200, train:, losses=0.7210, top1=70.7198, 0=87.0204, 1=93.7069, 2=78.1653, 3=71.4423, 4=82.8984, 5=76.4168, 6=88.8376, 7=89.5261, 8=92.4221, 9=91.4963, val:, losses=0.5246, top1=83.2600, 0=89.0000, 1=96.4000, 2=63.6000, 3=60.6000, 4=86.0000, 5=80.4000, 6=90.4000, 7=90.0000, 8=84.8000, 9=91.4000, test:, losses=0.5370, top1=83.1000, 0=90.0000, 1=96.8000, 2=59.1000, 3=61.1000, 4=88.1000, 5=81.6000, 6=88.2000, 7=89.5000, 8=85.9000, 9=90.7000, 41.8s 11.2m/2.3h
Epoch: [16]
epoch 16/200, train:, losses=0.7142, top1=70.7120, 0=86.8867, 1=94.7131, 2=78.7976, 3=70.7584, 4=84.4077, 5=76.7593, 6=88.9232, 7=88.5752, 8=91.8259, 9=92.0215, val:, losses=0.5488, top1=81.7200, 0=78.2000, 1=87.4000, 2=89.8000, 3=53.6000, 4=79.4000, 5=73.6000, 6=87.8000, 7=82.0000, 8=88.2000, 9=97.2000, test:, losses=0.5510, top1=81.8700, 0=78.4000, 1=91.4000, 2=89.1000, 3=53.7000, 4=78.3000, 5=72.5000, 6=88.5000, 7=83.1000, 8=87.7000, 9=96.0000, 43.2s 11.9m/2.3h
Epoch: [17]
epoch 17/200, train:, losses=0.7095, top1=70.8199, 0=87.4943, 1=94.8540, 2=79.9444, 3=72.5970, 4=84.3985, 5=77.6722, 6=90.0905, 7=88.7846, 8=92.6559, 9=92.3043, val:, losses=0.5437, top1=81.5200, 0=92.6000, 1=93.2000, 2=73.6000, 3=84.0000, 4=75.4000, 5=61.8000, 6=70.2000, 7=83.2000, 8=92.2000, 9=89.0000, test:, losses=0.5644, top1=80.5000, 0=93.2000, 1=94.9000, 2=70.8000, 3=81.6000, 4=73.2000, 5=64.0000, 6=66.5000, 7=82.7000, 8=92.3000, 9=85.8000, 42.9s 12.6m/2.3h
Epoch: [18]
epoch 18/200, train:, losses=0.7322, top1=70.0603, 0=87.6549, 1=94.5254, 2=80.3414, 3=72.2138, 4=84.5577, 5=77.0189, 6=90.3996, 7=89.1194, 8=92.4327, 9=93.4217, val:, losses=0.5610, top1=82.4800, 0=80.6000, 1=94.2000, 2=73.2000, 3=56.4000, 4=94.2000, 5=68.2000, 6=90.6000, 7=85.6000, 8=94.2000, 9=87.6000, test:, losses=0.5639, top1=82.7100, 0=83.6000, 1=95.5000, 2=74.6000, 3=55.5000, 4=92.2000, 5=68.9000, 6=92.3000, 7=86.6000, 8=93.3000, 9=84.6000, 41.4s 13.3m/2.3h
Epoch: [19]
epoch 19/200, train:, losses=0.6888, top1=71.8218, 0=88.4528, 1=94.4493, 2=79.9458, 3=71.9190, 4=86.9604, 5=78.8889, 6=89.0997, 7=90.5018, 8=91.8662, 9=93.2889, val:, losses=0.5986, top1=79.8400, 0=66.0000, 1=95.6000, 2=79.8000, 3=56.2000, 4=73.6000, 5=90.6000, 6=83.8000, 7=71.6000, 8=87.6000, 9=93.6000, test:, losses=0.5833, top1=80.4400, 0=66.6000, 1=96.1000, 2=79.4000, 3=59.1000, 4=75.8000, 5=89.2000, 6=84.1000, 7=72.7000, 8=88.7000, 9=92.7000, 40.1s 14.0m/2.3h
Epoch: [20]
epoch 20/200, train:, losses=0.6656, top1=72.7554, 0=88.3433, 1=94.6963, 2=82.0766, 3=74.3830, 4=85.8714, 5=78.0928, 6=90.0382, 7=89.3333, 8=93.2022, 9=92.6085, val:, losses=0.5415, top1=81.3200, 0=79.2000, 1=95.2000, 2=67.4000, 3=52.0000, 4=82.6000, 5=89.0000, 6=85.0000, 7=86.0000, 8=88.4000, 9=88.4000, test:, losses=0.5299, top1=81.8200, 0=80.9000, 1=95.9000, 2=68.7000, 3=53.8000, 4=82.0000, 5=90.3000, 6=84.0000, 7=85.5000, 8=90.6000, 9=86.5000, 40.3s 14.6m/2.3h
Epoch: [21]
epoch 21/200, train:, losses=0.6459, top1=73.7520, 0=89.7257, 1=94.4756, 2=82.3477, 3=74.6046, 4=86.4317, 5=79.8965, 6=90.1272, 7=90.1468, 8=93.6731, 9=93.3478, val:, losses=0.5905, top1=80.3400, 0=89.4000, 1=94.4000, 2=79.6000, 3=80.6000, 4=58.0000, 5=58.2000, 6=93.2000, 7=85.2000, 8=87.2000, 9=77.6000, test:, losses=0.5844, top1=80.1800, 0=89.0000, 1=95.3000, 2=78.6000, 3=80.3000, 4=56.4000, 5=57.6000, 6=92.2000, 7=85.7000, 8=90.2000, 9=76.5000, 43.2s 15.4m/2.3h
Epoch: [22]
epoch 22/200, train:, losses=0.6577, top1=72.9599, 0=88.4889, 1=94.2998, 2=82.5175, 3=76.4292, 4=86.4138, 5=78.8143, 6=89.9332, 7=89.4967, 8=93.9134, 9=92.5444, val:, losses=0.4996, top1=83.7400, 0=90.2000, 1=98.0000, 2=81.2000, 3=63.2000, 4=82.0000, 5=64.2000, 6=90.2000, 7=94.4000, 8=88.4000, 9=85.6000, test:, losses=0.5081, top1=83.5900, 0=90.7000, 1=98.4000, 2=79.7000, 3=61.9000, 4=82.1000, 5=67.4000, 6=91.0000, 7=94.1000, 8=86.6000, 9=84.0000, 42.7s 16.1m/2.3h
Epoch: [23]
epoch 23/200, train:, losses=0.6318, top1=74.0523, 0=88.7215, 1=94.8866, 2=82.4434, 3=75.7392, 4=86.1406, 5=78.7951, 6=90.6492, 7=90.4161, 8=93.1745, 9=92.7423, val:, losses=0.4904, top1=83.9400, 0=78.2000, 1=96.8000, 2=64.6000, 3=72.0000, 4=91.6000, 5=86.4000, 6=81.0000, 7=83.4000, 8=94.4000, 9=91.0000, test:, losses=0.4869, top1=84.1300, 0=80.3000, 1=97.5000, 2=61.9000, 3=71.2000, 4=93.0000, 5=86.3000, 6=82.6000, 7=84.6000, 8=93.1000, 9=90.8000, 41.6s 16.8m/2.3h
Epoch: [24]
epoch 24/200, train:, losses=0.6617, top1=72.7228, 0=89.1372, 1=95.5369, 2=83.0702, 3=74.6168, 4=87.9357, 5=78.5394, 6=89.5906, 7=90.1790, 8=93.0201, 9=93.4382, val:, losses=0.4993, top1=83.6800, 0=85.8000, 1=93.8000, 2=88.6000, 3=76.4000, 4=77.0000, 5=56.2000, 6=90.8000, 7=86.4000, 8=96.2000, 9=85.6000, test:, losses=0.4981, top1=83.6300, 0=86.4000, 1=94.3000, 2=87.2000, 3=77.4000, 4=78.7000, 5=57.2000, 6=93.1000, 7=83.9000, 8=94.7000, 9=83.4000, 42.1s 17.5m/2.3h
Epoch: [25]
epoch 25/200, train:, losses=0.6376, top1=73.8530, 0=89.6269, 1=95.0997, 2=83.5273, 3=76.4967, 4=87.9015, 5=79.5272, 6=91.2626, 7=90.8734, 8=93.4448, 9=93.2143, val:, losses=0.5303, top1=82.7000, 0=90.0000, 1=93.4000, 2=64.2000, 3=68.2000, 4=70.4000, 5=80.0000, 6=94.4000, 7=84.0000, 8=92.0000, 9=90.4000, test:, losses=0.5406, top1=82.1400, 0=90.3000, 1=95.3000, 2=60.6000, 3=67.6000, 4=70.6000, 5=79.7000, 6=94.5000, 7=81.5000, 8=91.4000, 9=89.9000, 42.3s 18.2m/2.3h
Epoch: [26]
epoch 26/200, train:, losses=0.6131, top1=75.0081, 0=89.3740, 1=94.4811, 2=81.9611, 3=77.6976, 4=88.0524, 5=80.5579, 6=90.9091, 7=90.8372, 8=93.3605, 9=93.8282, val:, losses=0.4974, top1=83.7000, 0=83.2000, 1=90.6000, 2=74.0000, 3=73.4000, 4=89.8000, 5=66.8000, 6=95.8000, 7=80.4000, 8=93.4000, 9=89.6000, test:, losses=0.5068, top1=83.3700, 0=83.2000, 1=90.9000, 2=68.4000, 3=72.6000, 4=89.3000, 5=70.2000, 6=97.3000, 7=79.8000, 8=94.0000, 9=88.0000, 43.1s 18.9m/2.3h
Epoch: [27]
epoch 27/200, train:, losses=0.6688, top1=72.6180, 0=90.8563, 1=94.8300, 2=85.7607, 3=78.0511, 4=88.4652, 5=81.8611, 6=90.8749, 7=89.7634, 8=93.4331, 9=93.9619, val:, losses=0.4044, top1=86.8600, 0=84.2000, 1=92.8000, 2=88.0000, 3=66.8000, 4=87.6000, 5=85.4000, 6=84.0000, 7=89.6000, 8=96.0000, 9=94.2000, test:, losses=0.4190, top1=85.8800, 0=84.7000, 1=94.1000, 2=86.3000, 3=63.3000, 4=88.0000, 5=84.0000, 6=83.3000, 7=87.2000, 8=95.3000, 9=92.6000, 43.8s 19.6m/2.3h
Epoch: [28]
epoch 28/200, train:, losses=0.5829, top1=76.2113, 0=90.3839, 1=95.3349, 2=84.0096, 3=78.0150, 4=87.7179, 5=81.4251, 6=90.7807, 7=91.0615, 8=94.2440, 9=93.0658, val:, losses=0.5261, top1=82.5000, 0=71.0000, 1=90.6000, 2=74.0000, 3=61.0000, 4=85.6000, 5=89.4000, 6=74.0000, 7=90.2000, 8=94.8000, 9=94.4000, test:, losses=0.5211, top1=81.8200, 0=69.0000, 1=91.8000, 2=73.1000, 3=60.0000, 4=85.1000, 5=89.1000, 6=71.2000, 7=90.1000, 8=95.3000, 9=93.5000, 42.9s 20.3m/2.3h
Epoch: [29]
epoch 29/200, train:, losses=0.7056, top1=70.9539, 0=89.7669, 1=94.3469, 2=85.2477, 3=78.0311, 4=89.6362, 5=81.1909, 6=91.0947, 7=91.7272, 8=93.4462, 9=92.8732, val:, losses=0.5080, top1=82.7400, 0=82.8000, 1=89.8000, 2=78.0000, 3=56.2000, 4=80.6000, 5=89.6000, 6=81.0000, 7=78.8000, 8=92.8000, 9=97.8000, test:, losses=0.5143, top1=82.6600, 0=87.5000, 1=92.2000, 2=73.5000, 3=56.5000, 4=80.7000, 5=90.9000, 6=79.3000, 7=77.6000, 8=92.2000, 9=96.2000, 42.8s 21.1m/2.4h
Epoch: [30]
epoch 30/200, train:, losses=0.6275, top1=73.8338, 0=89.7684, 1=95.5041, 2=83.5470, 3=78.2886, 4=88.1334, 5=81.4619, 6=91.3376, 7=90.8206, 8=94.5486, 9=93.6714, val:, losses=0.5757, top1=80.6200, 0=72.2000, 1=86.0000, 2=72.6000, 3=72.0000, 4=87.0000, 5=89.2000, 6=77.4000, 7=74.8000, 8=89.4000, 9=85.6000, test:, losses=0.5728, top1=81.3000, 0=75.2000, 1=86.9000, 2=73.5000, 3=70.5000, 4=88.3000, 5=89.9000, 6=79.1000, 7=74.6000, 8=90.7000, 9=84.3000, 41.5s 21.7m/2.4h
Epoch: [31]
epoch 31/200, train:, losses=0.5987, top1=75.1082, 0=89.6803, 1=95.6612, 2=85.7556, 3=78.4728, 4=89.2252, 5=81.1813, 6=91.2409, 7=91.2829, 8=93.7858, 9=93.0963, val:, losses=0.4634, top1=85.0400, 0=95.0000, 1=92.0000, 2=66.2000, 3=77.2000, 4=87.4000, 5=81.6000, 6=90.4000, 7=74.0000, 8=94.0000, 9=92.6000, test:, losses=0.4646, top1=84.6500, 0=93.8000, 1=93.5000, 2=60.9000, 3=76.8000, 4=87.7000, 5=82.0000, 6=92.2000, 7=73.9000, 8=93.9000, 9=91.8000, 41.8s 22.4m/2.3h
Epoch: [32]
epoch 32/200, train:, losses=0.6270, top1=74.0551, 0=89.8718, 1=95.6163, 2=85.5761, 3=79.2411, 4=89.5972, 5=82.5417, 6=90.4000, 7=92.5023, 8=94.3396, 9=93.5353, val:, losses=0.5359, top1=82.0400, 0=80.2000, 1=87.6000, 2=70.4000, 3=54.2000, 4=94.6000, 5=88.4000, 6=76.2000, 7=81.4000, 8=96.0000, 9=91.4000, test:, losses=0.5317, top1=82.0400, 0=81.0000, 1=89.5000, 2=66.8000, 3=53.5000, 4=96.1000, 5=89.1000, 6=78.2000, 7=80.6000, 8=94.4000, 9=91.2000, 44.0s 23.2m/2.4h
Epoch: [33]
epoch 33/200, train:, losses=0.6527, top1=72.8977, 0=90.2588, 1=94.0734, 2=86.3808, 3=79.2734, 4=89.6825, 5=82.1937, 6=91.6905, 7=92.1002, 8=94.1945, 9=92.8132, val:, losses=0.4258, top1=86.7600, 0=91.2000, 1=92.8000, 2=73.2000, 3=75.8000, 4=94.2000, 5=77.0000, 6=84.6000, 7=92.2000, 8=95.8000, 9=90.8000, test:, losses=0.4357, top1=86.0700, 0=92.5000, 1=94.6000, 2=68.7000, 3=72.9000, 4=93.7000, 5=78.3000, 6=84.2000, 7=92.1000, 8=93.2000, 9=90.5000, 41.5s 23.9m/2.4h
Epoch: [34]
epoch 34/200, train:, losses=0.6397, top1=73.6081, 0=89.9329, 1=95.0882, 2=85.4713, 3=78.8471, 4=88.8016, 5=81.0208, 6=91.3337, 7=91.9410, 8=93.6630, 9=93.7829, val:, losses=0.4225, top1=85.8200, 0=94.4000, 1=96.0000, 2=82.2000, 3=68.8000, 4=90.4000, 5=75.0000, 6=90.8000, 7=88.2000, 8=81.0000, 9=91.4000, test:, losses=0.4303, top1=85.2900, 0=94.7000, 1=95.9000, 2=79.6000, 3=68.8000, 4=91.3000, 5=75.7000, 6=91.4000, 7=87.4000, 8=79.4000, 9=88.7000, 42.1s 24.6m/2.4h
Epoch: [35]
epoch 35/200, train:, losses=0.6102, top1=74.6886, 0=91.2296, 1=95.0286, 2=85.0022, 3=78.3920, 4=89.0394, 5=82.1876, 6=91.4763, 7=91.5239, 8=94.5566, 9=93.9198, val:, losses=0.4744, top1=85.0800, 0=94.2000, 1=98.2000, 2=78.6000, 3=74.2000, 4=90.8000, 5=66.8000, 6=91.6000, 7=88.6000, 8=90.6000, 9=77.2000, test:, losses=0.4848, top1=84.2900, 0=94.9000, 1=98.5000, 2=73.7000, 3=72.1000, 4=91.2000, 5=67.8000, 6=92.1000, 7=89.8000, 8=90.1000, 9=72.7000, 43.0s 25.3m/2.4h
Epoch: [36]
epoch 36/200, train:, losses=0.5975, top1=75.7483, 0=90.9212, 1=95.2231, 2=85.0182, 3=81.9369, 4=90.2383, 5=82.8017, 6=91.3444, 7=92.4707, 8=94.2087, 9=93.5396, val:, losses=0.6063, top1=79.6800, 0=87.8000, 1=94.2000, 2=70.4000, 3=42.2000, 4=74.8000, 5=93.6000, 6=64.6000, 7=85.4000, 8=87.6000, 9=96.2000, test:, losses=0.6110, top1=79.4800, 0=87.7000, 1=95.3000, 2=71.9000, 3=38.9000, 4=76.8000, 5=92.8000, 6=63.3000, 7=86.7000, 8=86.0000, 9=95.4000, 43.0s 26.0m/2.4h
Epoch: [37]
epoch 37/200, train:, losses=0.5948, top1=75.3396, 0=91.1441, 1=95.5307, 2=86.4830, 3=79.4677, 4=89.9873, 5=82.8658, 6=92.0362, 7=91.0535, 8=94.8135, 9=94.0647, val:, losses=0.4760, top1=84.8000, 0=85.0000, 1=94.4000, 2=77.8000, 3=82.4000, 4=87.8000, 5=81.0000, 6=86.4000, 7=73.2000, 8=88.6000, 9=91.4000, test:, losses=0.4762, top1=84.7300, 0=89.0000, 1=95.2000, 2=75.2000, 3=81.7000, 4=88.1000, 5=81.8000, 6=86.0000, 7=71.4000, 8=88.0000, 9=90.9000, 42.3s 26.7m/2.4h
Epoch: [38]
epoch 38/200, train:, losses=0.6024, top1=75.1257, 0=90.3631, 1=95.1881, 2=86.6935, 3=81.3460, 4=89.2661, 5=83.0353, 6=92.6269, 7=91.8542, 8=94.1230, 9=94.4395, val:, losses=0.4198, top1=87.4600, 0=90.4000, 1=94.8000, 2=82.0000, 3=81.4000, 4=85.4000, 5=77.4000, 6=88.2000, 7=91.8000, 8=95.2000, 9=88.0000, test:, losses=0.4255, top1=86.8500, 0=90.0000, 1=94.9000, 2=78.1000, 3=79.6000, 4=86.5000, 5=79.3000, 6=87.4000, 7=91.4000, 8=94.9000, 9=86.4000, 41.9s 27.4m/2.4h
Epoch: [39]
epoch 39/200, train:, losses=0.5992, top1=75.7494, 0=91.4197, 1=95.9893, 2=87.1714, 3=81.7259, 4=90.8159, 5=84.1327, 6=92.2637, 7=92.4051, 8=95.1570, 9=94.2351, val:, losses=0.4409, top1=85.9600, 0=92.0000, 1=95.4000, 2=75.8000, 3=87.6000, 4=83.2000, 5=70.0000, 6=92.0000, 7=81.8000, 8=89.6000, 9=92.2000, test:, losses=0.4479, top1=85.6600, 0=91.1000, 1=96.6000, 2=74.2000, 3=85.6000, 4=82.5000, 5=72.1000, 6=92.6000, 7=82.2000, 8=89.3000, 9=90.4000, 41.2s 28.1m/2.4h
Epoch: [40]
epoch 40/200, train:, losses=0.6648, top1=72.4809, 0=92.4883, 1=96.1559, 2=87.0331, 3=80.7734, 4=90.2054, 5=85.4102, 6=93.1900, 7=92.0792, 8=95.6819, 9=93.9810, val:, losses=0.4238, top1=87.2400, 0=88.0000, 1=96.0000, 2=84.4000, 3=82.4000, 4=85.6000, 5=82.0000, 6=84.4000, 7=88.8000, 8=91.6000, 9=89.2000, test:, losses=0.4221, top1=87.6000, 0=91.1000, 1=96.7000, 2=82.5000, 3=84.1000, 4=86.4000, 5=83.7000, 6=84.4000, 7=89.2000, 8=91.2000, 9=86.7000, 40.8s 28.8m/2.4h
Epoch: [41]
epoch 41/200, train:, losses=0.5963, top1=75.1448, 0=91.0230, 1=95.6137, 2=87.2358, 3=80.1319, 4=90.9243, 5=84.0668, 6=92.9955, 7=91.9507, 8=94.3020, 9=94.2402, val:, losses=0.4098, top1=86.8000, 0=93.8000, 1=97.0000, 2=85.4000, 3=82.2000, 4=89.6000, 5=73.6000, 6=88.8000, 7=83.6000, 8=96.6000, 9=77.4000, test:, losses=0.4155, top1=85.9800, 0=92.7000, 1=98.0000, 2=83.5000, 3=81.2000, 4=88.6000, 5=71.8000, 6=90.3000, 7=84.0000, 8=95.6000, 9=74.1000, 42.5s 29.5m/2.4h
Epoch: [42]
epoch 42/200, train:, losses=0.5794, top1=76.2043, 0=90.8699, 1=96.0442, 2=85.2204, 3=81.0544, 4=91.0878, 5=83.3702, 6=92.7474, 7=92.5827, 8=94.1528, 9=94.2598, val:, losses=0.4079, top1=86.0600, 0=84.6000, 1=90.4000, 2=88.2000, 3=74.0000, 4=90.4000, 5=78.0000, 6=81.4000, 7=87.2000, 8=90.6000, 9=95.8000, test:, losses=0.4037, top1=86.4700, 0=85.8000, 1=92.3000, 2=87.9000, 3=72.3000, 4=89.2000, 5=81.5000, 6=80.6000, 7=88.0000, 8=91.0000, 9=96.1000, 43.0s 30.2m/2.4h
Epoch: [43]
epoch 43/200, train:, losses=0.6202, top1=74.5303, 0=90.7246, 1=95.9567, 2=87.5488, 3=82.6149, 4=90.2252, 5=84.0189, 6=92.6676, 7=92.0778, 8=94.4472, 9=94.2619, val:, losses=0.4036, top1=87.6400, 0=90.8000, 1=95.8000, 2=85.2000, 3=80.2000, 4=92.6000, 5=78.8000, 6=90.4000, 7=87.8000, 8=85.4000, 9=89.4000, test:, losses=0.4084, top1=87.3000, 0=92.6000, 1=94.8000, 2=87.0000, 3=77.2000, 4=90.5000, 5=77.6000, 6=91.3000, 7=88.0000, 8=85.3000, 9=88.7000, 41.6s 30.9m/2.4h
Epoch: [44]
epoch 44/200, train:, losses=0.6168, top1=74.6246, 0=91.8673, 1=96.5427, 2=86.7184, 3=81.4715, 4=89.8972, 5=83.7668, 6=92.9637, 7=92.8372, 8=94.6818, 9=94.2711, val:, losses=0.4513, top1=85.6800, 0=89.4000, 1=93.0000, 2=91.6000, 3=73.8000, 4=89.6000, 5=62.6000, 6=89.6000, 7=86.2000, 8=96.2000, 9=84.8000, test:, losses=0.4531, top1=85.3300, 0=88.2000, 1=93.6000, 2=88.7000, 3=72.0000, 4=91.6000, 5=64.4000, 6=88.8000, 7=86.6000, 8=94.8000, 9=84.6000, 42.4s 31.6m/2.4h
Epoch: [45]
epoch 45/200, train:, losses=0.6216, top1=74.3276, 0=92.2578, 1=95.4028, 2=86.3351, 3=82.6401, 4=90.9847, 5=84.0672, 6=92.8149, 7=92.3359, 8=94.9279, 9=93.7642, val:, losses=0.5285, top1=82.9000, 0=75.8000, 1=87.4000, 2=73.0000, 3=71.8000, 4=86.2000, 5=90.6000, 6=81.8000, 7=82.4000, 8=87.8000, 9=92.2000, test:, losses=0.5170, top1=83.8100, 0=78.1000, 1=90.9000, 2=73.3000, 3=68.7000, 4=88.3000, 5=91.8000, 6=82.6000, 7=83.2000, 8=89.6000, 9=91.6000, 41.9s 32.3m/2.4h
Epoch: [46]
epoch 46/200, train:, losses=0.5691, top1=76.5524, 0=91.5082, 1=95.8606, 2=86.7130, 3=80.4460, 4=91.3866, 5=83.6262, 6=92.9862, 7=92.7412, 8=94.0318, 9=93.9061, val:, losses=0.4308, top1=85.8000, 0=88.0000, 1=88.0000, 2=82.4000, 3=71.4000, 4=74.2000, 5=81.4000, 6=88.6000, 7=93.4000, 8=97.4000, 9=93.2000, test:, losses=0.4235, top1=86.0900, 0=88.7000, 1=89.6000, 2=81.2000, 3=69.7000, 4=76.4000, 5=83.0000, 6=89.1000, 7=94.0000, 8=96.5000, 9=92.7000, 43.6s 33.0m/2.4h
Epoch: [47]
epoch 47/200, train:, losses=0.5503, top1=77.0919, 0=91.2804, 1=95.6368, 2=86.8358, 3=82.0523, 4=90.6587, 5=85.0138, 6=92.6424, 7=93.1295, 8=94.1411, 9=94.1199, val:, losses=0.5322, top1=82.4200, 0=91.4000, 1=90.0000, 2=63.4000, 3=76.0000, 4=88.8000, 5=55.0000, 6=77.2000, 7=89.8000, 8=97.4000, 9=95.2000, test:, losses=0.5293, top1=82.7100, 0=92.9000, 1=92.5000, 2=61.7000, 3=76.5000, 4=88.1000, 5=57.8000, 6=74.7000, 7=92.1000, 8=96.2000, 9=94.6000, 43.6s 33.7m/2.4h
Epoch: [48]
epoch 48/200, train:, losses=0.6125, top1=74.9610, 0=92.5647, 1=96.3733, 2=87.9845, 3=82.7500, 4=91.0063, 5=84.2105, 6=92.9730, 7=93.6180, 8=95.1597, 9=95.1386, val:, losses=0.4067, top1=86.9600, 0=85.4000, 1=95.8000, 2=82.6000, 3=72.6000, 4=86.6000, 5=87.6000, 6=88.8000, 7=83.0000, 8=95.4000, 9=91.8000, test:, losses=0.4009, top1=87.6100, 0=86.9000, 1=97.7000, 2=82.3000, 3=75.5000, 4=88.4000, 5=88.4000, 6=87.3000, 7=83.6000, 8=95.0000, 9=91.0000, 41.4s 34.4m/2.4h
Epoch: [49]
epoch 49/200, train:, losses=0.5781, top1=76.2452, 0=91.9803, 1=95.3978, 2=87.0651, 3=82.6857, 4=90.7302, 5=84.9546, 6=92.8263, 7=92.8196, 8=95.1712, 9=94.3128, val:, losses=0.4442, top1=85.2800, 0=88.4000, 1=98.4000, 2=85.8000, 3=53.0000, 4=91.0000, 5=76.2000, 6=85.4000, 7=93.4000, 8=96.0000, 9=85.2000, test:, losses=0.4483, top1=85.2100, 0=87.5000, 1=98.6000, 2=85.3000, 3=52.6000, 4=91.8000, 5=78.2000, 6=86.6000, 7=93.1000, 8=94.9000, 9=83.5000, 42.2s 35.1m/2.4h
Epoch: [50]
epoch 50/200, train:, losses=0.5914, top1=75.5753, 0=92.0072, 1=95.7306, 2=87.1853, 3=82.5440, 4=91.5361, 5=85.1327, 6=93.7891, 7=93.1859, 8=94.8430, 9=94.4777, val:, losses=0.4041, top1=87.5800, 0=93.4000, 1=93.4000, 2=83.2000, 3=78.6000, 4=83.0000, 5=85.2000, 6=90.0000, 7=90.8000, 8=84.2000, 9=94.0000, test:, losses=0.3960, top1=87.3500, 0=92.3000, 1=93.2000, 2=82.0000, 3=79.9000, 4=83.0000, 5=84.5000, 6=88.7000, 7=91.0000, 8=85.1000, 9=93.8000, 42.0s 35.8m/2.4h
Epoch: [51]
epoch 51/200, train:, losses=0.6071, top1=75.2812, 0=92.1790, 1=95.7979, 2=87.7751, 3=82.9787, 4=91.4969, 5=83.7337, 6=93.6346, 7=93.0648, 8=95.2285, 9=94.7716, val:, losses=0.4247, top1=87.0800, 0=89.6000, 1=90.0000, 2=77.6000, 3=78.2000, 4=81.2000, 5=80.8000, 6=93.8000, 7=90.0000, 8=96.4000, 9=93.2000, test:, losses=0.4204, top1=86.6200, 0=88.8000, 1=91.9000, 2=74.6000, 3=79.5000, 4=78.7000, 5=80.7000, 6=94.7000, 7=89.0000, 8=96.4000, 9=91.9000, 41.5s 36.5m/2.4h
Epoch: [52]
epoch 52/200, train:, losses=0.5764, top1=76.3453, 0=91.0699, 1=96.0877, 2=87.9622, 3=83.4204, 4=91.1739, 5=85.7143, 6=92.9748, 7=93.2759, 8=94.8618, 9=95.3029, val:, losses=0.4287, top1=86.0600, 0=90.4000, 1=94.8000, 2=73.0000, 3=72.2000, 4=88.4000, 5=70.2000, 6=93.6000, 7=87.8000, 8=96.0000, 9=94.2000, test:, losses=0.4333, top1=86.2300, 0=91.3000, 1=96.2000, 2=72.1000, 3=69.3000, 4=89.9000, 5=73.7000, 6=94.9000, 7=87.1000, 8=93.4000, 9=94.4000, 42.2s 37.2m/2.4h
Epoch: [53]
epoch 53/200, train:, losses=0.6130, top1=74.5839, 0=92.0951, 1=95.8115, 2=87.4355, 3=83.5261, 4=91.8512, 5=84.7076, 6=93.1808, 7=92.4109, 8=95.6235, 9=94.8071, val:, losses=0.5138, top1=82.8600, 0=69.6000, 1=93.4000, 2=91.4000, 3=83.4000, 4=84.8000, 5=54.0000, 6=90.2000, 7=77.0000, 8=90.6000, 9=94.2000, test:, losses=0.4961, top1=83.9600, 0=70.5000, 1=95.2000, 2=92.8000, 3=84.2000, 4=86.0000, 5=58.3000, 6=89.3000, 7=76.9000, 8=91.7000, 9=94.7000, 41.7s 37.9m/2.4h
Epoch: [54]
epoch 54/200, train:, losses=0.5580, top1=76.7967, 0=91.7738, 1=96.4271, 2=88.2056, 3=82.1192, 4=91.7575, 5=85.0086, 6=93.3081, 7=93.9035, 8=95.2298, 9=94.0849, val:, losses=0.4678, top1=85.1400, 0=88.8000, 1=99.4000, 2=81.2000, 3=71.0000, 4=91.0000, 5=77.4000, 6=90.2000, 7=91.6000, 8=88.4000, 9=72.4000, test:, losses=0.4729, top1=84.5400, 0=89.3000, 1=99.2000, 2=82.3000, 3=67.3000, 4=91.7000, 5=76.4000, 6=90.5000, 7=92.8000, 8=88.0000, 9=67.9000, 41.6s 38.6m/2.4h
Epoch: [55]
epoch 55/200, train:, losses=0.5529, top1=77.2518, 0=92.1438, 1=95.9526, 2=87.7534, 3=82.9039, 4=92.2253, 5=85.2459, 6=92.9474, 7=92.8118, 8=94.3532, 9=94.5626, val:, losses=0.4243, top1=86.9600, 0=85.4000, 1=93.2000, 2=82.2000, 3=85.4000, 4=85.0000, 5=62.0000, 6=94.6000, 7=94.4000, 8=97.4000, 9=90.0000, test:, losses=0.4198, top1=86.7500, 0=87.7000, 1=93.2000, 2=81.6000, 3=84.9000, 4=86.9000, 5=59.9000, 6=95.3000, 7=92.0000, 8=95.4000, 9=90.6000, 41.5s 39.3m/2.4h
Epoch: [56]
epoch 56/200, train:, losses=0.5792, top1=75.8804, 0=91.0428, 1=95.5851, 2=87.9198, 3=82.0536, 4=91.4298, 5=84.4541, 6=93.7253, 7=93.5772, 8=94.2982, 9=95.0109, val:, losses=0.4343, top1=86.4800, 0=81.4000, 1=96.8000, 2=91.0000, 3=66.4000, 4=92.4000, 5=68.6000, 6=90.4000, 7=90.4000, 8=95.4000, 9=92.0000, test:, losses=0.4358, top1=86.2100, 0=82.8000, 1=96.8000, 2=91.5000, 3=65.1000, 4=92.1000, 5=67.6000, 6=90.0000, 7=91.2000, 8=93.4000, 9=91.6000, 41.6s 40.0m/2.4h
Epoch: [57]
epoch 57/200, train:, losses=0.5617, top1=76.9141, 0=92.5846, 1=96.0640, 2=88.3516, 3=83.7640, 4=90.5440, 5=84.8459, 6=94.0094, 7=93.8219, 8=95.0000, 9=94.6845, val:, losses=0.4824, top1=84.5800, 0=87.2000, 1=91.6000, 2=81.2000, 3=50.8000, 4=93.8000, 5=74.2000, 6=92.0000, 7=84.0000, 8=93.2000, 9=97.8000, test:, losses=0.4751, top1=84.6600, 0=87.9000, 1=92.9000, 2=80.4000, 3=48.7000, 4=92.6000, 5=76.2000, 6=92.6000, 7=84.8000, 8=93.4000, 9=97.1000, 41.8s 40.7m/2.4h
Epoch: [58]
epoch 58/200, train:, losses=0.5761, top1=76.5285, 0=92.0613, 1=96.1434, 2=87.9709, 3=83.8724, 4=91.4431, 5=86.0146, 6=93.6227, 7=93.0790, 8=94.6108, 9=94.7320, val:, losses=0.5541, top1=82.0000, 0=84.2000, 1=85.6000, 2=76.6000, 3=52.6000, 4=82.6000, 5=80.0000, 6=97.6000, 7=83.0000, 8=98.4000, 9=79.4000, test:, losses=0.5574, top1=81.3500, 0=85.8000, 1=86.8000, 2=72.4000, 3=52.1000, 4=81.4000, 5=80.3000, 6=98.4000, 7=82.6000, 8=97.3000, 9=76.4000, 41.3s 41.4m/2.4h
Epoch: [59]
epoch 59/200, train:, losses=0.5542, top1=77.3733, 0=92.4391, 1=95.8492, 2=87.4363, 3=83.5328, 4=91.6487, 5=85.5225, 6=92.9638, 7=94.2796, 8=95.3599, 9=95.0279, val:, losses=0.4513, top1=85.8000, 0=78.8000, 1=93.8000, 2=75.8000, 3=67.2000, 4=92.4000, 5=82.8000, 6=96.4000, 7=82.8000, 8=97.0000, 9=91.0000, test:, losses=0.4450, top1=86.4000, 0=82.2000, 1=96.2000, 2=73.3000, 3=68.5000, 4=92.8000, 5=82.7000, 6=98.0000, 7=83.3000, 8=96.5000, 9=90.5000, 42.4s 42.1m/2.4h
Epoch: [60]
epoch 60/200, train:, losses=0.5648, top1=76.9266, 0=92.6375, 1=96.5487, 2=88.9532, 3=84.2637, 4=92.2913, 5=85.6337, 6=93.5958, 7=93.8623, 8=95.5813, 9=94.8707, val:, losses=0.4034, top1=86.7000, 0=81.6000, 1=88.0000, 2=85.4000, 3=72.6000, 4=93.2000, 5=85.6000, 6=92.2000, 7=83.0000, 8=98.2000, 9=87.2000, test:, losses=0.4046, top1=86.8600, 0=81.7000, 1=88.5000, 2=84.0000, 3=74.7000, 4=93.9000, 5=86.7000, 6=93.1000, 7=82.2000, 8=96.0000, 9=87.8000, 42.6s 42.8m/2.4h
Epoch: [61]
epoch 61/200, train:, losses=0.6130, top1=75.1150, 0=92.6168, 1=95.9670, 2=88.7385, 3=84.6487, 4=92.0098, 5=85.7858, 6=94.4280, 7=94.1320, 8=95.4436, 9=94.3898, val:, losses=0.4510, top1=84.8400, 0=85.0000, 1=95.2000, 2=68.4000, 3=59.2000, 4=81.6000, 5=83.2000, 6=92.6000, 7=94.8000, 8=95.2000, 9=93.2000, test:, losses=0.4583, top1=84.6100, 0=87.5000, 1=95.9000, 2=65.4000, 3=59.3000, 4=81.2000, 5=82.5000, 6=93.5000, 7=93.5000, 8=94.2000, 9=93.1000, 43.0s 43.5m/2.4h
Epoch: [62]
epoch 62/200, train:, losses=0.5977, top1=75.2463, 0=92.5736, 1=96.0923, 2=89.5264, 3=83.5828, 4=91.5581, 5=85.9155, 6=93.7235, 7=93.6645, 8=95.7096, 9=95.6322, val:, losses=0.4084, top1=87.7400, 0=94.4000, 1=94.6000, 2=80.0000, 3=73.8000, 4=87.2000, 5=83.8000, 6=91.6000, 7=90.8000, 8=92.4000, 9=88.8000, test:, losses=0.3986, top1=88.3000, 0=94.3000, 1=94.5000, 2=78.4000, 3=75.9000, 4=89.5000, 5=85.5000, 6=93.0000, 7=92.2000, 8=91.9000, 9=87.8000, 42.7s 44.2m/2.4h
Epoch: [63]
epoch 63/200, train:, losses=0.5770, top1=76.2918, 0=92.2727, 1=95.8898, 2=87.7354, 3=82.4337, 4=91.1961, 5=84.6539, 6=93.3814, 7=94.5549, 8=94.8718, 9=94.6882, val:, losses=0.4275, top1=86.6800, 0=95.0000, 1=93.0000, 2=71.0000, 3=77.6000, 4=79.6000, 5=80.4000, 6=92.4000, 7=88.2000, 8=94.2000, 9=95.4000, test:, losses=0.4254, top1=86.5800, 0=95.3000, 1=94.6000, 2=69.7000, 3=78.7000, 4=77.9000, 5=81.3000, 6=92.1000, 7=87.9000, 8=93.7000, 9=94.6000, 42.8s 45.0m/2.4h
Epoch: [64]
epoch 64/200, train:, losses=0.5853, top1=75.8958, 0=91.8919, 1=96.2460, 2=88.7760, 3=84.7032, 4=91.8256, 5=87.3164, 6=94.1231, 7=93.9422, 8=95.8144, 9=94.7491, val:, losses=0.5713, top1=82.7600, 0=86.2000, 1=97.4000, 2=95.6000, 3=45.6000, 4=77.8000, 5=72.4000, 6=86.6000, 7=86.8000, 8=92.0000, 9=87.2000, test:, losses=0.5759, top1=82.0800, 0=87.1000, 1=96.7000, 2=95.3000, 3=44.6000, 4=75.9000, 5=72.5000, 6=87.4000, 7=84.5000, 8=91.3000, 9=85.5000, 41.9s 45.7m/2.4h
Epoch: [65]
epoch 65/200, train:, losses=0.5757, top1=75.9267, 0=92.6994, 1=96.4035, 2=88.1956, 3=82.9983, 4=91.0256, 5=84.8665, 6=93.9580, 7=93.7835, 8=95.2855, 9=94.6524, val:, losses=0.4137, top1=86.4400, 0=81.0000, 1=94.0000, 2=85.8000, 3=76.0000, 4=80.0000, 5=70.4000, 6=95.0000, 7=92.6000, 8=97.4000, 9=92.2000, test:, losses=0.4095, top1=86.4500, 0=82.9000, 1=94.9000, 2=83.9000, 3=75.5000, 4=77.4000, 5=72.7000, 6=94.4000, 7=93.9000, 8=96.9000, 9=92.0000, 42.0s 46.4m/2.4h
Epoch: [66]
epoch 66/200, train:, losses=0.5970, top1=75.2148, 0=92.8705, 1=95.9357, 2=90.2088, 3=83.1614, 4=91.9482, 5=86.0009, 6=94.3907, 7=93.5077, 8=94.8034, 9=94.6705, val:, losses=0.4371, top1=86.7200, 0=89.6000, 1=92.2000, 2=78.6000, 3=74.0000, 4=91.6000, 5=80.6000, 6=87.2000, 7=87.4000, 8=95.2000, 9=90.8000, test:, losses=0.4164, top1=87.6200, 0=89.5000, 1=95.5000, 2=78.8000, 3=76.0000, 4=91.9000, 5=82.8000, 6=88.3000, 7=89.8000, 8=94.4000, 9=89.2000, 43.0s 47.1m/2.4h
Epoch: [67]
epoch 67/200, train:, losses=0.5758, top1=76.3254, 0=92.1180, 1=95.8606, 2=88.0868, 3=83.6465, 4=91.8786, 5=86.2361, 6=93.4963, 7=92.8477, 8=95.1037, 9=94.5534, val:, losses=0.4690, top1=85.4400, 0=86.0000, 1=96.4000, 2=84.4000, 3=82.6000, 4=90.0000, 5=74.4000, 6=92.4000, 7=79.6000, 8=78.0000, 9=90.6000, test:, losses=0.4582, top1=85.6600, 0=85.9000, 1=97.3000, 2=83.2000, 3=82.7000, 4=87.3000, 5=76.4000, 6=93.9000, 7=81.8000, 8=79.8000, 9=88.3000, 42.5s 47.8m/2.4h
Epoch: [68]
epoch 68/200, train:, losses=0.5695, top1=76.8674, 0=92.3899, 1=95.5330, 2=89.1988, 3=84.0440, 4=91.9530, 5=85.9508, 6=94.7818, 7=93.5412, 8=95.6522, 9=95.0885, val:, losses=0.4350, top1=86.7400, 0=86.4000, 1=98.4000, 2=83.0000, 3=81.4000, 4=90.6000, 5=78.4000, 6=92.0000, 7=87.0000, 8=80.8000, 9=89.4000, test:, losses=0.4283, top1=87.3400, 0=86.4000, 1=98.4000, 2=83.5000, 3=80.9000, 4=90.6000, 5=79.7000, 6=94.6000, 7=86.7000, 8=81.9000, 9=90.7000, 39.9s 48.4m/2.4h
Epoch: [69]
epoch 69/200, train:, losses=0.5437, top1=77.8760, 0=92.8448, 1=96.3628, 2=87.8628, 3=82.9991, 4=91.6881, 5=87.0266, 6=93.6494, 7=93.1095, 8=95.9184, 9=95.6745, val:, losses=0.5062, top1=83.6600, 0=98.8000, 1=97.8000, 2=77.0000, 3=62.0000, 4=78.2000, 5=66.6000, 6=94.6000, 7=85.4000, 8=89.4000, 9=86.8000, test:, losses=0.5195, top1=83.0100, 0=97.5000, 1=98.5000, 2=71.6000, 3=63.2000, 4=78.5000, 5=66.9000, 6=96.0000, 7=86.1000, 8=87.6000, 9=84.2000, 41.3s 49.1m/2.4h
Epoch: [70]
epoch 70/200, train:, losses=0.5942, top1=75.4598, 0=93.1564, 1=96.5776, 2=89.7031, 3=83.5952, 4=92.5981, 5=86.5376, 6=94.1960, 7=94.4741, 8=96.0317, 9=95.4155, val:, losses=0.4218, top1=86.3600, 0=86.0000, 1=94.8000, 2=91.6000, 3=67.0000, 4=71.2000, 5=79.4000, 6=91.0000, 7=95.8000, 8=94.4000, 9=92.4000, test:, losses=0.4246, top1=86.4300, 0=86.9000, 1=96.1000, 2=92.6000, 3=64.4000, 4=70.2000, 5=82.3000, 6=92.1000, 7=95.3000, 8=93.4000, 9=91.0000, 41.4s 49.8m/2.4h
Epoch: [71]
epoch 71/200, train:, losses=0.5984, top1=74.9395, 0=93.1323, 1=95.8257, 2=87.9909, 3=84.7676, 4=92.4467, 5=86.6636, 6=94.3466, 7=93.0398, 8=95.6208, 9=94.2029, val:, losses=0.4046, top1=86.5000, 0=87.8000, 1=97.8000, 2=80.8000, 3=86.0000, 4=83.4000, 5=74.0000, 6=86.8000, 7=87.4000, 8=94.0000, 9=87.0000, test:, losses=0.3985, top1=86.8500, 0=91.0000, 1=98.6000, 2=78.5000, 3=86.7000, 4=81.3000, 5=77.0000, 6=88.7000, 7=88.5000, 8=91.6000, 9=86.6000, 42.9s 50.5m/2.4h
Epoch: [72]
epoch 72/200, train:, losses=0.5989, top1=74.7444, 0=93.2203, 1=96.5184, 2=88.1421, 3=84.9562, 4=92.0218, 5=86.9036, 6=94.5098, 7=93.5407, 8=95.6017, 9=94.9275, val:, losses=0.4393, top1=86.3400, 0=86.6000, 1=93.4000, 2=86.8000, 3=79.6000, 4=88.8000, 5=87.2000, 6=79.0000, 7=86.4000, 8=94.8000, 9=80.8000, test:, losses=0.4376, top1=86.5900, 0=85.4000, 1=93.8000, 2=86.1000, 3=79.9000, 4=91.0000, 5=86.3000, 6=80.1000, 7=86.1000, 8=94.9000, 9=82.3000, 41.8s 51.2m/2.4h
Epoch: [73]
epoch 73/200, train:, losses=0.5811, top1=76.0562, 0=92.6306, 1=95.8777, 2=89.4427, 3=84.9614, 4=91.9125, 5=85.8656, 6=93.9254, 7=93.7158, 8=94.9634, 9=95.0600, val:, losses=0.4185, top1=86.5000, 0=91.6000, 1=95.0000, 2=89.8000, 3=83.4000, 4=69.2000, 5=79.8000, 6=87.4000, 7=87.2000, 8=91.6000, 9=90.0000, test:, losses=0.4064, top1=87.4400, 0=92.3000, 1=97.1000, 2=90.6000, 3=84.1000, 4=75.0000, 5=80.9000, 6=87.9000, 7=86.3000, 8=91.9000, 9=88.3000, 41.1s 51.9m/2.4h
Epoch: [74]
epoch 74/200, train:, losses=0.5719, top1=76.1101, 0=92.0203, 1=96.2741, 2=89.4118, 3=84.0294, 4=92.1395, 5=85.8253, 6=94.6105, 7=93.1881, 8=96.0292, 9=94.6987, val:, losses=0.4827, top1=83.9000, 0=81.0000, 1=92.6000, 2=70.8000, 3=75.4000, 4=94.8000, 5=76.2000, 6=66.0000, 7=94.6000, 8=95.6000, 9=92.0000, test:, losses=0.4934, top1=83.8900, 0=84.3000, 1=91.3000, 2=70.4000, 3=76.0000, 4=94.3000, 5=77.5000, 6=64.0000, 7=94.7000, 8=95.0000, 9=91.4000, 40.2s 52.6m/2.3h
Epoch: [75]
epoch 75/200, train:, losses=0.5778, top1=76.0854, 0=92.4690, 1=95.5587, 2=89.6405, 3=85.3333, 4=91.9647, 5=86.5205, 6=94.6804, 7=94.3119, 8=95.1452, 9=95.1702, val:, losses=0.5463, top1=83.2600, 0=78.2000, 1=93.0000, 2=74.6000, 3=63.0000, 4=84.8000, 5=84.2000, 6=97.2000, 7=73.0000, 8=97.0000, 9=87.6000, test:, losses=0.5469, top1=82.9500, 0=77.2000, 1=92.1000, 2=73.6000, 3=65.5000, 4=85.0000, 5=83.2000, 6=97.8000, 7=72.2000, 8=95.8000, 9=87.1000, 41.6s 53.3m/2.3h
Epoch: [76]
epoch 76/200, train:, losses=0.5838, top1=75.7742, 0=92.5023, 1=96.2545, 2=89.0176, 3=84.8165, 4=92.4188, 5=86.3463, 6=94.5908, 7=93.2935, 8=95.4905, 9=95.4315, val:, losses=0.4299, top1=86.3800, 0=80.2000, 1=88.6000, 2=76.2000, 3=82.6000, 4=90.2000, 5=67.8000, 6=94.8000, 7=90.2000, 8=99.4000, 9=93.8000, test:, losses=0.4265, top1=86.0500, 0=80.9000, 1=89.5000, 2=71.9000, 3=83.1000, 4=90.0000, 5=70.1000, 6=95.6000, 7=88.9000, 8=97.8000, 9=92.7000, 42.8s 54.0m/2.3h
Epoch: [77]
epoch 77/200, train:, losses=0.5829, top1=76.0316, 0=92.4977, 1=96.3014, 2=89.0653, 3=84.1631, 4=92.6716, 5=87.0370, 6=94.5847, 7=94.2962, 8=94.8956, 9=95.5848, val:, losses=0.4889, top1=83.1800, 0=83.4000, 1=89.2000, 2=77.6000, 3=54.4000, 4=75.6000, 5=94.8000, 6=82.4000, 7=80.6000, 8=97.6000, 9=96.2000, test:, losses=0.4933, top1=83.0900, 0=84.0000, 1=90.6000, 2=76.4000, 3=52.9000, 4=76.0000, 5=95.3000, 6=82.1000, 7=81.7000, 8=96.6000, 9=95.3000, 42.6s 54.7m/2.3h
Epoch: [78]
epoch 78/200, train:, losses=0.5296, top1=78.2100, 0=91.8315, 1=96.4676, 2=89.7243, 3=84.8185, 4=92.3904, 5=85.6242, 6=93.8835, 7=94.0441, 8=95.4276, 9=95.2302, val:, losses=0.4463, top1=85.4000, 0=91.0000, 1=95.0000, 2=89.0000, 3=66.6000, 4=84.6000, 5=89.4000, 6=72.6000, 7=85.2000, 8=87.6000, 9=93.0000, test:, losses=0.4375, top1=85.7500, 0=90.3000, 1=94.1000, 2=89.6000, 3=67.8000, 4=84.1000, 5=90.8000, 6=74.2000, 7=85.8000, 8=88.3000, 9=92.5000, 42.1s 55.4m/2.3h
Epoch: [79]
epoch 79/200, train:, losses=0.5383, top1=77.9076, 0=93.1617, 1=96.1408, 2=89.5458, 3=84.3112, 4=92.6819, 5=87.1132, 6=94.5934, 7=94.2474, 8=95.5851, 9=94.5809, val:, losses=0.5273, top1=83.0600, 0=85.2000, 1=96.4000, 2=91.8000, 3=43.6000, 4=93.2000, 5=73.6000, 6=91.2000, 7=80.6000, 8=83.4000, 9=91.6000, test:, losses=0.5227, top1=83.0900, 0=85.1000, 1=98.0000, 2=90.7000, 3=42.5000, 4=93.4000, 5=74.7000, 6=92.8000, 7=81.5000, 8=82.8000, 9=89.4000, 41.8s 56.1m/2.3h
Epoch: [80]
epoch 80/200, train:, losses=0.5504, top1=77.4569, 0=93.7128, 1=96.1328, 2=88.7790, 3=85.0380, 4=91.9574, 5=86.2679, 6=93.7630, 7=94.0286, 8=95.5919, 9=95.3247, val:, losses=0.3671, top1=87.6400, 0=87.8000, 1=91.6000, 2=90.8000, 3=74.4000, 4=90.8000, 5=74.8000, 6=84.2000, 7=91.0000, 8=96.2000, 9=94.8000, test:, losses=0.3655, top1=87.7600, 0=90.7000, 1=93.2000, 2=92.7000, 3=73.4000, 4=91.3000, 5=73.2000, 6=86.2000, 7=92.1000, 8=93.4000, 9=91.4000, 42.7s 56.8m/2.4h
Epoch: [81]
epoch 81/200, train:, losses=0.5818, top1=75.8089, 0=93.3638, 1=96.5517, 2=89.3753, 3=85.0977, 4=91.4561, 5=85.5245, 6=94.3093, 7=94.0985, 8=95.9759, 9=96.2595, val:, losses=0.4493, top1=86.4400, 0=91.8000, 1=95.8000, 2=74.2000, 3=82.4000, 4=85.6000, 5=82.6000, 6=81.4000, 7=90.4000, 8=88.0000, 9=92.2000, test:, losses=0.4475, top1=86.9000, 0=92.4000, 1=96.0000, 2=74.9000, 3=82.2000, 4=87.3000, 5=84.8000, 6=83.1000, 7=89.7000, 8=86.1000, 9=92.5000, 42.8s 57.5m/2.4h
Epoch: [82]
epoch 82/200, train:, losses=0.5662, top1=76.7049, 0=93.4998, 1=96.6635, 2=88.8994, 3=85.8459, 4=92.3150, 5=87.1960, 6=93.7500, 7=94.5647, 8=95.6642, 9=95.4198, val:, losses=0.3830, top1=87.8200, 0=91.0000, 1=97.8000, 2=76.8000, 3=77.0000, 4=86.0000, 5=80.2000, 6=94.4000, 7=92.2000, 8=95.2000, 9=87.6000, test:, losses=0.3751, top1=88.1800, 0=90.2000, 1=98.3000, 2=77.1000, 3=79.6000, 4=86.9000, 5=82.5000, 6=94.2000, 7=92.5000, 8=93.2000, 9=87.3000, 41.6s 58.2m/2.4h
Epoch: [83]
epoch 83/200, train:, losses=0.5551, top1=77.2280, 0=93.3021, 1=96.1860, 2=89.2308, 3=85.6364, 4=91.7300, 5=85.9512, 6=94.6186, 7=94.1670, 8=95.5998, 9=95.2936, val:, losses=0.4255, top1=86.8200, 0=94.8000, 1=96.8000, 2=63.6000, 3=78.4000, 4=87.6000, 5=85.4000, 6=88.0000, 7=95.6000, 8=92.0000, 9=86.0000, test:, losses=0.4404, top1=86.3800, 0=95.0000, 1=96.6000, 2=61.4000, 3=78.3000, 4=89.4000, 5=86.9000, 6=87.4000, 7=95.2000, 8=89.5000, 9=84.1000, 42.2s 58.9m/2.4h
Epoch: [84]
epoch 84/200, train:, losses=0.5835, top1=75.6749, 0=93.3272, 1=96.3165, 2=89.3657, 3=85.9416, 4=92.7640, 5=87.0312, 6=93.3658, 7=94.1845, 8=96.7134, 9=95.6873, val:, losses=0.4387, top1=86.3600, 0=85.8000, 1=92.6000, 2=94.2000, 3=67.4000, 4=91.2000, 5=81.2000, 6=79.6000, 7=86.0000, 8=96.4000, 9=89.2000, test:, losses=0.4358, top1=86.0600, 0=85.3000, 1=94.9000, 2=92.5000, 3=64.9000, 4=91.2000, 5=82.5000, 6=81.2000, 7=85.6000, 8=95.1000, 9=87.4000, 42.3s 59.6m/2.4h
Epoch: [85]
epoch 85/200, train:, losses=0.5212, top1=78.5237, 0=92.8302, 1=96.3084, 2=88.6169, 3=84.3610, 4=92.3467, 5=86.0924, 6=94.6929, 7=94.0501, 8=95.2893, 9=94.8304, val:, losses=0.4102, top1=85.9600, 0=86.0000, 1=89.4000, 2=86.6000, 3=74.2000, 4=72.0000, 5=85.4000, 6=93.4000, 7=82.6000, 8=93.8000, 9=96.2000, test:, losses=0.4034, top1=86.8200, 0=87.0000, 1=90.8000, 2=87.1000, 3=75.1000, 4=74.6000, 5=85.1000, 6=94.4000, 7=83.3000, 8=93.4000, 9=97.4000, 41.5s 1.0h/2.4h
Epoch: [86]
epoch 86/200, train:, losses=0.5849, top1=75.7583, 0=92.8852, 1=96.9007, 2=90.5523, 3=85.7346, 4=92.9924, 5=87.2849, 6=93.9867, 7=94.3379, 8=96.4559, 9=95.1090, val:, losses=0.4752, top1=84.2600, 0=94.8000, 1=96.0000, 2=71.8000, 3=64.0000, 4=90.8000, 5=79.0000, 6=80.4000, 7=96.6000, 8=92.6000, 9=76.6000, test:, losses=0.4781, top1=84.5400, 0=96.1000, 1=95.9000, 2=70.1000, 3=64.6000, 4=92.4000, 5=79.3000, 6=83.8000, 7=96.3000, 8=91.3000, 9=75.6000, 42.4s 1.0h/2.4h
Epoch: [87]
epoch 87/200, train:, losses=0.5551, top1=77.1014, 0=93.2396, 1=96.4154, 2=89.5656, 3=85.8303, 4=92.1973, 5=87.4288, 6=94.5316, 7=93.8266, 8=95.2446, 9=95.3232, val:, losses=0.5276, top1=83.1400, 0=87.8000, 1=88.2000, 2=79.6000, 3=52.2000, 4=84.4000, 5=74.0000, 6=91.2000, 7=90.6000, 8=86.2000, 9=97.2000, test:, losses=0.5254, top1=83.0000, 0=90.3000, 1=91.8000, 2=78.3000, 3=49.8000, 4=84.6000, 5=73.6000, 6=90.0000, 7=88.6000, 8=85.3000, 9=97.7000, 41.8s 1.0h/2.4h
Epoch: [88]
epoch 88/200, train:, losses=0.5434, top1=77.6028, 0=93.5121, 1=96.1917, 2=89.2446, 3=85.3891, 4=92.5616, 5=86.8674, 6=94.3235, 7=94.4973, 8=96.1424, 9=95.2069, val:, losses=0.3952, top1=87.2200, 0=82.2000, 1=96.6000, 2=89.0000, 3=67.2000, 4=80.0000, 5=86.4000, 6=92.2000, 7=89.4000, 8=95.0000, 9=94.2000, test:, losses=0.3858, top1=87.4500, 0=83.5000, 1=97.0000, 2=88.9000, 3=68.0000, 4=81.0000, 5=86.1000, 6=92.2000, 7=89.2000, 8=94.1000, 9=94.5000, 41.5s 1.0h/2.3h
Epoch: [89]
epoch 89/200, train:, losses=0.5390, top1=77.8907, 0=93.4544, 1=96.2544, 2=89.8379, 3=85.1930, 4=91.8316, 5=86.6215, 6=94.5616, 7=94.4564, 8=96.1120, 9=94.6093, val:, losses=0.3695, top1=88.8600, 0=86.4000, 1=96.0000, 2=82.2000, 3=71.8000, 4=92.6000, 5=89.0000, 6=92.4000, 7=93.0000, 8=97.4000, 9=87.8000, test:, losses=0.3716, top1=88.6200, 0=87.3000, 1=96.7000, 2=82.6000, 3=69.5000, 4=91.1000, 5=89.6000, 6=93.3000, 7=92.2000, 8=96.6000, 9=87.3000, 41.4s 1.1h/2.3h
Epoch: [90]
epoch 90/200, train:, losses=0.5951, top1=75.6058, 0=92.9940, 1=96.5709, 2=89.7498, 3=85.8853, 4=93.7374, 5=86.6633, 6=94.3286, 7=94.7917, 8=95.7286, 9=96.2198, val:, losses=0.4822, top1=84.2600, 0=86.4000, 1=93.4000, 2=66.6000, 3=91.2000, 4=78.4000, 5=65.4000, 6=92.0000, 7=83.0000, 8=95.0000, 9=91.2000, test:, losses=0.4830, top1=83.8200, 0=86.1000, 1=95.0000, 2=64.3000, 3=90.5000, 4=74.8000, 5=69.7000, 6=90.5000, 7=82.0000, 8=95.0000, 9=90.3000, 42.2s 1.1h/2.3h
Epoch: [91]
epoch 91/200, train:, losses=0.5553, top1=77.4267, 0=93.4034, 1=96.6961, 2=89.6950, 3=83.9609, 4=92.7598, 5=86.7380, 6=94.6021, 7=94.4850, 8=95.9136, 9=95.8022, val:, losses=0.4441, top1=85.3200, 0=92.0000, 1=89.0000, 2=88.6000, 3=62.4000, 4=93.2000, 5=70.6000, 6=80.4000, 7=87.2000, 8=92.4000, 9=97.4000, test:, losses=0.4428, top1=85.5100, 0=92.0000, 1=90.9000, 2=87.4000, 3=63.5000, 4=93.4000, 5=71.7000, 6=81.5000, 7=87.2000, 8=91.6000, 9=95.9000, 41.7s 1.1h/2.3h
Epoch: [92]
epoch 92/200, train:, losses=0.5624, top1=76.4413, 0=93.2341, 1=96.7320, 2=89.5614, 3=85.3767, 4=92.4496, 5=87.1658, 6=95.1419, 7=94.4004, 8=96.0192, 9=95.7280, val:, losses=0.4419, top1=85.8400, 0=84.2000, 1=98.2000, 2=94.2000, 3=69.6000, 4=91.4000, 5=72.0000, 6=89.8000, 7=84.6000, 8=89.2000, 9=85.2000, test:, losses=0.4251, top1=86.0500, 0=85.4000, 1=98.2000, 2=93.9000, 3=72.1000, 4=89.6000, 5=72.7000, 6=88.9000, 7=84.9000, 8=89.9000, 9=84.9000, 41.5s 1.1h/2.3h
Epoch: [93]
epoch 93/200, train:, losses=0.5610, top1=76.4943, 0=93.5455, 1=96.5744, 2=89.6414, 3=85.3321, 4=93.1869, 5=86.9258, 6=94.2498, 7=93.8740, 8=96.2037, 9=95.4565, val:, losses=0.3828, top1=87.5800, 0=92.8000, 1=96.2000, 2=81.6000, 3=73.4000, 4=85.8000, 5=80.2000, 6=94.2000, 7=87.2000, 8=95.2000, 9=89.2000, test:, losses=0.3927, top1=87.2800, 0=93.2000, 1=96.7000, 2=77.2000, 3=70.7000, 4=84.9000, 5=82.9000, 6=95.9000, 7=87.8000, 8=92.8000, 9=90.7000, 41.2s 1.1h/2.3h
Epoch: [94]
epoch 94/200, train:, losses=0.5605, top1=77.1969, 0=93.4450, 1=96.5534, 2=90.3517, 3=84.2628, 4=92.9141, 5=86.8610, 6=94.8276, 7=94.1399, 8=95.5756, 9=94.7961, val:, losses=0.3987, top1=89.0000, 0=91.0000, 1=96.2000, 2=86.4000, 3=72.8000, 4=93.6000, 5=80.6000, 6=94.2000, 7=91.8000, 8=92.2000, 9=91.2000, test:, losses=0.3949, top1=89.0000, 0=91.7000, 1=96.8000, 2=83.1000, 3=71.3000, 4=94.3000, 5=81.2000, 6=96.5000, 7=93.9000, 8=91.0000, 9=90.2000, 41.7s 1.1h/2.3h
Epoch: [95]
epoch 95/200, train:, losses=0.5394, top1=77.5557, 0=92.6923, 1=96.4107, 2=89.4648, 3=84.4648, 4=92.7170, 5=87.3044, 6=94.7026, 7=94.6960, 8=95.5422, 9=95.1022, val:, losses=0.4629, top1=85.0200, 0=92.2000, 1=92.6000, 2=77.0000, 3=79.4000, 4=94.8000, 5=57.0000, 6=85.2000, 7=83.0000, 8=97.0000, 9=92.0000, test:, losses=0.4601, top1=85.3600, 0=92.1000, 1=94.1000, 2=75.0000, 3=81.0000, 4=95.2000, 5=62.1000, 6=85.7000, 7=83.5000, 8=94.0000, 9=90.9000, 43.7s 1.1h/2.3h
Epoch: [96]
epoch 96/200, train:, losses=0.5026, top1=79.3286, 0=93.4656, 1=97.2940, 2=90.3368, 3=85.6465, 4=92.4171, 5=87.0048, 6=95.0301, 7=94.0299, 8=96.3022, 9=95.5599, val:, losses=0.3795, top1=88.0600, 0=87.0000, 1=93.6000, 2=72.6000, 3=61.4000, 4=92.2000, 5=90.2000, 6=97.6000, 7=96.2000, 8=96.6000, 9=93.2000, test:, losses=0.3784, top1=87.8600, 0=88.3000, 1=94.1000, 2=71.5000, 3=61.8000, 4=91.7000, 5=90.2000, 6=97.2000, 7=96.7000, 8=94.5000, 9=92.6000, 43.4s 1.1h/2.3h
Epoch: [97]
epoch 97/200, train:, losses=0.5392, top1=77.6911, 0=93.4829, 1=97.2350, 2=89.8447, 3=85.8757, 4=92.3240, 5=87.0457, 6=94.3372, 7=94.2869, 8=95.9758, 9=95.6447, val:, losses=0.4202, top1=86.4000, 0=85.6000, 1=92.4000, 2=87.6000, 3=73.8000, 4=78.6000, 5=90.6000, 6=79.8000, 7=88.6000, 8=93.0000, 9=94.0000, test:, losses=0.4125, top1=86.8200, 0=85.9000, 1=95.9000, 2=86.7000, 3=72.1000, 4=78.8000, 5=91.4000, 6=79.7000, 7=89.3000, 8=93.7000, 9=94.7000, 41.4s 1.1h/2.3h
Epoch: [98]
epoch 98/200, train:, losses=0.5400, top1=78.0467, 0=93.6642, 1=96.6712, 2=90.1163, 3=84.4425, 4=92.2941, 5=86.3259, 6=94.3463, 7=94.7861, 8=95.7042, 9=96.3251, val:, losses=0.4015, top1=87.2800, 0=93.4000, 1=87.8000, 2=86.4000, 3=72.0000, 4=78.6000, 5=90.8000, 6=85.4000, 7=92.2000, 8=96.6000, 9=89.6000, test:, losses=0.3954, top1=87.4700, 0=92.4000, 1=90.1000, 2=88.5000, 3=70.7000, 4=78.6000, 5=90.7000, 6=86.2000, 7=92.2000, 8=95.5000, 9=89.8000, 41.9s 1.2h/2.3h
Epoch: [99]
epoch 99/200, train:, losses=0.5548, top1=77.0469, 0=93.5239, 1=96.3855, 2=89.3933, 3=85.4513, 4=92.7880, 5=87.9503, 6=95.0552, 7=94.5511, 8=95.8225, 9=95.1556, val:, losses=0.3888, top1=88.3600, 0=91.4000, 1=96.0000, 2=86.4000, 3=66.2000, 4=87.8000, 5=91.0000, 6=88.4000, 7=89.8000, 8=95.6000, 9=91.0000, test:, losses=0.3888, top1=88.3900, 0=91.8000, 1=95.6000, 2=86.0000, 3=67.4000, 4=87.2000, 5=90.9000, 6=88.5000, 7=90.6000, 8=95.2000, 9=90.7000, 41.7s 1.2h/2.3h
Epoch: [100]
epoch 100/200, train:, losses=0.5171, top1=79.0599, 0=96.1259, 1=98.1592, 2=92.9688, 3=90.3195, 4=96.0173, 5=90.9265, 6=97.2368, 7=96.6714, 8=97.6098, 9=96.7033, val:, losses=0.2685, top1=92.2800, 0=93.2000, 1=96.0000, 2=87.6000, 3=83.8000, 4=95.8000, 5=86.0000, 6=92.6000, 7=94.8000, 8=98.2000, 9=94.8000, test:, losses=0.2751, top1=91.7200, 0=92.8000, 1=95.2000, 2=86.7000, 3=83.4000, 4=94.7000, 5=86.8000, 6=92.7000, 7=94.1000, 8=96.4000, 9=94.4000, 42.6s 1.2h/2.3h
Epoch: [101]
epoch 101/200, train:, losses=0.4717, top1=80.5357, 0=97.4451, 1=98.7433, 2=94.9107, 3=91.6369, 4=97.0027, 5=93.0064, 6=97.7148, 7=97.7818, 8=98.4267, 9=97.9005, val:, losses=0.2406, top1=92.4600, 0=92.8000, 1=96.2000, 2=90.4000, 3=85.2000, 4=93.8000, 5=85.4000, 6=94.2000, 7=94.0000, 8=98.0000, 9=94.6000, test:, losses=0.2496, top1=92.2400, 0=92.7000, 1=95.4000, 2=90.2000, 3=85.6000, 4=92.9000, 5=86.2000, 6=94.1000, 7=93.9000, 8=96.9000, 9=94.5000, 41.9s 1.2h/2.3h
Epoch: [102]
epoch 102/200, train:, losses=0.4239, top1=82.8311, 0=97.3583, 1=98.9117, 2=94.8826, 3=93.4916, 4=96.8567, 5=93.2112, 6=98.0016, 7=97.3978, 8=98.4134, 9=97.9079, val:, losses=0.2358, top1=92.7000, 0=93.4000, 1=97.0000, 2=90.4000, 3=82.0000, 4=95.0000, 5=88.0000, 6=94.4000, 7=94.6000, 8=97.4000, 9=94.8000, test:, losses=0.2443, top1=92.4700, 0=93.0000, 1=96.1000, 2=91.6000, 3=82.6000, 4=93.8000, 5=87.6000, 6=94.4000, 7=94.6000, 8=96.2000, 9=94.8000, 40.3s 1.2h/2.3h
Epoch: [103]
epoch 103/200, train:, losses=0.4461, top1=81.8893, 0=97.1874, 1=98.7386, 2=95.8606, 3=93.9633, 4=97.7292, 5=93.6404, 6=97.8280, 7=98.0794, 8=98.3500, 9=98.3326, val:, losses=0.2459, top1=92.6400, 0=94.8000, 1=97.2000, 2=89.4000, 3=81.0000, 4=94.4000, 5=89.2000, 6=94.8000, 7=93.6000, 8=97.4000, 9=94.6000, test:, losses=0.2519, top1=92.3500, 0=93.9000, 1=95.7000, 2=89.2000, 3=82.5000, 4=94.1000, 5=89.1000, 6=94.8000, 7=94.4000, 8=96.0000, 9=93.8000, 40.3s 1.2h/2.3h
Epoch: [104]
epoch 104/200, train:, losses=0.4645, top1=81.0646, 0=98.0347, 1=98.7901, 2=95.1802, 3=94.7344, 4=97.5440, 5=94.0987, 6=98.0939, 7=98.4082, 8=98.7523, 9=98.4112, val:, losses=0.2492, top1=92.7800, 0=94.8000, 1=96.6000, 2=89.4000, 3=85.2000, 4=94.8000, 5=86.2000, 6=94.2000, 7=94.2000, 8=97.8000, 9=94.6000, test:, losses=0.2533, top1=92.3600, 0=94.1000, 1=96.0000, 2=89.9000, 3=85.9000, 4=93.2000, 5=85.6000, 6=94.6000, 7=94.4000, 8=95.7000, 9=94.2000, 42.1s 1.2h/2.3h
Epoch: [105]
epoch 105/200, train:, losses=0.4404, top1=81.7732, 0=97.7537, 1=98.7764, 2=96.0859, 3=93.9469, 4=97.5063, 5=93.8767, 6=98.1007, 7=98.2580, 8=98.7315, 9=98.2714, val:, losses=0.2417, top1=92.8400, 0=95.4000, 1=96.8000, 2=90.0000, 3=83.4000, 4=95.6000, 5=88.0000, 6=93.6000, 7=92.8000, 8=97.6000, 9=95.2000, test:, losses=0.2491, top1=92.3900, 0=93.8000, 1=95.5000, 2=89.0000, 3=85.3000, 4=94.4000, 5=87.4000, 6=94.2000, 7=93.2000, 8=96.3000, 9=94.8000, 42.9s 1.2h/2.3h
Epoch: [106]
epoch 106/200, train:, losses=0.4445, top1=81.5980, 0=98.4085, 1=98.3798, 2=96.2382, 3=94.2462, 4=97.8398, 5=94.2324, 6=98.2766, 7=97.8424, 8=98.8724, 9=98.5228, val:, losses=0.2568, top1=92.9400, 0=94.4000, 1=97.4000, 2=90.4000, 3=82.6000, 4=96.8000, 5=86.4000, 6=94.2000, 7=94.2000, 8=97.8000, 9=95.2000, test:, losses=0.2625, top1=92.7100, 0=93.4000, 1=96.5000, 2=90.9000, 3=84.5000, 4=95.4000, 5=86.9000, 6=94.5000, 7=93.8000, 8=96.3000, 9=94.9000, 42.1s 1.2h/2.3h
Epoch: [107]
epoch 107/200, train:, losses=0.4331, top1=81.7798, 0=97.8636, 1=98.6971, 2=96.6047, 3=93.7950, 4=97.4900, 5=94.7734, 6=98.3897, 7=98.0124, 8=98.8669, 9=98.3287, val:, losses=0.2288, top1=93.0000, 0=94.6000, 1=96.0000, 2=90.8000, 3=85.6000, 4=95.4000, 5=85.0000, 6=95.0000, 7=94.6000, 8=98.2000, 9=94.8000, test:, losses=0.2362, top1=92.6900, 0=93.2000, 1=95.3000, 2=90.6000, 3=86.6000, 4=94.4000, 5=86.0000, 6=94.8000, 7=94.5000, 8=96.7000, 9=94.8000, 41.5s 1.3h/2.3h
Epoch: [108]
epoch 108/200, train:, losses=0.4833, top1=79.6176, 0=98.1141, 1=99.3810, 2=96.1119, 3=94.7393, 4=98.0039, 5=94.6710, 6=98.4211, 7=98.1965, 8=99.1858, 9=98.6596, val:, losses=0.2390, top1=92.8400, 0=94.0000, 1=96.2000, 2=89.0000, 3=84.8000, 4=95.4000, 5=86.4000, 6=94.6000, 7=96.0000, 8=96.4000, 9=95.6000, test:, losses=0.2419, top1=92.4400, 0=92.1000, 1=95.9000, 2=90.3000, 3=86.1000, 4=94.1000, 5=87.4000, 6=92.6000, 7=95.1000, 8=95.1000, 9=95.7000, 41.3s 1.3h/2.3h
Epoch: [109]
epoch 109/200, train:, losses=0.4236, top1=82.7552, 0=98.0736, 1=99.1736, 2=96.5176, 3=95.5051, 4=98.0924, 5=95.3397, 6=99.1243, 7=98.4064, 8=98.7633, 9=98.4273, val:, losses=0.2490, top1=92.9000, 0=94.4000, 1=95.4000, 2=90.4000, 3=85.6000, 4=96.0000, 5=84.6000, 6=94.4000, 7=95.2000, 8=97.8000, 9=95.2000, test:, losses=0.2502, top1=92.6500, 0=93.9000, 1=94.7000, 2=91.0000, 3=85.6000, 4=94.6000, 5=85.9000, 6=94.0000, 7=94.8000, 8=96.4000, 9=95.6000, 41.9s 1.3h/2.3h
Epoch: [110]
epoch 110/200, train:, losses=0.4432, top1=82.1829, 0=97.8495, 1=99.3735, 2=96.4404, 3=95.4089, 4=97.8376, 5=95.2586, 6=98.3051, 7=98.2971, 8=98.7379, 9=98.4601, val:, losses=0.2649, top1=92.7800, 0=94.6000, 1=96.0000, 2=88.2000, 3=87.2000, 4=95.0000, 5=86.2000, 6=94.4000, 7=93.2000, 8=98.2000, 9=94.8000, test:, losses=0.2666, top1=92.4200, 0=93.8000, 1=95.8000, 2=89.3000, 3=87.4000, 4=94.2000, 5=86.1000, 6=93.9000, 7=92.4000, 8=96.9000, 9=94.4000, 41.8s 1.3h/2.3h
Epoch: [111]
epoch 111/200, train:, losses=0.4833, top1=79.9123, 0=98.4040, 1=99.1508, 2=96.6942, 3=94.6322, 4=98.1290, 5=95.5034, 6=98.3471, 7=98.7118, 8=99.1220, 9=98.3075, val:, losses=0.2360, top1=92.8400, 0=94.2000, 1=96.8000, 2=92.0000, 3=85.0000, 4=94.8000, 5=84.6000, 6=95.0000, 7=93.6000, 8=97.6000, 9=94.8000, test:, losses=0.2396, top1=92.6800, 0=93.6000, 1=95.4000, 2=91.7000, 3=85.4000, 4=94.6000, 5=85.3000, 6=95.3000, 7=93.8000, 8=96.0000, 9=95.7000, 42.8s 1.3h/2.3h
Epoch: [112]
epoch 112/200, train:, losses=0.4209, top1=82.7809, 0=97.7148, 1=99.0000, 2=96.8596, 3=95.4013, 4=98.0994, 5=96.2314, 6=98.2143, 7=98.9276, 8=99.0917, 9=99.0317, val:, losses=0.2448, top1=93.0800, 0=96.0000, 1=97.0000, 2=89.4000, 3=84.0000, 4=95.2000, 5=88.0000, 6=95.0000, 7=95.2000, 8=96.8000, 9=94.2000, test:, losses=0.2504, top1=92.5900, 0=94.3000, 1=96.4000, 2=89.7000, 3=84.8000, 4=94.3000, 5=88.0000, 6=95.0000, 7=94.0000, 8=95.0000, 9=94.4000, 42.5s 1.3h/2.3h
Epoch: [113]
epoch 113/200, train:, losses=0.4068, top1=83.3736, 0=98.1959, 1=99.0913, 2=97.1096, 3=95.4090, 4=97.7703, 5=96.0068, 6=98.9167, 7=98.5049, 8=99.0456, 9=98.4723, val:, losses=0.2427, top1=93.2000, 0=95.6000, 1=97.8000, 2=91.4000, 3=83.6000, 4=96.2000, 5=88.2000, 6=93.4000, 7=94.4000, 8=96.6000, 9=94.8000, test:, losses=0.2482, top1=92.6700, 0=94.4000, 1=96.6000, 2=90.9000, 3=85.0000, 4=94.0000, 5=88.4000, 6=93.1000, 7=93.9000, 8=95.1000, 9=95.3000, 43.0s 1.3h/2.3h
Epoch: [114]
epoch 114/200, train:, losses=0.4400, top1=82.0041, 0=98.4870, 1=99.3359, 2=96.9921, 3=95.5312, 4=98.1802, 5=96.0265, 6=98.5439, 7=98.6047, 8=99.0173, 9=98.9734, val:, losses=0.2407, top1=93.2200, 0=94.0000, 1=98.0000, 2=90.0000, 3=86.2000, 4=95.0000, 5=85.8000, 6=95.4000, 7=95.4000, 8=97.8000, 9=94.6000, test:, losses=0.2484, top1=92.4400, 0=93.8000, 1=96.8000, 2=88.8000, 3=85.8000, 4=94.0000, 5=86.4000, 6=94.9000, 7=93.7000, 8=95.9000, 9=94.3000, 42.8s 1.3h/2.3h
Epoch: [115]
epoch 115/200, train:, losses=0.4361, top1=82.4475, 0=98.4171, 1=99.3030, 2=96.9995, 3=96.5741, 4=98.0751, 5=95.9518, 6=98.5595, 7=98.6902, 8=98.9026, 9=98.7124, val:, losses=0.2437, top1=93.0200, 0=94.0000, 1=96.4000, 2=90.4000, 3=82.6000, 4=96.6000, 5=89.4000, 6=94.4000, 7=92.2000, 8=98.4000, 9=95.8000, test:, losses=0.2488, top1=92.6700, 0=93.5000, 1=95.5000, 2=90.4000, 3=82.7000, 4=95.6000, 5=89.7000, 6=94.7000, 7=92.7000, 8=96.4000, 9=95.5000, 42.8s 1.4h/2.3h
Epoch: [116]
epoch 116/200, train:, losses=0.4318, top1=81.7069, 0=98.1802, 1=99.0493, 2=97.5265, 3=95.5889, 4=98.7325, 5=96.0265, 6=98.9295, 7=98.8860, 8=99.1127, 9=98.8506, val:, losses=0.2606, top1=92.9200, 0=95.8000, 1=96.0000, 2=90.4000, 3=81.8000, 4=94.4000, 5=89.6000, 6=95.0000, 7=94.4000, 8=97.2000, 9=94.6000, test:, losses=0.2654, top1=92.4100, 0=94.1000, 1=95.8000, 2=90.9000, 3=82.3000, 4=93.2000, 5=89.2000, 6=94.9000, 7=93.4000, 8=95.3000, 9=95.0000, 42.1s 1.4h/2.3h
Epoch: [117]
epoch 117/200, train:, losses=0.4002, top1=83.6008, 0=98.7564, 1=99.0381, 2=97.2504, 3=96.0951, 4=98.5798, 5=95.7358, 6=99.1397, 7=98.8817, 8=98.9739, 9=98.2630, val:, losses=0.2419, top1=93.4000, 0=95.0000, 1=96.6000, 2=90.0000, 3=84.4000, 4=96.0000, 5=88.0000, 6=95.4000, 7=94.6000, 8=97.8000, 9=96.2000, test:, losses=0.2469, top1=92.9300, 0=93.3000, 1=96.3000, 2=89.8000, 3=85.5000, 4=95.5000, 5=87.9000, 6=94.7000, 7=94.3000, 8=95.7000, 9=96.3000, 41.5s 1.4h/2.3h
Epoch: [118]
epoch 118/200, train:, losses=0.4173, top1=82.7786, 0=98.7174, 1=99.0594, 2=97.2138, 3=95.4724, 4=98.6778, 5=96.4767, 6=98.7375, 7=98.5011, 8=99.1402, 9=98.7191, val:, losses=0.2315, top1=93.1600, 0=93.8000, 1=97.0000, 2=90.2000, 3=85.8000, 4=95.4000, 5=86.4000, 6=95.2000, 7=94.8000, 8=97.4000, 9=95.6000, test:, losses=0.2419, top1=92.7300, 0=92.8000, 1=96.1000, 2=91.5000, 3=86.2000, 4=94.5000, 5=86.5000, 6=94.3000, 7=94.5000, 8=95.5000, 9=95.4000, 41.5s 1.4h/2.3h
Epoch: [119]
epoch 119/200, train:, losses=0.4455, top1=81.6265, 0=98.5206, 1=99.7163, 2=97.0107, 3=96.6529, 4=97.8007, 5=96.1429, 6=98.8431, 7=98.6959, 8=99.1811, 9=98.9357, val:, losses=0.2383, top1=93.4000, 0=94.4000, 1=97.0000, 2=90.0000, 3=88.0000, 4=95.0000, 5=87.2000, 6=94.4000, 7=95.4000, 8=96.6000, 9=96.0000, test:, losses=0.2398, top1=92.8500, 0=94.0000, 1=96.5000, 2=90.2000, 3=88.6000, 4=94.1000, 5=86.5000, 6=93.5000, 7=94.5000, 8=95.7000, 9=94.9000, 42.3s 1.4h/2.3h
Epoch: [120]
epoch 120/200, train:, losses=0.4045, top1=82.8654, 0=98.5632, 1=99.0612, 2=97.5519, 3=96.5970, 4=98.5453, 5=96.1667, 6=99.0527, 7=99.1597, 8=99.2928, 9=98.5240, val:, losses=0.2472, top1=93.0800, 0=94.8000, 1=96.2000, 2=88.4000, 3=84.6000, 4=95.8000, 5=87.8000, 6=94.2000, 7=95.4000, 8=98.2000, 9=95.4000, test:, losses=0.2496, top1=92.7600, 0=93.7000, 1=96.0000, 2=88.4000, 3=87.2000, 4=94.6000, 5=87.4000, 6=93.7000, 7=94.6000, 8=96.9000, 9=95.1000, 42.6s 1.4h/2.3h
Epoch: [121]
epoch 121/200, train:, losses=0.4616, top1=81.2661, 0=98.9542, 1=99.0835, 2=97.3407, 3=96.6683, 4=97.9808, 5=96.7337, 6=98.9269, 7=98.4064, 8=99.2455, 9=99.3299, val:, losses=0.2418, top1=92.8600, 0=95.4000, 1=97.0000, 2=90.0000, 3=84.6000, 4=93.8000, 5=85.8000, 6=95.6000, 7=95.2000, 8=97.4000, 9=93.8000, test:, losses=0.2457, top1=92.6900, 0=95.0000, 1=96.6000, 2=90.7000, 3=86.4000, 4=93.0000, 5=86.2000, 6=95.9000, 7=94.9000, 8=95.0000, 9=93.2000, 41.7s 1.4h/2.3h
Epoch: [122]
epoch 122/200, train:, losses=0.4146, top1=83.3051, 0=98.8117, 1=99.4516, 2=97.1158, 3=95.9272, 4=98.0446, 5=96.3083, 6=98.9051, 7=98.6890, 8=99.1191, 9=98.9204, val:, losses=0.2267, top1=93.1000, 0=96.0000, 1=96.6000, 2=90.0000, 3=86.6000, 4=95.0000, 5=85.2000, 6=95.2000, 7=94.4000, 8=98.0000, 9=94.0000, test:, losses=0.2313, top1=92.8400, 0=94.4000, 1=96.5000, 2=89.5000, 3=87.9000, 4=94.0000, 5=85.8000, 6=95.2000, 7=94.7000, 8=96.6000, 9=93.8000, 40.3s 1.4h/2.3h
Epoch: [123]
epoch 123/200, train:, losses=0.4110, top1=82.9752, 0=98.7526, 1=99.0650, 2=97.3172, 3=96.4752, 4=98.2323, 5=96.8112, 6=98.9154, 7=98.8684, 8=99.0271, 9=98.4655, val:, losses=0.2330, top1=93.2800, 0=94.6000, 1=98.0000, 2=91.0000, 3=84.6000, 4=96.0000, 5=89.6000, 6=94.0000, 7=92.6000, 8=98.0000, 9=94.4000, test:, losses=0.2377, top1=92.4200, 0=92.5000, 1=96.7000, 2=90.4000, 3=84.6000, 4=94.9000, 5=87.7000, 6=93.0000, 7=94.0000, 8=96.8000, 9=93.6000, 41.5s 1.4h/2.3h
Epoch: [124]
epoch 124/200, train:, losses=0.4265, top1=82.8217, 0=98.2918, 1=99.2823, 2=97.6068, 3=96.6102, 4=98.8241, 5=96.8822, 6=99.1477, 7=99.1429, 8=99.4034, 9=98.8532, val:, losses=0.2705, top1=93.1200, 0=94.0000, 1=97.8000, 2=88.8000, 3=85.6000, 4=96.6000, 5=85.6000, 6=96.2000, 7=94.8000, 8=97.6000, 9=94.2000, test:, losses=0.2739, top1=92.6100, 0=93.6000, 1=97.2000, 2=88.4000, 3=86.6000, 4=94.7000, 5=85.3000, 6=95.9000, 7=94.3000, 8=96.1000, 9=94.0000, 42.7s 1.5h/2.3h
Epoch: [125]
epoch 125/200, train:, losses=0.4008, top1=83.6743, 0=98.8764, 1=99.3890, 2=97.8535, 3=96.1061, 4=98.5167, 5=96.0351, 6=98.9557, 7=98.7665, 8=99.0752, 9=99.1109, val:, losses=0.2378, top1=92.9200, 0=92.8000, 1=95.6000, 2=91.0000, 3=83.0000, 4=95.8000, 5=87.8000, 6=95.4000, 7=94.6000, 8=98.2000, 9=95.0000, test:, losses=0.2404, top1=92.8800, 0=92.3000, 1=95.6000, 2=91.2000, 3=83.5000, 4=95.7000, 5=89.1000, 6=94.6000, 7=94.4000, 8=96.4000, 9=96.0000, 42.7s 1.5h/2.3h
Epoch: [126]
epoch 126/200, train:, losses=0.3969, top1=83.2679, 0=98.9082, 1=99.5468, 2=97.4484, 3=96.5895, 4=98.7045, 5=96.9064, 6=99.0453, 7=98.8415, 8=99.1491, 9=99.0236, val:, losses=0.2193, top1=93.0800, 0=94.8000, 1=97.0000, 2=88.8000, 3=83.6000, 4=95.4000, 5=89.4000, 6=95.0000, 7=94.6000, 8=97.4000, 9=94.8000, test:, losses=0.2257, top1=92.8300, 0=93.9000, 1=96.8000, 2=88.3000, 3=84.6000, 4=94.7000, 5=89.2000, 6=94.6000, 7=95.0000, 8=96.2000, 9=95.0000, 40.7s 1.5h/2.3h
Epoch: [127]
epoch 127/200, train:, losses=0.4271, top1=82.2892, 0=98.7506, 1=99.0103, 2=97.7876, 3=96.7469, 4=98.8713, 5=97.1913, 6=98.9635, 7=98.9451, 8=99.0014, 9=99.0777, val:, losses=0.2672, top1=92.9000, 0=94.2000, 1=97.0000, 2=91.4000, 3=81.8000, 4=95.0000, 5=87.2000, 6=96.0000, 7=93.8000, 8=98.0000, 9=94.6000, test:, losses=0.2699, top1=92.7100, 0=94.0000, 1=96.1000, 2=90.5000, 3=83.8000, 4=95.7000, 5=87.0000, 6=96.2000, 7=93.3000, 8=95.8000, 9=94.7000, 41.1s 1.5h/2.3h
Epoch: [128]
epoch 128/200, train:, losses=0.4192, top1=82.9394, 0=98.7694, 1=99.2596, 2=98.4835, 3=96.4928, 4=98.3402, 5=96.5856, 6=99.0502, 7=99.3578, 8=99.2332, 9=98.9753, val:, losses=0.2462, top1=93.1400, 0=94.4000, 1=96.4000, 2=90.4000, 3=81.0000, 4=96.2000, 5=90.0000, 6=95.0000, 7=94.8000, 8=97.4000, 9=95.8000, test:, losses=0.2504, top1=92.7300, 0=93.2000, 1=95.5000, 2=88.7000, 3=82.2000, 4=95.7000, 5=90.2000, 6=94.2000, 7=94.2000, 8=97.0000, 9=96.4000, 41.3s 1.5h/2.3h
Epoch: [129]
epoch 129/200, train:, losses=0.3870, top1=84.0295, 0=98.9409, 1=99.3240, 2=97.3867, 3=97.5823, 4=98.9618, 5=97.1417, 6=99.0413, 7=98.9996, 8=99.1287, 9=99.0581, val:, losses=0.2435, top1=93.2600, 0=93.6000, 1=95.8000, 2=90.6000, 3=84.8000, 4=96.0000, 5=88.2000, 6=94.8000, 7=94.6000, 8=98.0000, 9=96.2000, test:, losses=0.2450, top1=92.7400, 0=92.5000, 1=96.7000, 2=89.4000, 3=86.1000, 4=95.4000, 5=86.7000, 6=93.9000, 7=93.8000, 8=96.9000, 9=96.0000, 43.4s 1.5h/2.3h
Epoch: [130]
epoch 130/200, train:, losses=0.4370, top1=82.1688, 0=98.8921, 1=99.3871, 2=98.3522, 3=97.0588, 4=98.2873, 5=97.1240, 6=99.2015, 7=99.0610, 8=99.3741, 9=99.1329, val:, losses=0.2519, top1=93.0600, 0=93.6000, 1=98.0000, 2=92.2000, 3=83.4000, 4=95.6000, 5=85.8000, 6=95.8000, 7=95.0000, 8=97.6000, 9=93.6000, test:, losses=0.2571, top1=92.6600, 0=92.0000, 1=97.2000, 2=91.6000, 3=84.9000, 4=94.5000, 5=86.0000, 6=95.1000, 7=95.3000, 8=96.4000, 9=93.6000, 43.4s 1.5h/2.3h
Epoch: [131]
epoch 131/200, train:, losses=0.4204, top1=83.0043, 0=98.9046, 1=99.4364, 2=97.9386, 3=96.7382, 4=98.6772, 5=96.8349, 6=98.9440, 7=98.9212, 8=99.5325, 9=98.8889, val:, losses=0.2436, top1=93.2400, 0=94.8000, 1=98.2000, 2=90.8000, 3=82.6000, 4=94.4000, 5=87.8000, 6=96.6000, 7=94.6000, 8=97.8000, 9=94.8000, test:, losses=0.2475, top1=92.6900, 0=94.8000, 1=97.7000, 2=89.5000, 3=83.3000, 4=93.6000, 5=88.2000, 6=95.4000, 7=95.1000, 8=96.1000, 9=93.2000, 41.9s 1.5h/2.3h
Epoch: [132]
epoch 132/200, train:, losses=0.3934, top1=83.9300, 0=98.3764, 1=99.5388, 2=97.8316, 3=96.8067, 4=98.8266, 5=97.3310, 6=99.0129, 7=98.8982, 8=99.0344, 9=99.0304, val:, losses=0.2302, top1=93.0800, 0=95.8000, 1=97.0000, 2=90.6000, 3=85.6000, 4=94.4000, 5=83.8000, 6=96.2000, 7=95.6000, 8=97.6000, 9=94.2000, test:, losses=0.2327, top1=92.9600, 0=94.1000, 1=96.3000, 2=90.8000, 3=87.6000, 4=94.5000, 5=85.2000, 6=95.6000, 7=95.3000, 8=95.7000, 9=94.5000, 41.9s 1.6h/2.3h
Epoch: [133]
epoch 133/200, train:, losses=0.4099, top1=82.6943, 0=98.9469, 1=98.9954, 2=97.6068, 3=96.5166, 4=98.8019, 5=96.5032, 6=99.2000, 7=98.9916, 8=99.2288, 9=98.9983, val:, losses=0.2313, top1=93.0200, 0=93.0000, 1=98.2000, 2=90.6000, 3=86.6000, 4=94.4000, 5=84.4000, 6=95.6000, 7=95.2000, 8=98.0000, 9=94.2000, test:, losses=0.2349, top1=92.9800, 0=92.1000, 1=97.3000, 2=90.7000, 3=89.4000, 4=95.1000, 5=84.8000, 6=94.2000, 7=95.4000, 8=96.7000, 9=94.1000, 42.8s 1.6h/2.3h
Epoch: [134]
epoch 134/200, train:, losses=0.3926, top1=83.8294, 0=98.9252, 1=99.6571, 2=98.1725, 3=97.1560, 4=98.7941, 5=97.0750, 6=98.9831, 7=99.0864, 8=99.4744, 9=99.1529, val:, losses=0.2237, top1=92.9800, 0=94.4000, 1=96.0000, 2=89.8000, 3=83.8000, 4=95.6000, 5=88.4000, 6=94.2000, 7=94.6000, 8=97.6000, 9=95.4000, test:, losses=0.2294, top1=92.9500, 0=93.0000, 1=96.3000, 2=89.1000, 3=85.1000, 4=96.0000, 5=89.5000, 6=93.7000, 7=94.6000, 8=96.7000, 9=95.5000, 42.6s 1.6h/2.3h
Epoch: [135]
epoch 135/200, train:, losses=0.3992, top1=83.5871, 0=98.5581, 1=99.5741, 2=98.0110, 3=97.7254, 4=98.7851, 5=97.7913, 6=99.1976, 7=99.3571, 8=99.5761, 9=99.3200, val:, losses=0.2494, top1=93.0600, 0=94.2000, 1=96.4000, 2=90.4000, 3=86.2000, 4=93.8000, 5=87.8000, 6=95.2000, 7=93.6000, 8=96.8000, 9=96.2000, test:, losses=0.2532, top1=92.7200, 0=92.8000, 1=96.5000, 2=90.3000, 3=87.7000, 4=93.7000, 5=87.1000, 6=93.8000, 7=93.1000, 8=96.4000, 9=95.8000, 42.0s 1.6h/2.3h
Epoch: [136]
epoch 136/200, train:, losses=0.3906, top1=83.8470, 0=98.8946, 1=99.3127, 2=98.2671, 3=96.6839, 4=98.8593, 5=97.0251, 6=98.8540, 7=98.8401, 8=99.4007, 9=99.1358, val:, losses=0.2494, top1=93.2200, 0=96.4000, 1=97.2000, 2=90.4000, 3=86.0000, 4=95.8000, 5=86.0000, 6=94.2000, 7=94.6000, 8=96.4000, 9=95.2000, test:, losses=0.2567, top1=92.5100, 0=94.8000, 1=96.3000, 2=88.8000, 3=86.6000, 4=95.6000, 5=85.4000, 6=93.9000, 7=94.2000, 8=95.1000, 9=94.4000, 41.2s 1.6h/2.3h
Epoch: [137]
epoch 137/200, train:, losses=0.4153, top1=82.9552, 0=98.8024, 1=99.4814, 2=97.9991, 3=97.8386, 4=98.8734, 5=97.3973, 6=99.3375, 7=99.1957, 8=99.3697, 9=99.3789, val:, losses=0.2432, top1=92.8800, 0=91.8000, 1=97.0000, 2=91.4000, 3=84.0000, 4=94.4000, 5=87.8000, 6=95.8000, 7=93.6000, 8=98.2000, 9=94.8000, test:, losses=0.2429, top1=92.8600, 0=91.1000, 1=96.2000, 2=92.3000, 3=84.1000, 4=95.2000, 5=88.4000, 6=95.9000, 7=93.6000, 8=96.5000, 9=95.3000, 41.5s 1.6h/2.3h
Epoch: [138]
epoch 138/200, train:, losses=0.4353, top1=82.0472, 0=98.9652, 1=99.2082, 2=98.2185, 3=97.0274, 4=98.9758, 5=97.6136, 6=99.2485, 7=98.8485, 8=99.2523, 9=99.2195, val:, losses=0.2500, top1=92.9400, 0=93.4000, 1=95.8000, 2=91.6000, 3=83.8000, 4=93.6000, 5=89.2000, 6=94.8000, 7=95.0000, 8=97.0000, 9=95.2000, test:, losses=0.2458, top1=92.8600, 0=92.8000, 1=97.0000, 2=91.7000, 3=84.8000, 4=93.9000, 5=89.5000, 6=93.3000, 7=93.9000, 8=96.0000, 9=95.7000, 41.6s 1.6h/2.3h
Epoch: [139]
epoch 139/200, train:, losses=0.3919, top1=83.9100, 0=98.9506, 1=99.3916, 2=98.0009, 3=97.6664, 4=99.1674, 5=97.9046, 6=99.1626, 7=99.3814, 8=99.3873, 9=99.4389, val:, losses=0.2388, top1=92.9600, 0=91.8000, 1=96.6000, 2=92.4000, 3=85.4000, 4=94.6000, 5=87.8000, 6=94.2000, 7=93.8000, 8=97.0000, 9=96.0000, test:, losses=0.2402, top1=93.2300, 0=92.8000, 1=96.7000, 2=92.1000, 3=89.5000, 4=94.2000, 5=85.9000, 6=93.8000, 7=94.5000, 8=96.4000, 9=96.4000, 42.2s 1.6h/2.3h
Epoch: [140]
epoch 140/200, train:, losses=0.4070, top1=83.1789, 0=99.1349, 1=99.5016, 2=97.9121, 3=96.9944, 4=98.6456, 5=97.5777, 6=99.1559, 7=98.6743, 8=99.2428, 9=99.0834, val:, losses=0.2211, top1=93.2600, 0=94.2000, 1=97.4000, 2=90.2000, 3=84.0000, 4=94.6000, 5=88.8000, 6=95.0000, 7=95.8000, 8=98.0000, 9=94.6000, test:, losses=0.2285, top1=92.9100, 0=93.3000, 1=96.6000, 2=89.6000, 3=85.5000, 4=94.2000, 5=89.3000, 6=93.8000, 7=95.2000, 8=96.6000, 9=95.0000, 42.3s 1.6h/2.3h
Epoch: [141]
epoch 141/200, train:, losses=0.4268, top1=82.1995, 0=98.6673, 1=99.3694, 2=98.1886, 3=97.0736, 4=98.5480, 5=97.9352, 6=99.3169, 7=99.0541, 8=99.5843, 9=99.1720, val:, losses=0.2494, top1=92.6400, 0=95.4000, 1=96.4000, 2=90.4000, 3=82.0000, 4=93.2000, 5=88.0000, 6=96.0000, 7=94.8000, 8=96.4000, 9=93.8000, test:, losses=0.2493, top1=92.8600, 0=94.6000, 1=96.7000, 2=89.8000, 3=83.8000, 4=94.6000, 5=89.0000, 6=95.2000, 7=94.8000, 8=95.6000, 9=94.5000, 41.3s 1.7h/2.3h
Epoch: [142]
epoch 142/200, train:, losses=0.4083, top1=82.8270, 0=99.1464, 1=99.4874, 2=98.4181, 3=97.0424, 4=98.5420, 5=97.3090, 6=99.2688, 7=98.9006, 8=99.2265, 9=99.3971, val:, losses=0.2472, top1=93.0000, 0=94.8000, 1=96.6000, 2=91.8000, 3=84.6000, 4=92.2000, 5=86.4000, 6=95.0000, 7=96.2000, 8=97.0000, 9=95.4000, test:, losses=0.2519, top1=92.6000, 0=93.9000, 1=96.4000, 2=90.5000, 3=85.1000, 4=93.1000, 5=85.6000, 6=94.4000, 7=95.7000, 8=96.5000, 9=94.8000, 42.0s 1.7h/2.3h
Epoch: [143]
epoch 143/200, train:, losses=0.4450, top1=81.5156, 0=99.1954, 1=99.4739, 2=98.5224, 3=97.5122, 4=98.9810, 5=97.2846, 6=99.3717, 7=98.9776, 8=99.1843, 9=99.1371, val:, losses=0.2339, top1=93.2000, 0=94.6000, 1=98.0000, 2=90.6000, 3=82.6000, 4=94.0000, 5=88.8000, 6=95.2000, 7=95.0000, 8=98.2000, 9=95.0000, test:, losses=0.2444, top1=92.7200, 0=93.6000, 1=97.5000, 2=89.3000, 3=83.9000, 4=94.4000, 5=88.0000, 6=94.8000, 7=94.4000, 8=96.5000, 9=94.8000, 42.0s 1.7h/2.3h
Epoch: [144]
epoch 144/200, train:, losses=0.4124, top1=82.9943, 0=98.8579, 1=99.5660, 2=98.6023, 3=97.4359, 4=98.7281, 5=97.5111, 6=99.2612, 7=99.0218, 8=99.3286, 9=99.3307, val:, losses=0.2359, top1=93.0800, 0=93.6000, 1=97.6000, 2=89.2000, 3=83.6000, 4=95.2000, 5=89.8000, 6=94.8000, 7=94.2000, 8=97.4000, 9=95.4000, test:, losses=0.2404, top1=92.7300, 0=92.7000, 1=96.7000, 2=89.1000, 3=85.5000, 4=95.5000, 5=88.1000, 6=93.4000, 7=94.7000, 8=96.3000, 9=95.3000, 42.3s 1.7h/2.3h
Epoch: [145]
epoch 145/200, train:, losses=0.4321, top1=82.8257, 0=99.0795, 1=99.4816, 2=98.4180, 3=97.8723, 4=98.7835, 5=98.0523, 6=99.2708, 7=99.2176, 8=99.5712, 9=99.2233, val:, losses=0.2593, top1=93.0400, 0=94.4000, 1=97.2000, 2=90.2000, 3=82.6000, 4=94.4000, 5=89.2000, 6=95.4000, 7=94.6000, 8=97.8000, 9=94.6000, test:, losses=0.2626, top1=92.7000, 0=93.4000, 1=96.8000, 2=91.3000, 3=84.5000, 4=94.4000, 5=88.1000, 6=94.6000, 7=93.6000, 8=96.0000, 9=94.3000, 42.6s 1.7h/2.3h
Epoch: [146]
epoch 146/200, train:, losses=0.3796, top1=84.4166, 0=99.3300, 1=99.3071, 2=98.5426, 3=97.2376, 4=98.8452, 5=97.3533, 6=99.0897, 7=99.3447, 8=99.1821, 9=98.6712, val:, losses=0.2714, top1=92.7400, 0=91.2000, 1=96.8000, 2=91.0000, 3=84.2000, 4=94.6000, 5=85.6000, 6=94.6000, 7=95.4000, 8=97.6000, 9=96.4000, test:, losses=0.2711, top1=92.8200, 0=89.4000, 1=96.5000, 2=91.6000, 3=86.2000, 4=95.7000, 5=86.4000, 6=94.9000, 7=95.3000, 8=96.9000, 9=95.3000, 42.1s 1.7h/2.3h
Epoch: [147]
epoch 147/200, train:, losses=0.4140, top1=82.8960, 0=99.0446, 1=99.3386, 2=98.3467, 3=97.4149, 4=99.2453, 5=97.7171, 6=98.9705, 7=99.2701, 8=99.4643, 9=99.0866, val:, losses=0.2285, top1=93.1800, 0=93.8000, 1=96.2000, 2=91.4000, 3=84.4000, 4=94.0000, 5=86.2000, 6=95.2000, 7=96.8000, 8=98.2000, 9=95.6000, test:, losses=0.2308, top1=93.0800, 0=93.0000, 1=96.8000, 2=91.1000, 3=86.0000, 4=95.1000, 5=87.3000, 6=94.2000, 7=95.7000, 8=97.0000, 9=94.6000, 42.0s 1.7h/2.3h
Epoch: [148]
epoch 148/200, train:, losses=0.4279, top1=81.9118, 0=98.8421, 1=99.7709, 2=98.3380, 3=97.8200, 4=99.0327, 5=97.5152, 6=99.4963, 7=99.3097, 8=99.4183, 9=99.3939, val:, losses=0.2253, top1=93.2000, 0=94.4000, 1=96.4000, 2=89.4000, 3=83.8000, 4=94.0000, 5=88.2000, 6=96.0000, 7=96.6000, 8=98.2000, 9=95.0000, test:, losses=0.2267, top1=93.1200, 0=93.2000, 1=96.4000, 2=90.7000, 3=86.4000, 4=95.2000, 5=87.8000, 6=94.6000, 7=95.3000, 8=96.5000, 9=95.1000, 43.4s 1.7h/2.3h
Epoch: [149]
epoch 149/200, train:, losses=0.4329, top1=81.9849, 0=99.1084, 1=99.3440, 2=98.2709, 3=97.4275, 4=98.7625, 5=98.1999, 6=99.1485, 7=99.0038, 8=99.3948, 9=99.5765, val:, losses=0.2236, top1=93.0600, 0=95.8000, 1=96.6000, 2=87.4000, 3=85.0000, 4=93.6000, 5=89.8000, 6=94.8000, 7=95.6000, 8=97.6000, 9=94.4000, test:, losses=0.2253, top1=93.0100, 0=94.7000, 1=96.6000, 2=89.3000, 3=85.7000, 4=93.6000, 5=90.0000, 6=94.3000, 7=94.6000, 8=96.2000, 9=95.1000, 43.2s 1.8h/2.3h
Epoch: [150]
epoch 150/200, train:, losses=0.3908, top1=83.7642, 0=99.0505, 1=99.5599, 2=98.5175, 3=98.8069, 4=99.0218, 5=98.2033, 6=99.4794, 7=99.3342, 8=99.1746, 9=99.5573, val:, losses=0.2269, top1=93.3600, 0=93.4000, 1=97.0000, 2=90.4000, 3=85.6000, 4=95.0000, 5=87.8000, 6=95.8000, 7=95.2000, 8=97.8000, 9=95.6000, test:, losses=0.2280, top1=93.3100, 0=93.4000, 1=96.8000, 2=90.9000, 3=86.0000, 4=95.3000, 5=88.9000, 6=95.2000, 7=94.2000, 8=96.9000, 9=95.5000, 42.0s 1.8h/2.3h
Epoch: [151]
epoch 151/200, train:, losses=0.3874, top1=83.9930, 0=99.2314, 1=99.7417, 2=98.9974, 3=98.3577, 4=99.0979, 5=98.1857, 6=99.3905, 7=99.3991, 8=99.6957, 9=99.4850, val:, losses=0.2411, top1=93.1400, 0=93.0000, 1=96.4000, 2=90.2000, 3=84.6000, 4=95.0000, 5=88.4000, 6=95.6000, 7=95.0000, 8=97.6000, 9=95.6000, test:, losses=0.2416, top1=93.2700, 0=92.7000, 1=96.8000, 2=91.0000, 3=84.7000, 4=95.7000, 5=89.8000, 6=95.4000, 7=94.0000, 8=97.2000, 9=95.4000, 42.6s 1.8h/2.3h
Epoch: [152]
epoch 152/200, train:, losses=0.3553, top1=85.1716, 0=99.3582, 1=99.6120, 2=98.9637, 3=98.3011, 4=99.4425, 5=98.4861, 6=99.4131, 7=99.4460, 8=99.8385, 9=99.5950, val:, losses=0.2315, top1=93.6000, 0=93.6000, 1=97.2000, 2=91.4000, 3=83.4000, 4=95.0000, 5=90.2000, 6=95.8000, 7=95.6000, 8=97.6000, 9=96.2000, test:, losses=0.2323, top1=93.2900, 0=93.2000, 1=96.8000, 2=91.8000, 3=84.0000, 4=95.7000, 5=90.2000, 6=95.8000, 7=94.2000, 8=96.1000, 9=95.1000, 40.9s 1.8h/2.3h
Epoch: [153]
epoch 153/200, train:, losses=0.4145, top1=82.3160, 0=99.3210, 1=99.5690, 2=99.2202, 3=97.9973, 4=99.3761, 5=98.6344, 6=99.5186, 7=99.4202, 8=99.5047, 9=99.6451, val:, losses=0.2238, top1=93.6200, 0=94.4000, 1=97.0000, 2=91.2000, 3=83.2000, 4=94.8000, 5=89.8000, 6=96.2000, 7=95.8000, 8=98.0000, 9=95.8000, test:, losses=0.2257, top1=93.3500, 0=93.5000, 1=96.5000, 2=91.8000, 3=84.2000, 4=95.5000, 5=90.3000, 6=96.0000, 7=94.1000, 8=96.4000, 9=95.2000, 42.4s 1.8h/2.3h
Epoch: [154]
epoch 154/200, train:, losses=0.3752, top1=84.5354, 0=99.0324, 1=99.6669, 2=98.8764, 3=98.6169, 4=99.4494, 5=98.6397, 6=99.3364, 7=99.3305, 8=99.5920, 9=99.4418, val:, losses=0.2441, top1=93.5200, 0=94.4000, 1=97.4000, 2=90.6000, 3=83.8000, 4=95.4000, 5=89.0000, 6=95.8000, 7=96.0000, 8=97.8000, 9=95.0000, test:, losses=0.2451, top1=93.2400, 0=93.4000, 1=96.6000, 2=90.8000, 3=84.9000, 4=95.5000, 5=90.0000, 6=95.7000, 7=94.2000, 8=96.9000, 9=94.4000, 42.9s 1.8h/2.3h
Epoch: [155]
epoch 155/200, train:, losses=0.3827, top1=84.1610, 0=99.4693, 1=99.5646, 2=99.0460, 3=98.7294, 4=99.4037, 5=98.3240, 6=99.5339, 7=99.5234, 8=99.6099, 9=99.2586, val:, losses=0.2288, top1=93.6000, 0=94.6000, 1=97.4000, 2=91.4000, 3=83.6000, 4=94.8000, 5=88.6000, 6=96.0000, 7=96.2000, 8=98.0000, 9=95.4000, test:, losses=0.2296, top1=93.3700, 0=93.6000, 1=96.9000, 2=92.0000, 3=84.8000, 4=95.4000, 5=89.3000, 6=95.9000, 7=94.8000, 8=96.3000, 9=94.7000, 41.2s 1.8h/2.3h
Epoch: [156]
epoch 156/200, train:, losses=0.3505, top1=85.5421, 0=99.5984, 1=99.7239, 2=99.1952, 3=98.4903, 4=99.0998, 5=98.7112, 6=99.5202, 7=99.4402, 8=99.4286, 9=99.5229, val:, losses=0.2098, top1=93.7800, 0=94.6000, 1=97.2000, 2=92.0000, 3=84.8000, 4=95.8000, 5=88.4000, 6=96.0000, 7=96.2000, 8=97.8000, 9=95.0000, test:, losses=0.2135, top1=93.5200, 0=93.7000, 1=96.8000, 2=91.9000, 3=85.9000, 4=96.0000, 5=88.9000, 6=95.4000, 7=95.2000, 8=96.5000, 9=94.9000, 39.9s 1.8h/2.3h
Epoch: [157]
epoch 157/200, train:, losses=0.4013, top1=83.7739, 0=99.5045, 1=99.6774, 2=99.2276, 3=98.4828, 4=99.5961, 5=98.8971, 6=99.6729, 7=99.7271, 8=99.5516, 9=99.3668, val:, losses=0.2595, top1=93.5400, 0=93.6000, 1=97.4000, 2=91.4000, 3=85.0000, 4=95.0000, 5=88.4000, 6=96.4000, 7=95.4000, 8=98.0000, 9=94.8000, test:, losses=0.2580, top1=93.1600, 0=92.9000, 1=96.6000, 2=91.0000, 3=86.4000, 4=95.3000, 5=88.3000, 6=96.0000, 7=94.2000, 8=96.4000, 9=94.5000, 41.4s 1.8h/2.3h
Epoch: [158]
epoch 158/200, train:, losses=0.3876, top1=84.3559, 0=99.4898, 1=99.7200, 2=98.8356, 3=98.8734, 4=99.1135, 5=98.3644, 6=99.4977, 7=99.4989, 8=99.9064, 9=99.4209, val:, losses=0.2242, top1=93.5200, 0=94.2000, 1=97.0000, 2=91.2000, 3=84.0000, 4=94.8000, 5=89.0000, 6=95.8000, 7=96.4000, 8=97.8000, 9=95.0000, test:, losses=0.2251, top1=93.3800, 0=93.5000, 1=96.8000, 2=91.8000, 3=85.6000, 4=94.7000, 5=89.4000, 6=95.6000, 7=94.9000, 8=96.9000, 9=94.6000, 41.9s 1.9h/2.3h
Epoch: [159]
epoch 159/200, train:, losses=0.4336, top1=82.1200, 0=99.5933, 1=99.7548, 2=98.7958, 3=98.7799, 4=99.2110, 5=98.2821, 6=99.6575, 7=99.6579, 8=99.4965, 9=99.5057, val:, losses=0.2609, top1=93.4600, 0=93.8000, 1=97.4000, 2=90.8000, 3=83.6000, 4=95.6000, 5=88.8000, 6=96.4000, 7=95.4000, 8=97.8000, 9=95.0000, test:, losses=0.2600, top1=93.2200, 0=93.0000, 1=97.1000, 2=91.0000, 3=85.5000, 4=95.7000, 5=88.9000, 6=95.8000, 7=94.3000, 8=96.4000, 9=94.5000, 42.8s 1.9h/2.3h
Epoch: [160]
epoch 160/200, train:, losses=0.4176, top1=83.0186, 0=99.3093, 1=99.6228, 2=99.1819, 3=98.7155, 4=99.4256, 5=98.8095, 6=99.3836, 7=99.9043, 8=99.8536, 9=99.3333, val:, losses=0.2418, top1=93.5000, 0=94.2000, 1=97.4000, 2=90.8000, 3=82.6000, 4=95.2000, 5=89.2000, 6=96.2000, 7=95.6000, 8=98.0000, 9=95.8000, test:, losses=0.2427, top1=93.1700, 0=93.5000, 1=96.5000, 2=91.2000, 3=84.7000, 4=95.3000, 5=88.7000, 6=96.0000, 7=94.3000, 8=96.5000, 9=95.0000, 42.7s 1.9h/2.3h
Epoch: [161]
epoch 161/200, train:, losses=0.4152, top1=82.4334, 0=99.5550, 1=99.4917, 2=99.0549, 3=99.1252, 4=99.4998, 5=98.8278, 6=99.1465, 7=99.6885, 8=99.7251, 9=99.5023, val:, losses=0.2123, top1=93.6000, 0=94.0000, 1=97.2000, 2=91.4000, 3=84.8000, 4=95.6000, 5=88.2000, 6=95.8000, 7=95.8000, 8=98.0000, 9=95.2000, test:, losses=0.2165, top1=93.4900, 0=93.7000, 1=97.1000, 2=91.9000, 3=86.0000, 4=95.9000, 5=88.6000, 6=95.2000, 7=94.7000, 8=96.9000, 9=94.9000, 41.1s 1.9h/2.3h
Epoch: [162]
epoch 162/200, train:, losses=0.4056, top1=83.1011, 0=99.4273, 1=99.7795, 2=99.2231, 3=98.8938, 4=99.3392, 5=98.7404, 6=99.4626, 7=99.3377, 8=99.8229, 9=99.6422, val:, losses=0.2392, top1=93.4400, 0=94.8000, 1=97.2000, 2=90.6000, 3=84.6000, 4=94.6000, 5=88.4000, 6=95.0000, 7=96.2000, 8=98.0000, 9=95.0000, test:, losses=0.2393, top1=93.3200, 0=94.3000, 1=97.2000, 2=91.4000, 3=86.0000, 4=95.1000, 5=88.5000, 6=95.0000, 7=94.7000, 8=96.5000, 9=94.5000, 41.7s 1.9h/2.3h
Epoch: [163]
epoch 163/200, train:, losses=0.4142, top1=83.0961, 0=99.2901, 1=99.4982, 2=99.1659, 3=98.7007, 4=99.3088, 5=98.5294, 6=99.4028, 7=99.4006, 8=99.8131, 9=99.5296, val:, losses=0.2436, top1=93.4800, 0=94.2000, 1=97.2000, 2=90.2000, 3=83.2000, 4=95.2000, 5=89.8000, 6=96.0000, 7=95.6000, 8=98.0000, 9=95.4000, test:, losses=0.2436, top1=93.2700, 0=93.4000, 1=96.7000, 2=91.1000, 3=84.8000, 4=95.6000, 5=89.5000, 6=95.7000, 7=94.6000, 8=96.6000, 9=94.7000, 42.9s 1.9h/2.3h
Epoch: [164]
epoch 164/200, train:, losses=0.3840, top1=83.9270, 0=99.6971, 1=99.8282, 2=99.1139, 3=98.7644, 4=99.5399, 5=98.8070, 6=99.4913, 7=99.4022, 8=99.7438, 9=99.4913, val:, losses=0.2285, top1=93.4400, 0=94.2000, 1=96.4000, 2=90.2000, 3=84.6000, 4=94.8000, 5=89.4000, 6=96.0000, 7=96.0000, 8=98.0000, 9=94.8000, test:, losses=0.2289, top1=93.2300, 0=93.3000, 1=96.4000, 2=90.8000, 3=85.9000, 4=95.1000, 5=88.8000, 6=95.6000, 7=94.3000, 8=96.9000, 9=95.2000, 43.0s 1.9h/2.3h
Epoch: [165]
epoch 165/200, train:, losses=0.3868, top1=83.8261, 0=99.6996, 1=99.8729, 2=99.5355, 3=98.8039, 4=99.4878, 5=98.4576, 6=99.5015, 7=99.3622, 8=99.5366, 9=99.7011, val:, losses=0.2247, top1=93.5400, 0=94.0000, 1=97.4000, 2=91.2000, 3=83.4000, 4=95.0000, 5=90.4000, 6=95.8000, 7=96.2000, 8=98.0000, 9=94.0000, test:, losses=0.2252, top1=93.2600, 0=93.5000, 1=96.9000, 2=91.8000, 3=84.3000, 4=95.2000, 5=89.7000, 6=95.3000, 7=94.6000, 8=96.7000, 9=94.6000, 42.3s 1.9h/2.3h
Epoch: [166]
epoch 166/200, train:, losses=0.4076, top1=82.8717, 0=99.5643, 1=99.7861, 2=98.9912, 3=98.6486, 4=99.3926, 5=98.6603, 6=99.4291, 7=99.4311, 8=99.6549, 9=99.4004, val:, losses=0.2352, top1=93.5800, 0=94.2000, 1=96.8000, 2=90.4000, 3=84.8000, 4=95.2000, 5=89.0000, 6=96.0000, 7=96.4000, 8=97.8000, 9=95.2000, test:, losses=0.2349, top1=93.1700, 0=93.5000, 1=96.5000, 2=90.6000, 3=85.5000, 4=95.5000, 5=88.7000, 6=95.3000, 7=95.0000, 8=96.3000, 9=94.8000, 42.9s 2.0h/2.3h
Epoch: [167]
epoch 167/200, train:, losses=0.3744, top1=84.7240, 0=99.5998, 1=99.7390, 2=99.4597, 3=98.5599, 4=99.3249, 5=98.6902, 6=99.2860, 7=99.5425, 8=99.7805, 9=99.6473, val:, losses=0.2228, top1=93.7200, 0=93.8000, 1=96.8000, 2=91.8000, 3=85.0000, 4=95.2000, 5=89.2000, 6=96.0000, 7=96.4000, 8=98.0000, 9=95.0000, test:, losses=0.2242, top1=93.2600, 0=93.4000, 1=96.5000, 2=91.2000, 3=85.5000, 4=95.5000, 5=88.8000, 6=95.6000, 7=94.4000, 8=96.8000, 9=94.9000, 41.7s 2.0h/2.3h
Epoch: [168]
epoch 168/200, train:, losses=0.3771, top1=84.3736, 0=99.6188, 1=99.7502, 2=99.2073, 3=98.9809, 4=99.6636, 5=98.9371, 6=99.6085, 7=99.4492, 8=99.6619, 9=99.5327, val:, losses=0.2256, top1=93.7400, 0=94.0000, 1=96.8000, 2=91.8000, 3=83.8000, 4=95.0000, 5=89.8000, 6=95.8000, 7=96.2000, 8=97.8000, 9=96.4000, test:, losses=0.2269, top1=93.2800, 0=93.3000, 1=96.4000, 2=91.8000, 3=84.8000, 4=95.6000, 5=89.1000, 6=95.2000, 7=94.6000, 8=96.3000, 9=95.7000, 42.5s 2.0h/2.3h
Epoch: [169]
epoch 169/200, train:, losses=0.4045, top1=83.2628, 0=99.1775, 1=99.7792, 2=99.1413, 3=98.8703, 4=99.2653, 5=98.8798, 6=99.6617, 7=99.6014, 8=99.6935, 9=99.4674, val:, losses=0.2670, top1=93.5400, 0=92.4000, 1=97.4000, 2=91.0000, 3=84.0000, 4=95.2000, 5=89.8000, 6=95.8000, 7=96.0000, 8=97.6000, 9=96.2000, test:, losses=0.2659, top1=93.0800, 0=92.4000, 1=96.4000, 2=91.6000, 3=84.9000, 4=95.3000, 5=88.4000, 6=95.4000, 7=94.5000, 8=96.5000, 9=95.4000, 42.5s 2.0h/2.3h
Epoch: [170]
epoch 170/200, train:, losses=0.3765, top1=84.3121, 0=99.1903, 1=99.5650, 2=99.0885, 3=99.3794, 4=99.3824, 5=99.0761, 6=99.7803, 7=99.6502, 8=99.7367, 9=99.6025, val:, losses=0.2459, top1=93.6000, 0=93.6000, 1=97.4000, 2=92.2000, 3=83.4000, 4=95.2000, 5=88.0000, 6=96.6000, 7=96.0000, 8=98.0000, 9=95.6000, test:, losses=0.2457, top1=93.2100, 0=93.5000, 1=96.6000, 2=91.5000, 3=85.1000, 4=95.3000, 5=88.3000, 6=95.9000, 7=94.5000, 8=96.4000, 9=95.0000, 41.7s 2.0h/2.3h
Epoch: [171]
epoch 171/200, train:, losses=0.3885, top1=83.8865, 0=99.6435, 1=99.6970, 2=98.9305, 3=98.7208, 4=99.3800, 5=98.6225, 6=99.6002, 7=99.5585, 8=99.6499, 9=99.5721, val:, losses=0.2422, top1=93.5000, 0=93.6000, 1=97.4000, 2=91.6000, 3=84.0000, 4=95.0000, 5=89.4000, 6=95.6000, 7=95.6000, 8=97.8000, 9=95.0000, test:, losses=0.2428, top1=93.1000, 0=93.1000, 1=96.5000, 2=92.0000, 3=84.9000, 4=95.4000, 5=89.0000, 6=94.7000, 7=94.5000, 8=96.3000, 9=94.6000, 42.2s 2.0h/2.3h
Epoch: [172]
epoch 172/200, train:, losses=0.3936, top1=84.0750, 0=99.5989, 1=99.6540, 2=99.3857, 3=98.9658, 4=99.3733, 5=99.2441, 6=99.4697, 7=99.6893, 8=99.6892, 9=99.3013, val:, losses=0.2162, top1=93.6400, 0=94.6000, 1=96.6000, 2=92.0000, 3=85.4000, 4=95.4000, 5=88.0000, 6=95.6000, 7=95.4000, 8=98.0000, 9=95.4000, test:, losses=0.2183, top1=93.4000, 0=94.0000, 1=96.5000, 2=91.5000, 3=86.0000, 4=95.7000, 5=88.6000, 6=95.0000, 7=95.0000, 8=96.6000, 9=95.1000, 42.1s 2.0h/2.3h
Epoch: [173]
epoch 173/200, train:, losses=0.3680, top1=84.9504, 0=99.3582, 1=99.7433, 2=98.9035, 3=98.7179, 4=99.3918, 5=98.6111, 6=99.6460, 7=99.4085, 8=99.6941, 9=99.7450, val:, losses=0.2445, top1=93.4400, 0=94.0000, 1=97.2000, 2=91.8000, 3=84.2000, 4=95.0000, 5=86.8000, 6=96.2000, 7=95.4000, 8=98.2000, 9=95.6000, test:, losses=0.2442, top1=93.2100, 0=93.9000, 1=97.0000, 2=91.5000, 3=85.0000, 4=95.3000, 5=88.3000, 6=95.9000, 7=94.3000, 8=96.5000, 9=94.4000, 41.4s 2.0h/2.3h
Epoch: [174]
epoch 174/200, train:, losses=0.4059, top1=83.2188, 0=99.6793, 1=99.7768, 2=98.9728, 3=98.8844, 4=99.3357, 5=99.2727, 6=99.6377, 7=99.5514, 8=99.5853, 9=99.7331, val:, losses=0.2315, top1=93.8200, 0=94.8000, 1=97.0000, 2=92.0000, 3=85.0000, 4=95.4000, 5=87.8000, 6=95.8000, 7=96.6000, 8=98.2000, 9=95.6000, test:, losses=0.2320, top1=93.2900, 0=93.7000, 1=96.9000, 2=91.4000, 3=85.6000, 4=96.0000, 5=88.1000, 6=94.8000, 7=94.9000, 8=96.6000, 9=94.9000, 42.1s 2.0h/2.3h
Epoch: [175]
epoch 175/200, train:, losses=0.3620, top1=84.8733, 0=99.5002, 1=99.7135, 2=99.0262, 3=98.8618, 4=99.2593, 5=98.9274, 6=99.3937, 7=99.5473, 8=99.9593, 9=99.6218, val:, losses=0.2133, top1=93.9200, 0=95.2000, 1=97.2000, 2=91.6000, 3=86.6000, 4=95.0000, 5=87.4000, 6=96.0000, 7=96.0000, 8=98.2000, 9=96.0000, test:, losses=0.2173, top1=93.3300, 0=93.9000, 1=96.7000, 2=90.8000, 3=87.2000, 4=95.4000, 5=87.8000, 6=94.7000, 7=95.0000, 8=96.5000, 9=95.3000, 41.3s 2.1h/2.3h
Epoch: [176]
epoch 176/200, train:, losses=0.3995, top1=83.3522, 0=99.7378, 1=99.7811, 2=99.1548, 3=98.8189, 4=99.5612, 5=98.5550, 6=99.6553, 7=99.5639, 8=99.6893, 9=99.3498, val:, losses=0.2415, top1=93.5600, 0=94.6000, 1=97.0000, 2=91.2000, 3=85.2000, 4=94.6000, 5=88.6000, 6=95.6000, 7=95.6000, 8=97.4000, 9=95.8000, test:, losses=0.2426, top1=93.2300, 0=93.7000, 1=96.7000, 2=91.6000, 3=86.1000, 4=95.3000, 5=88.3000, 6=94.8000, 7=94.6000, 8=96.1000, 9=95.1000, 41.4s 2.1h/2.3h
Epoch: [177]
epoch 177/200, train:, losses=0.4043, top1=83.3341, 0=99.4403, 1=99.6063, 2=99.3464, 3=98.5975, 4=99.4408, 5=98.9222, 6=99.7592, 7=99.6599, 8=99.7621, 9=99.1888, val:, losses=0.2191, top1=93.9000, 0=94.2000, 1=97.4000, 2=91.8000, 3=85.6000, 4=95.2000, 5=89.0000, 6=96.2000, 7=96.4000, 8=97.8000, 9=95.4000, test:, losses=0.2221, top1=93.2500, 0=93.6000, 1=96.8000, 2=91.1000, 3=85.8000, 4=95.7000, 5=88.4000, 6=95.4000, 7=94.8000, 8=95.9000, 9=95.0000, 41.2s 2.1h/2.3h
Epoch: [178]
epoch 178/200, train:, losses=0.3989, top1=83.7890, 0=99.6312, 1=99.6239, 2=99.3179, 3=98.8642, 4=99.4413, 5=99.0650, 6=99.4321, 7=99.3030, 8=99.8191, 9=99.4017, val:, losses=0.2221, top1=93.6000, 0=94.4000, 1=97.4000, 2=91.2000, 3=84.0000, 4=95.0000, 5=89.8000, 6=95.6000, 7=95.4000, 8=97.8000, 9=95.4000, test:, losses=0.2239, top1=93.2700, 0=93.5000, 1=96.7000, 2=91.2000, 3=85.8000, 4=95.3000, 5=89.5000, 6=94.9000, 7=94.6000, 8=95.9000, 9=95.3000, 42.3s 2.1h/2.3h
Epoch: [179]
epoch 179/200, train:, losses=0.4107, top1=82.8112, 0=99.5563, 1=99.7683, 2=98.9001, 3=98.8474, 4=99.3398, 5=99.1589, 6=99.3830, 7=99.4112, 8=99.6777, 9=99.7766, val:, losses=0.2383, top1=93.6600, 0=93.8000, 1=97.2000, 2=91.0000, 3=84.6000, 4=95.0000, 5=88.6000, 6=96.2000, 7=96.2000, 8=97.8000, 9=96.2000, test:, losses=0.2387, top1=93.1200, 0=92.8000, 1=96.7000, 2=91.1000, 3=84.9000, 4=95.6000, 5=88.2000, 6=95.4000, 7=95.0000, 8=96.3000, 9=95.2000, 43.0s 2.1h/2.3h
Epoch: [180]
epoch 180/200, train:, losses=0.3531, top1=85.4472, 0=99.3554, 1=99.5006, 2=99.0046, 3=98.7195, 4=99.4184, 5=98.8852, 6=99.6796, 7=99.4229, 8=99.7122, 9=99.6279, val:, losses=0.2546, top1=93.4600, 0=93.8000, 1=97.0000, 2=91.6000, 3=83.2000, 4=95.0000, 5=89.0000, 6=96.0000, 7=95.8000, 8=98.0000, 9=95.2000, test:, losses=0.2548, top1=93.0500, 0=93.3000, 1=97.0000, 2=91.3000, 3=84.5000, 4=95.5000, 5=88.5000, 6=95.0000, 7=94.9000, 8=96.3000, 9=94.2000, 42.0s 2.1h/2.3h
Epoch: [181]
epoch 181/200, train:, losses=0.4130, top1=82.6498, 0=99.3222, 1=99.4998, 2=98.9111, 3=99.1256, 4=99.3144, 5=98.5122, 6=99.5894, 7=99.4149, 8=99.7205, 9=99.4555, val:, losses=0.2302, top1=93.5200, 0=94.4000, 1=97.4000, 2=90.0000, 3=84.2000, 4=94.6000, 5=89.6000, 6=96.0000, 7=95.6000, 8=98.2000, 9=95.2000, test:, losses=0.2307, top1=93.3400, 0=93.1000, 1=97.0000, 2=91.0000, 3=86.7000, 4=94.7000, 5=89.0000, 6=95.3000, 7=94.8000, 8=96.9000, 9=94.9000, 41.9s 2.1h/2.3h
Epoch: [182]
epoch 182/200, train:, losses=0.3888, top1=84.0895, 0=99.6448, 1=99.6773, 2=99.0781, 3=98.9432, 4=99.4725, 5=98.8283, 6=99.3243, 7=99.6864, 8=99.7319, 9=99.2694, val:, losses=0.2257, top1=93.7400, 0=94.4000, 1=97.0000, 2=91.6000, 3=84.4000, 4=95.0000, 5=88.8000, 6=96.2000, 7=96.2000, 8=98.2000, 9=95.6000, test:, losses=0.2276, top1=93.2800, 0=93.4000, 1=96.5000, 2=91.5000, 3=84.4000, 4=95.9000, 5=88.6000, 6=95.5000, 7=95.2000, 8=96.7000, 9=95.1000, 42.6s 2.1h/2.3h
Epoch: [183]
epoch 183/200, train:, losses=0.3959, top1=83.4991, 0=99.5011, 1=99.6889, 2=99.1877, 3=99.3566, 4=99.5550, 5=99.2223, 6=99.8227, 7=99.7759, 8=99.6868, 9=99.7290, val:, losses=0.2218, top1=93.6800, 0=94.4000, 1=97.4000, 2=91.6000, 3=85.0000, 4=94.8000, 5=89.4000, 6=95.8000, 7=95.8000, 8=98.0000, 9=94.6000, test:, losses=0.2237, top1=93.3000, 0=93.7000, 1=96.9000, 2=91.9000, 3=85.9000, 4=95.2000, 5=88.9000, 6=94.9000, 7=94.2000, 8=97.1000, 9=94.3000, 43.0s 2.2h/2.3h
Epoch: [184]
epoch 184/200, train:, losses=0.3924, top1=84.1052, 0=99.6183, 1=99.6676, 2=99.1228, 3=99.2544, 4=99.3020, 5=98.7454, 6=99.6253, 7=99.6789, 8=99.6305, 9=99.4891, val:, losses=0.2132, top1=93.8600, 0=94.4000, 1=97.4000, 2=91.6000, 3=86.0000, 4=95.2000, 5=88.6000, 6=96.2000, 7=96.0000, 8=98.0000, 9=95.2000, test:, losses=0.2166, top1=93.4500, 0=94.0000, 1=96.6000, 2=91.4000, 3=86.3000, 4=95.9000, 5=88.2000, 6=95.6000, 7=94.9000, 8=96.7000, 9=94.9000, 44.2s 2.2h/2.3h
Epoch: [185]
epoch 185/200, train:, losses=0.3903, top1=83.6480, 0=99.4365, 1=99.6943, 2=99.3109, 3=99.0158, 4=99.2679, 5=98.6237, 6=99.4843, 7=99.4716, 8=99.6983, 9=99.7444, val:, losses=0.2406, top1=93.5600, 0=94.4000, 1=97.4000, 2=91.4000, 3=84.4000, 4=95.0000, 5=88.4000, 6=96.2000, 7=96.2000, 8=97.6000, 9=94.6000, test:, losses=0.2409, top1=93.1300, 0=93.3000, 1=97.1000, 2=91.4000, 3=86.0000, 4=95.5000, 5=88.1000, 6=95.2000, 7=94.6000, 8=96.1000, 9=94.0000, 42.0s 2.2h/2.3h
Epoch: [186]
epoch 186/200, train:, losses=0.4439, top1=81.6092, 0=99.4675, 1=99.7323, 2=99.3597, 3=99.0967, 4=99.4550, 5=99.2647, 6=99.7875, 7=99.5895, 8=99.6880, 9=99.6830, val:, losses=0.2449, top1=93.5800, 0=94.4000, 1=97.6000, 2=90.6000, 3=84.0000, 4=95.2000, 5=89.4000, 6=96.2000, 7=96.2000, 8=97.6000, 9=94.6000, test:, losses=0.2450, top1=93.0400, 0=93.7000, 1=97.0000, 2=90.1000, 3=85.1000, 4=95.4000, 5=88.8000, 6=95.6000, 7=94.6000, 8=95.9000, 9=94.2000, 41.7s 2.2h/2.3h
Epoch: [187]
epoch 187/200, train:, losses=0.3618, top1=85.2537, 0=99.3229, 1=99.5781, 2=99.3325, 3=99.0894, 4=99.3779, 5=99.0783, 6=99.7113, 7=99.4715, 8=99.6660, 9=99.5492, val:, losses=0.2374, top1=93.4400, 0=94.0000, 1=97.0000, 2=90.8000, 3=85.2000, 4=95.0000, 5=88.2000, 6=96.2000, 7=95.4000, 8=97.8000, 9=94.8000, test:, losses=0.2378, top1=93.3000, 0=93.0000, 1=96.7000, 2=91.9000, 3=86.4000, 4=95.7000, 5=88.2000, 6=95.3000, 7=94.7000, 8=96.4000, 9=94.7000, 41.4s 2.2h/2.3h
Epoch: [188]
epoch 188/200, train:, losses=0.3705, top1=84.6294, 0=99.5327, 1=99.7880, 2=99.2057, 3=99.2013, 4=99.4100, 5=98.7030, 6=99.6634, 7=99.7437, 8=99.6526, 9=99.5765, val:, losses=0.2175, top1=93.6400, 0=94.6000, 1=97.2000, 2=90.4000, 3=85.8000, 4=95.0000, 5=89.4000, 6=96.2000, 7=95.4000, 8=98.0000, 9=94.4000, test:, losses=0.2196, top1=93.3500, 0=93.7000, 1=96.7000, 2=90.6000, 3=86.7000, 4=95.9000, 5=88.6000, 6=95.7000, 7=95.0000, 8=96.2000, 9=94.4000, 42.2s 2.2h/2.3h
Epoch: [189]
epoch 189/200, train:, losses=0.3914, top1=83.4866, 0=99.7408, 1=99.8689, 2=98.8735, 3=98.7714, 4=99.6111, 5=98.6289, 6=99.6907, 7=99.6552, 8=99.7005, 9=99.3475, val:, losses=0.2226, top1=93.7800, 0=94.2000, 1=97.4000, 2=92.4000, 3=85.0000, 4=95.2000, 5=88.6000, 6=96.0000, 7=96.4000, 8=97.8000, 9=94.8000, test:, losses=0.2239, top1=93.1900, 0=93.0000, 1=96.9000, 2=91.7000, 3=85.0000, 4=95.7000, 5=88.2000, 6=95.5000, 7=95.1000, 8=96.3000, 9=94.5000, 42.8s 2.2h/2.3h
Epoch: [190]
epoch 190/200, train:, losses=0.3829, top1=84.2370, 0=99.5011, 1=99.4707, 2=99.2647, 3=98.8447, 4=99.4488, 5=99.0318, 6=99.7309, 7=99.4144, 8=99.6851, 9=99.5423, val:, losses=0.2258, top1=93.5800, 0=94.0000, 1=97.6000, 2=91.8000, 3=83.4000, 4=95.0000, 5=89.6000, 6=96.0000, 7=95.8000, 8=97.8000, 9=94.8000, test:, losses=0.2276, top1=93.2100, 0=93.2000, 1=97.0000, 2=91.6000, 3=84.6000, 4=95.7000, 5=89.1000, 6=95.2000, 7=94.8000, 8=96.4000, 9=94.5000, 41.9s 2.2h/2.3h
Epoch: [191]
epoch 191/200, train:, losses=0.3870, top1=83.9216, 0=99.6039, 1=99.7804, 2=99.2174, 3=98.9432, 4=99.3435, 5=98.8286, 6=99.6994, 7=99.4789, 8=99.6927, 9=99.5201, val:, losses=0.2224, top1=93.7200, 0=94.2000, 1=97.4000, 2=91.8000, 3=84.6000, 4=95.4000, 5=88.8000, 6=96.0000, 7=96.4000, 8=98.0000, 9=94.6000, test:, losses=0.2245, top1=93.2900, 0=93.3000, 1=97.3000, 2=91.7000, 3=84.2000, 4=96.1000, 5=88.4000, 6=95.5000, 7=95.0000, 8=96.7000, 9=94.7000, 41.1s 2.2h/2.3h
Epoch: [192]
epoch 192/200, train:, losses=0.3903, top1=83.8483, 0=99.5996, 1=99.7370, 2=99.2428, 3=99.2325, 4=99.6553, 5=99.0621, 6=99.7344, 7=99.5658, 8=99.8621, 9=99.5540, val:, losses=0.2300, top1=93.6600, 0=94.2000, 1=97.2000, 2=91.4000, 3=83.8000, 4=95.4000, 5=89.8000, 6=96.2000, 7=95.4000, 8=97.8000, 9=95.4000, test:, losses=0.2310, top1=93.1900, 0=93.3000, 1=96.5000, 2=91.2000, 3=84.3000, 4=95.5000, 5=89.0000, 6=95.8000, 7=94.5000, 8=96.6000, 9=95.2000, 41.3s 2.3h/2.3h
Epoch: [193]
epoch 193/200, train:, losses=0.3837, top1=84.0375, 0=99.3846, 1=99.7847, 2=99.3945, 3=99.0642, 4=99.6619, 5=98.8689, 6=99.7872, 7=99.4801, 8=99.6089, 9=99.4268, val:, losses=0.2357, top1=93.6600, 0=94.2000, 1=97.0000, 2=90.6000, 3=84.4000, 4=95.4000, 5=90.0000, 6=96.0000, 7=95.2000, 8=98.0000, 9=95.8000, test:, losses=0.2362, top1=93.2600, 0=93.0000, 1=96.5000, 2=91.4000, 3=84.2000, 4=95.8000, 5=89.2000, 6=95.6000, 7=94.7000, 8=96.9000, 9=95.3000, 41.7s 2.3h/2.3h
Epoch: [194]
epoch 194/200, train:, losses=0.3726, top1=84.4073, 0=99.6217, 1=99.8321, 2=99.1547, 3=98.4302, 4=99.4930, 5=99.0066, 6=99.3740, 7=99.5303, 8=99.6685, 9=99.4561, val:, losses=0.2286, top1=93.6400, 0=93.8000, 1=97.2000, 2=90.8000, 3=85.8000, 4=95.6000, 5=88.6000, 6=95.8000, 7=96.0000, 8=97.6000, 9=95.2000, test:, losses=0.2291, top1=93.2400, 0=92.7000, 1=97.0000, 2=91.8000, 3=86.1000, 4=95.6000, 5=88.4000, 6=95.1000, 7=94.5000, 8=96.3000, 9=94.9000, 42.5s 2.3h/2.3h
Epoch: [195]
epoch 195/200, train:, losses=0.3939, top1=83.6177, 0=99.6471, 1=99.7340, 2=99.3289, 3=99.1059, 4=99.4326, 5=99.1903, 6=99.7297, 7=99.6070, 8=99.7330, 9=99.7280, val:, losses=0.2269, top1=93.7800, 0=94.4000, 1=97.4000, 2=92.0000, 3=87.4000, 4=95.0000, 5=88.2000, 6=95.6000, 7=95.0000, 8=97.8000, 9=95.0000, test:, losses=0.2303, top1=93.3800, 0=93.3000, 1=97.0000, 2=92.1000, 3=87.0000, 4=95.6000, 5=88.2000, 6=94.8000, 7=94.3000, 8=96.5000, 9=95.0000, 42.1s 2.3h/2.3h
Epoch: [196]
epoch 196/200, train:, losses=0.4077, top1=83.0416, 0=99.3668, 1=99.7732, 2=99.2657, 3=99.1754, 4=99.6359, 5=98.8569, 6=99.5565, 7=99.8636, 8=99.7753, 9=99.5860, val:, losses=0.2597, top1=93.6600, 0=93.4000, 1=97.4000, 2=92.0000, 3=84.8000, 4=95.4000, 5=88.2000, 6=96.4000, 7=96.2000, 8=97.4000, 9=95.4000, test:, losses=0.2601, top1=93.0400, 0=92.3000, 1=96.8000, 2=91.7000, 3=85.2000, 4=95.3000, 5=88.1000, 6=95.6000, 7=94.9000, 8=95.9000, 9=94.6000, 42.2s 2.3h/2.3h
Epoch: [197]
epoch 197/200, train:, losses=0.3796, top1=84.1744, 0=99.3697, 1=99.8695, 2=99.1431, 3=98.5587, 4=99.4913, 5=98.6780, 6=99.6267, 7=99.7429, 8=99.7497, 9=99.4859, val:, losses=0.2514, top1=93.4800, 0=94.2000, 1=97.6000, 2=90.8000, 3=84.2000, 4=94.6000, 5=89.6000, 6=95.6000, 7=95.2000, 8=98.0000, 9=95.0000, test:, losses=0.2524, top1=92.9300, 0=93.0000, 1=97.4000, 2=91.1000, 3=84.0000, 4=95.0000, 5=88.9000, 6=94.8000, 7=94.6000, 8=96.1000, 9=94.4000, 41.8s 2.3h/2.3h
Epoch: [198]
epoch 198/200, train:, losses=0.3602, top1=85.3495, 0=99.6183, 1=99.7840, 2=98.8988, 3=98.7368, 4=99.4083, 5=99.5150, 6=99.7036, 7=99.8304, 8=99.7861, 9=99.5265, val:, losses=0.2560, top1=93.5400, 0=93.8000, 1=96.8000, 2=91.4000, 3=84.8000, 4=95.2000, 5=88.8000, 6=96.0000, 7=95.2000, 8=97.6000, 9=95.8000, test:, losses=0.2568, top1=93.0700, 0=92.5000, 1=96.8000, 2=91.7000, 3=85.4000, 4=95.2000, 5=88.7000, 6=95.1000, 7=94.7000, 8=96.1000, 9=94.5000, 42.2s 2.3h/2.3h
Epoch: [199]
epoch 199/200, train:, losses=0.3817, top1=84.3748, 0=99.4331, 1=99.8262, 2=99.3833, 3=99.0313, 4=99.7358, 5=99.0048, 6=99.7842, 7=99.6899, 8=99.9116, 9=99.5658, val:, losses=0.2335, top1=93.6800, 0=93.8000, 1=97.0000, 2=91.8000, 3=85.0000, 4=95.0000, 5=88.8000, 6=95.6000, 7=95.8000, 8=98.0000, 9=96.0000, test:, losses=0.2353, top1=93.2400, 0=93.0000, 1=96.8000, 2=91.9000, 3=85.0000, 4=95.3000, 5=89.0000, 6=94.9000, 7=94.5000, 8=96.7000, 9=95.3000, 42.9s 2.3h/2.3h
Epoch: [200]
epoch 200/200, train:, losses=0.3580, top1=85.1685, 0=99.7923, 1=99.7498, 2=99.3197, 3=98.4950, 4=99.4132, 5=98.7536, 6=99.5805, 7=99.7038, 8=99.7066, 9=99.3763, val:, losses=0.2079, top1=93.9400, 0=94.6000, 1=97.6000, 2=92.4000, 3=84.8000, 4=95.0000, 5=90.0000, 6=96.0000, 7=95.8000, 8=98.0000, 9=95.2000, test:, losses=0.2137, top1=93.4200, 0=93.5000, 1=96.9000, 2=92.3000, 3=84.8000, 4=95.9000, 5=89.3000, 6=94.7000, 7=95.2000, 8=96.7000, 9=94.9000, 42.3s 2.3h/2.3h
